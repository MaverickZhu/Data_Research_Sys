#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
ä½¿ç”¨10,000æ¡æ•°æ®è¿›è¡ŒåŽ‹åŠ›æµ‹è¯•ï¼ŒéªŒè¯ä¼˜åŒ–åŽçš„ç³»ç»Ÿæ€§èƒ½è¡¨çŽ°
"""

import os
import sys
import logging
import json
import time
import threading
import multiprocessing
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import psutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.database.connection import DatabaseManager
from src.utils.config import ConfigManager
from src.matching.similarity_scorer import SimilarityCalculator
from src.matching.address_normalizer import normalize_address_for_matching

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PerformanceBenchmarkTest:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.config_manager = ConfigManager()
        self.db_manager = DatabaseManager(config=self.config_manager.get_database_config())
        self.similarity_calculator = SimilarityCalculator(self.config_manager.get_matching_config())
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_comparisons': 0,
            'successful_matches': 0,
            'processing_times': [],
            'memory_usage': [],
            'cpu_usage': [],
            'throughput_per_second': 0,
            'average_response_time': 0,
            'peak_memory_usage': 0,
            'peak_cpu_usage': 0
        }
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            'total_test_records': 10000,
            'batch_size': 100,
            'thread_count': 4,
            'similarity_threshold': 0.6,
            'max_comparisons_per_record': 50
        }
        
    def run_performance_benchmark(self):
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ðŸš€ æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 50)
        
        try:
            # 1. å‡†å¤‡æµ‹è¯•æ•°æ®
            print("ðŸ“Š å‡†å¤‡æµ‹è¯•æ•°æ®...")
            test_data = self._prepare_large_test_dataset()
            
            if not test_data['source_addresses'] or not test_data['target_addresses']:
                print("âŒ æµ‹è¯•æ•°æ®å‡†å¤‡å¤±è´¥")
                return
            
            print(f"âœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆï¼š{len(test_data['source_addresses'])} æ¡æºåœ°å€ï¼Œ{len(test_data['target_addresses'])} æ¡ç›®æ ‡åœ°å€")
            
            # 2. å•çº¿ç¨‹æ€§èƒ½æµ‹è¯•
            print("\nðŸ”„ å•çº¿ç¨‹æ€§èƒ½æµ‹è¯•...")
            single_thread_results = self._run_single_thread_test(test_data)
            
            # 3. å¤šçº¿ç¨‹æ€§èƒ½æµ‹è¯•
            print("\nðŸ”„ å¤šçº¿ç¨‹æ€§èƒ½æµ‹è¯•...")
            multi_thread_results = self._run_multi_thread_test(test_data)
            
            # 4. æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•
            print("\nðŸ”„ æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•...")
            batch_processing_results = self._run_batch_processing_test(test_data)
            
            # 5. å†…å­˜å’ŒCPUç›‘æŽ§æµ‹è¯•
            print("\nðŸ”„ èµ„æºä½¿ç”¨ç›‘æŽ§æµ‹è¯•...")
            resource_monitoring_results = self._run_resource_monitoring_test(test_data)
            
            # 6. ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š
            performance_report = self._generate_performance_report({
                'single_thread': single_thread_results,
                'multi_thread': multi_thread_results,
                'batch_processing': batch_processing_results,
                'resource_monitoring': resource_monitoring_results,
                'test_config': self.test_config,
                'system_info': self._get_system_info()
            })
            
            # 7. ä¿å­˜æŠ¥å‘Š
            report_filename = f"performance_benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_filename, 'w', encoding='utf-8') as f:
                json.dump(performance_report, f, ensure_ascii=False, indent=2)
            
            # 8. æ˜¾ç¤ºç»“æžœæ‘˜è¦
            self._display_performance_summary(performance_report)
            
            print(f"\nðŸ“‹ è¯¦ç»†æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")
            print("âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
            print("âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥")
    
    def _prepare_large_test_dataset(self) -> Dict:
        """å‡†å¤‡å¤§è§„æ¨¡æµ‹è¯•æ•°æ®é›†"""
        try:
            # èŽ·å–æºè¡¨æ•°æ®
            source_collection = self.db_manager.get_collection('hztj_hzxx')
            source_addresses = list(source_collection.find(
                {'èµ·ç«åœ°ç‚¹': {'$exists': True, '$ne': ''}},
                {'èµ·ç«åœ°ç‚¹': 1}
            ).limit(self.test_config['total_test_records']))
            
            # èŽ·å–ç›®æ ‡è¡¨æ•°æ®
            target_collection = self.db_manager.get_collection('dwd_yljgxx')
            target_addresses = list(target_collection.find(
                {'ZCDZ': {'$exists': True, '$ne': ''}},
                {'ZCDZ': 1}
            ).limit(self.test_config['total_test_records']))
            
            return {
                'source_addresses': [addr['èµ·ç«åœ°ç‚¹'] for addr in source_addresses],
                'target_addresses': [addr['ZCDZ'] for addr in target_addresses]
            }
            
        except Exception as e:
            logger.error(f"å‡†å¤‡æµ‹è¯•æ•°æ®å¤±è´¥: {str(e)}")
            return {'source_addresses': [], 'target_addresses': []}
    
    def _run_single_thread_test(self, test_data: Dict) -> Dict:
        """è¿è¡Œå•çº¿ç¨‹æ€§èƒ½æµ‹è¯•"""
        start_time = time.time()
        successful_matches = 0
        total_comparisons = 0
        processing_times = []
        
        # é™åˆ¶æµ‹è¯•æ•°é‡ä»¥é¿å…è¿‡é•¿æ—¶é—´
        test_limit = min(1000, len(test_data['source_addresses']))
        
        for i, source_addr in enumerate(test_data['source_addresses'][:test_limit]):
            if i % 100 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{test_limit}")
            
            record_start_time = time.time()
            
            # æ ‡å‡†åŒ–æºåœ°å€
            normalized_source = normalize_address_for_matching(source_addr)
            
            best_score = 0.0
            # é™åˆ¶æ¯”è¾ƒæ•°é‡
            comparison_limit = min(self.test_config['max_comparisons_per_record'], len(test_data['target_addresses']))
            
            for target_addr in test_data['target_addresses'][:comparison_limit]:
                normalized_target = normalize_address_for_matching(target_addr)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self.similarity_calculator.calculate_string_similarity(
                    normalized_source, normalized_target
                )
                
                if similarity > best_score:
                    best_score = similarity
                
                total_comparisons += 1
            
            if best_score >= self.test_config['similarity_threshold']:
                successful_matches += 1
            
            record_time = time.time() - record_start_time
            processing_times.append(record_time)
        
        total_time = time.time() - start_time
        
        return {
            'total_time': total_time,
            'successful_matches': successful_matches,
            'total_comparisons': total_comparisons,
            'records_processed': test_limit,
            'throughput_per_second': test_limit / total_time if total_time > 0 else 0,
            'average_response_time': sum(processing_times) / len(processing_times) if processing_times else 0,
            'match_rate': successful_matches / test_limit if test_limit > 0 else 0
        }
    
    def _run_multi_thread_test(self, test_data: Dict) -> Dict:
        """è¿è¡Œå¤šçº¿ç¨‹æ€§èƒ½æµ‹è¯•"""
        start_time = time.time()
        successful_matches = 0
        total_comparisons = 0
        
        # é™åˆ¶æµ‹è¯•æ•°é‡
        test_limit = min(1000, len(test_data['source_addresses']))
        
        def process_batch(batch_data):
            batch_matches = 0
            batch_comparisons = 0
            
            for source_addr in batch_data:
                normalized_source = normalize_address_for_matching(source_addr)
                best_score = 0.0
                
                comparison_limit = min(self.test_config['max_comparisons_per_record'], len(test_data['target_addresses']))
                
                for target_addr in test_data['target_addresses'][:comparison_limit]:
                    normalized_target = normalize_address_for_matching(target_addr)
                    similarity = self.similarity_calculator.calculate_string_similarity(
                        normalized_source, normalized_target
                    )
                    
                    if similarity > best_score:
                        best_score = similarity
                    
                    batch_comparisons += 1
                
                if best_score >= self.test_config['similarity_threshold']:
                    batch_matches += 1
            
            return batch_matches, batch_comparisons
        
        # åˆ†æ‰¹å¤„ç†
        batch_size = self.test_config['batch_size']
        batches = [test_data['source_addresses'][i:i+batch_size] for i in range(0, test_limit, batch_size)]
        
        with ThreadPoolExecutor(max_workers=self.test_config['thread_count']) as executor:
            results = list(executor.map(process_batch, batches))
        
        for matches, comparisons in results:
            successful_matches += matches
            total_comparisons += comparisons
        
        total_time = time.time() - start_time
        
        return {
            'total_time': total_time,
            'successful_matches': successful_matches,
            'total_comparisons': total_comparisons,
            'records_processed': test_limit,
            'throughput_per_second': test_limit / total_time if total_time > 0 else 0,
            'match_rate': successful_matches / test_limit if test_limit > 0 else 0,
            'thread_count': self.test_config['thread_count']
        }
    
    def _run_batch_processing_test(self, test_data: Dict) -> Dict:
        """è¿è¡Œæ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•"""
        start_time = time.time()
        
        # æ‰¹é‡é¢„å¤„ç†åœ°å€
        print("  æ‰¹é‡æ ‡å‡†åŒ–æºåœ°å€...")
        normalized_sources = []
        for addr in test_data['source_addresses'][:1000]:
            normalized_sources.append(normalize_address_for_matching(addr))
        
        print("  æ‰¹é‡æ ‡å‡†åŒ–ç›®æ ‡åœ°å€...")
        normalized_targets = []
        for addr in test_data['target_addresses'][:1000]:
            normalized_targets.append(normalize_address_for_matching(addr))
        
        preprocessing_time = time.time() - start_time
        
        # æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—
        matching_start_time = time.time()
        successful_matches = 0
        total_comparisons = 0
        
        for i, source in enumerate(normalized_sources[:500]):  # é™åˆ¶æ•°é‡
            if i % 100 == 0:
                print(f"  æ‰¹å¤„ç†è¿›åº¦: {i}/500")
            
            best_score = 0.0
            for target in normalized_targets[:50]:  # é™åˆ¶æ¯”è¾ƒæ•°é‡
                similarity = self.similarity_calculator.calculate_string_similarity(source, target)
                if similarity > best_score:
                    best_score = similarity
                total_comparisons += 1
            
            if best_score >= self.test_config['similarity_threshold']:
                successful_matches += 1
        
        matching_time = time.time() - matching_start_time
        total_time = time.time() - start_time
        
        return {
            'total_time': total_time,
            'preprocessing_time': preprocessing_time,
            'matching_time': matching_time,
            'successful_matches': successful_matches,
            'total_comparisons': total_comparisons,
            'records_processed': 500,
            'throughput_per_second': 500 / total_time if total_time > 0 else 0,
            'match_rate': successful_matches / 500 if successful_matches > 0 else 0
        }
    
    def _run_resource_monitoring_test(self, test_data: Dict) -> Dict:
        """è¿è¡Œèµ„æºä½¿ç”¨ç›‘æŽ§æµ‹è¯•"""
        memory_usage = []
        cpu_usage = []
        
        def monitor_resources():
            while getattr(monitor_resources, 'running', True):
                memory_usage.append(psutil.virtual_memory().percent)
                cpu_usage.append(psutil.cpu_percent())
                time.sleep(0.5)
        
        # å¯åŠ¨ç›‘æŽ§çº¿ç¨‹
        monitor_resources.running = True
        monitor_thread = threading.Thread(target=monitor_resources)
        monitor_thread.start()
        
        try:
            # æ‰§è¡Œä¸€ä¸ªä¸­ç­‰è§„æ¨¡çš„æµ‹è¯•
            start_time = time.time()
            successful_matches = 0
            
            for i, source_addr in enumerate(test_data['source_addresses'][:500]):
                if i % 100 == 0:
                    print(f"  èµ„æºç›‘æŽ§æµ‹è¯•è¿›åº¦: {i}/500")
                
                normalized_source = normalize_address_for_matching(source_addr)
                best_score = 0.0
                
                for target_addr in test_data['target_addresses'][:30]:
                    normalized_target = normalize_address_for_matching(target_addr)
                    similarity = self.similarity_calculator.calculate_string_similarity(
                        normalized_source, normalized_target
                    )
                    
                    if similarity > best_score:
                        best_score = similarity
                
                if best_score >= self.test_config['similarity_threshold']:
                    successful_matches += 1
            
            total_time = time.time() - start_time
            
        finally:
            # åœæ­¢ç›‘æŽ§
            monitor_resources.running = False
            monitor_thread.join()
        
        return {
            'total_time': total_time,
            'successful_matches': successful_matches,
            'records_processed': 500,
            'peak_memory_usage': max(memory_usage) if memory_usage else 0,
            'average_memory_usage': sum(memory_usage) / len(memory_usage) if memory_usage else 0,
            'peak_cpu_usage': max(cpu_usage) if cpu_usage else 0,
            'average_cpu_usage': sum(cpu_usage) / len(cpu_usage) if cpu_usage else 0,
            'memory_samples': len(memory_usage),
            'cpu_samples': len(cpu_usage)
        }
    
    def _get_system_info(self) -> Dict:
        """èŽ·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            'cpu_count': multiprocessing.cpu_count(),
            'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
            'python_version': sys.version,
            'platform': sys.platform
        }
    
    def _generate_performance_report(self, results: Dict) -> Dict:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        return {
            'test_timestamp': datetime.now().isoformat(),
            'test_configuration': results['test_config'],
            'system_information': results['system_info'],
            'performance_results': {
                'single_thread_performance': results['single_thread'],
                'multi_thread_performance': results['multi_thread'],
                'batch_processing_performance': results['batch_processing'],
                'resource_monitoring': results['resource_monitoring']
            },
            'performance_comparison': {
                'single_vs_multi_thread_speedup': (
                    results['multi_thread']['throughput_per_second'] / 
                    results['single_thread']['throughput_per_second']
                ) if results['single_thread']['throughput_per_second'] > 0 else 0,
                'batch_processing_efficiency': (
                    results['batch_processing']['throughput_per_second'] / 
                    results['single_thread']['throughput_per_second']
                ) if results['single_thread']['throughput_per_second'] > 0 else 0
            },
            'recommendations': self._generate_performance_recommendations(results)
        }
    
    def _generate_performance_recommendations(self, results: Dict) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºŽæµ‹è¯•ç»“æžœç”Ÿæˆå»ºè®®
        single_thread = results['single_thread']
        multi_thread = results['multi_thread']
        batch_processing = results['batch_processing']
        resource_monitoring = results['resource_monitoring']
        
        if multi_thread['throughput_per_second'] > single_thread['throughput_per_second'] * 1.5:
            recommendations.append("å¤šçº¿ç¨‹å¤„ç†æ˜¾è‘—æå‡æ€§èƒ½ï¼Œå»ºè®®åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†")
        
        if batch_processing['throughput_per_second'] > single_thread['throughput_per_second'] * 1.2:
            recommendations.append("æ‰¹å¤„ç†æ¨¡å¼æå‡æ€§èƒ½ï¼Œå»ºè®®å¯¹åœ°å€è¿›è¡Œæ‰¹é‡é¢„å¤„ç†")
        
        if resource_monitoring['peak_memory_usage'] > 80:
            recommendations.append("å†…å­˜ä½¿ç”¨çŽ‡è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†æˆ–å¢žåŠ ç³»ç»Ÿå†…å­˜")
        
        if resource_monitoring['peak_cpu_usage'] > 90:
            recommendations.append("CPUä½¿ç”¨çŽ‡è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•æˆ–ä½¿ç”¨æ›´å¼ºçš„CPU")
        
        if single_thread['match_rate'] < 0.3:
            recommendations.append("åŒ¹é…çŽ‡è¾ƒä½Žï¼Œå»ºè®®è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼æˆ–ä¼˜åŒ–åŒ¹é…ç®—æ³•")
        
        return recommendations
    
    def _display_performance_summary(self, report: Dict):
        """æ˜¾ç¤ºæ€§èƒ½æµ‹è¯•æ‘˜è¦"""
        print("\nðŸŽ¯ æ€§èƒ½æµ‹è¯•ç»“æžœæ‘˜è¦:")
        print("=" * 50)
        
        results = report['performance_results']
        
        print(f"ðŸ“Š å•çº¿ç¨‹æ€§èƒ½:")
        print(f"  å¤„ç†é€Ÿåº¦: {results['single_thread_performance']['throughput_per_second']:.2f} æ¡/ç§’")
        print(f"  åŒ¹é…çŽ‡: {results['single_thread_performance']['match_rate']:.1%}")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {results['single_thread_performance']['average_response_time']:.3f} ç§’")
        
        print(f"\nðŸ“Š å¤šçº¿ç¨‹æ€§èƒ½:")
        print(f"  å¤„ç†é€Ÿåº¦: {results['multi_thread_performance']['throughput_per_second']:.2f} æ¡/ç§’")
        print(f"  åŒ¹é…çŽ‡: {results['multi_thread_performance']['match_rate']:.1%}")
        print(f"  çº¿ç¨‹æ•°: {results['multi_thread_performance']['thread_count']}")
        
        print(f"\nðŸ“Š æ‰¹å¤„ç†æ€§èƒ½:")
        print(f"  å¤„ç†é€Ÿåº¦: {results['batch_processing_performance']['throughput_per_second']:.2f} æ¡/ç§’")
        print(f"  åŒ¹é…çŽ‡: {results['batch_processing_performance']['match_rate']:.1%}")
        print(f"  é¢„å¤„ç†æ—¶é—´: {results['batch_processing_performance']['preprocessing_time']:.2f} ç§’")
        
        print(f"\nðŸ“Š èµ„æºä½¿ç”¨:")
        print(f"  å³°å€¼å†…å­˜ä½¿ç”¨: {results['resource_monitoring']['peak_memory_usage']:.1f}%")
        print(f"  å³°å€¼CPUä½¿ç”¨: {results['resource_monitoring']['peak_cpu_usage']:.1f}%")
        print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {results['resource_monitoring']['average_memory_usage']:.1f}%")
        print(f"  å¹³å‡CPUä½¿ç”¨: {results['resource_monitoring']['average_cpu_usage']:.1f}%")
        
        print(f"\nðŸš€ æ€§èƒ½å¯¹æ¯”:")
        comparison = report['performance_comparison']
        print(f"  å¤šçº¿ç¨‹åŠ é€Ÿæ¯”: {comparison['single_vs_multi_thread_speedup']:.2f}x")
        print(f"  æ‰¹å¤„ç†æ•ˆçŽ‡æ¯”: {comparison['batch_processing_efficiency']:.2f}x")
        
        print(f"\nðŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, recommendation in enumerate(report['recommendations'], 1):
            print(f"  {i}. {recommendation}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        benchmark = PerformanceBenchmarkTest()
        benchmark.run_performance_benchmark()
    except Exception as e:
        logger.error(f"æ€§èƒ½åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
        print("âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥")

if __name__ == "__main__":
    main()

