#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¶ˆé˜²å•ä½å»ºç­‘æ•°æ®å…³è”ç³»ç»Ÿæ•°æ®åº“ä¼˜åŒ–è„šæœ¬
åˆ›å»ºå¿…è¦çš„ç´¢å¼•ï¼Œä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½ï¼Œè§£å†³å¤„ç†é€Ÿåº¦ç“¶é¢ˆ
"""
import sys
import os
import time
from datetime import datetime
import pymongo
from pymongo import MongoClient, ASCENDING, DESCENDING, TEXT

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

class DatabaseOptimizer:
    def __init__(self):
        self.client = None
        self.db = None
        self.connect_to_database()
        
    def print_header(self, title):
        """æ‰“å°æ ‡é¢˜"""
        print("\n" + "=" * 80)
        print(f"ğŸ—„ï¸ {title}")
        print("=" * 80)
    
    def connect_to_database(self):
        """è¿æ¥åˆ°æ•°æ®åº“"""
        try:
            self.client = MongoClient('mongodb://localhost:27017/')
            self.db = self.client['Unit_Info']
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            return False
        return True
    
    def analyze_collections(self):
        """åˆ†æé›†åˆç»“æ„"""
        print(f"ğŸ” åˆ†ææ•°æ®åº“é›†åˆç»“æ„...")
        
        collections = self.db.list_collection_names()
        print(f"   å‘ç°é›†åˆ: {collections}")
        
        collection_stats = {}
        for collection_name in collections:
            try:
                collection = self.db[collection_name]
                count = collection.count_documents({})
                
                # è·å–æ ·æœ¬æ–‡æ¡£
                sample = collection.find_one()
                fields = list(sample.keys()) if sample else []
                
                collection_stats[collection_name] = {
                    'count': count,
                    'fields': fields
                }
                
                print(f"   {collection_name}: {count:,} æ¡è®°å½•")
                print(f"     å­—æ®µ: {fields[:10]}{'...' if len(fields) > 10 else ''}")
                
            except Exception as e:
                print(f"   {collection_name}: åˆ†æå¤±è´¥ - {str(e)}")
        
        return collection_stats
    
    def check_existing_indexes(self):
        """æ£€æŸ¥ç°æœ‰ç´¢å¼•"""
        print(f"ğŸ” æ£€æŸ¥ç°æœ‰ç´¢å¼•...")
        
        collections = ['supervision_data', 'inspection_data', 'match_results']
        existing_indexes = {}
        
        for collection_name in collections:
            try:
                collection = self.db[collection_name]
                indexes = list(collection.list_indexes())
                existing_indexes[collection_name] = indexes
                
                print(f"   {collection_name} ç°æœ‰ç´¢å¼•:")
                for idx in indexes:
                    index_name = idx.get('name', 'unknown')
                    index_keys = idx.get('key', {})
                    print(f"     - {index_name}: {dict(index_keys)}")
                    
            except Exception as e:
                print(f"   {collection_name}: æ£€æŸ¥å¤±è´¥ - {str(e)}")
        
        return existing_indexes
    
    def create_performance_indexes(self):
        """åˆ›å»ºæ€§èƒ½ä¼˜åŒ–ç´¢å¼•"""
        print(f"ğŸš€ åˆ›å»ºæ€§èƒ½ä¼˜åŒ–ç´¢å¼•...")
        
        # ç›‘ç£ç®¡ç†æ•°æ®ç´¢å¼•
        supervision_indexes = [
            # å•ä½åç§°ç´¢å¼•ï¼ˆæœ€é‡è¦ï¼‰
            {'keys': [('unit_name', ASCENDING)], 'name': 'idx_unit_name'},
            {'keys': [('unit_name', TEXT)], 'name': 'idx_unit_name_text'},
            
            # åœ°å€ç›¸å…³ç´¢å¼•
            {'keys': [('address', ASCENDING)], 'name': 'idx_address'},
            {'keys': [('address', TEXT)], 'name': 'idx_address_text'},
            
            # å¤åˆç´¢å¼•
            {'keys': [('unit_name', ASCENDING), ('address', ASCENDING)], 'name': 'idx_unit_address'},
            
            # çŠ¶æ€å’Œæ—¶é—´ç´¢å¼•
            {'keys': [('status', ASCENDING)], 'name': 'idx_status'},
            {'keys': [('created_at', DESCENDING)], 'name': 'idx_created_at'},
            
            # åŒ¹é…çŠ¶æ€ç´¢å¼•
            {'keys': [('is_matched', ASCENDING)], 'name': 'idx_is_matched'},
        ]
        
        # å®‰å…¨æ’æŸ¥æ•°æ®ç´¢å¼•
        inspection_indexes = [
            # å•ä½åç§°ç´¢å¼•ï¼ˆæœ€é‡è¦ï¼‰
            {'keys': [('unit_name', ASCENDING)], 'name': 'idx_unit_name'},
            {'keys': [('unit_name', TEXT)], 'name': 'idx_unit_name_text'},
            
            # åœ°å€ç›¸å…³ç´¢å¼•
            {'keys': [('address', ASCENDING)], 'name': 'idx_address'},
            {'keys': [('address', TEXT)], 'name': 'idx_address_text'},
            
            # å¤åˆç´¢å¼•
            {'keys': [('unit_name', ASCENDING), ('address', ASCENDING)], 'name': 'idx_unit_address'},
            
            # æ£€æŸ¥æ—¶é—´ç´¢å¼•
            {'keys': [('inspection_date', DESCENDING)], 'name': 'idx_inspection_date'},
            
            # åŒ¹é…çŠ¶æ€ç´¢å¼•
            {'keys': [('is_matched', ASCENDING)], 'name': 'idx_is_matched'},
        ]
        
        # åŒ¹é…ç»“æœç´¢å¼•
        match_result_indexes = [
            # æºæ•°æ®IDç´¢å¼•
            {'keys': [('supervision_id', ASCENDING)], 'name': 'idx_supervision_id'},
            {'keys': [('inspection_id', ASCENDING)], 'name': 'idx_inspection_id'},
            
            # å¤åˆç´¢å¼•
            {'keys': [('supervision_id', ASCENDING), ('inspection_id', ASCENDING)], 'name': 'idx_match_pair'},
            
            # åŒ¹é…åˆ†æ•°ç´¢å¼•
            {'keys': [('match_score', DESCENDING)], 'name': 'idx_match_score'},
            
            # åˆ›å»ºæ—¶é—´ç´¢å¼•
            {'keys': [('created_at', DESCENDING)], 'name': 'idx_created_at'},
        ]
        
        # åˆ›å»ºç´¢å¼•
        index_results = {}
        
        # ç›‘ç£ç®¡ç†æ•°æ®ç´¢å¼•
        index_results['supervision_data'] = self.create_indexes_for_collection(
            'supervision_data', supervision_indexes
        )
        
        # å®‰å…¨æ’æŸ¥æ•°æ®ç´¢å¼•
        index_results['inspection_data'] = self.create_indexes_for_collection(
            'inspection_data', inspection_indexes
        )
        
        # åŒ¹é…ç»“æœç´¢å¼•
        index_results['match_results'] = self.create_indexes_for_collection(
            'match_results', match_result_indexes
        )
        
        return index_results
    
    def create_indexes_for_collection(self, collection_name, indexes):
        """ä¸ºæŒ‡å®šé›†åˆåˆ›å»ºç´¢å¼•"""
        print(f"   åˆ›å»º {collection_name} ç´¢å¼•...")
        
        try:
            collection = self.db[collection_name]
            results = []
            
            for index_def in indexes:
                try:
                    keys = index_def['keys']
                    name = index_def['name']
                    
                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
                    existing_indexes = [idx['name'] for idx in collection.list_indexes()]
                    
                    if name in existing_indexes:
                        print(f"     âœ… ç´¢å¼•å·²å­˜åœ¨: {name}")
                        results.append({'name': name, 'status': 'exists'})
                        continue
                    
                    # åˆ›å»ºç´¢å¼•
                    start_time = time.time()
                    collection.create_index(keys, name=name, background=True)
                    elapsed = time.time() - start_time
                    
                    print(f"     âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ: {name} ({elapsed:.2f}ç§’)")
                    results.append({'name': name, 'status': 'created', 'time': elapsed})
                    
                except Exception as e:
                    print(f"     âŒ ç´¢å¼•åˆ›å»ºå¤±è´¥: {index_def['name']} - {str(e)}")
                    results.append({'name': index_def['name'], 'status': 'failed', 'error': str(e)})
            
            return results
            
        except Exception as e:
            print(f"   âŒ é›†åˆ {collection_name} ç´¢å¼•åˆ›å»ºå¤±è´¥: {str(e)}")
            return []
    
    def optimize_query_performance(self):
        """ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½"""
        print(f"ğŸ”§ ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½...")
        
        # è®¾ç½®æ•°æ®åº“å‚æ•°
        optimizations = []
        
        try:
            # è®¾ç½®è¯»åå¥½
            self.db.read_preference = pymongo.ReadPreference.SECONDARY_PREFERRED
            optimizations.append("è®¾ç½®è¯»åå¥½ä¸ºSECONDARY_PREFERRED")
            
            # è®¾ç½®å†™å…³æ³¨
            self.db.write_concern = pymongo.WriteConcern(w=1, j=False)
            optimizations.append("è®¾ç½®å†™å…³æ³¨ä¸ºw=1, j=False")
            
            print(f"   âœ… æŸ¥è¯¢ä¼˜åŒ–å®Œæˆ:")
            for opt in optimizations:
                print(f"     - {opt}")
                
        except Exception as e:
            print(f"   âŒ æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {str(e)}")
    
    def test_query_performance(self):
        """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
        print(f"ğŸ“Š æµ‹è¯•æŸ¥è¯¢æ€§èƒ½...")
        
        test_queries = [
            {
                'name': 'å•ä½åç§°æŸ¥è¯¢',
                'collection': 'supervision_data',
                'query': {'unit_name': {'$regex': 'ä¸Šæµ·', '$options': 'i'}},
                'limit': 100
            },
            {
                'name': 'åœ°å€æŸ¥è¯¢',
                'collection': 'supervision_data', 
                'query': {'address': {'$regex': 'æµ¦ä¸œ', '$options': 'i'}},
                'limit': 100
            },
            {
                'name': 'å¤åˆæŸ¥è¯¢',
                'collection': 'supervision_data',
                'query': {'unit_name': {'$exists': True}, 'address': {'$exists': True}},
                'limit': 100
            },
            {
                'name': 'åŒ¹é…çŠ¶æ€æŸ¥è¯¢',
                'collection': 'supervision_data',
                'query': {'is_matched': {'$ne': True}},
                'limit': 1000
            }
        ]
        
        performance_results = []
        
        for test in test_queries:
            try:
                collection = self.db[test['collection']]
                
                # æ‰§è¡ŒæŸ¥è¯¢å¹¶è®¡æ—¶
                start_time = time.time()
                cursor = collection.find(test['query']).limit(test['limit'])
                results = list(cursor)
                elapsed = time.time() - start_time
                
                result = {
                    'name': test['name'],
                    'elapsed': elapsed,
                    'count': len(results),
                    'speed': len(results) / elapsed if elapsed > 0 else 0
                }
                
                performance_results.append(result)
                
                print(f"   {test['name']}: {len(results)} æ¡è®°å½•, {elapsed:.3f}ç§’, {result['speed']:.1f} è®°å½•/ç§’")
                
            except Exception as e:
                print(f"   {test['name']}: æµ‹è¯•å¤±è´¥ - {str(e)}")
        
        return performance_results
    
    def generate_optimization_report(self, index_results, performance_results):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        print(f"\nğŸ“Š æ•°æ®åº“ä¼˜åŒ–æŠ¥å‘Š:")
        
        # ç´¢å¼•åˆ›å»ºç»Ÿè®¡
        total_indexes = 0
        created_indexes = 0
        existing_indexes = 0
        failed_indexes = 0
        
        for collection, results in index_results.items():
            for result in results:
                total_indexes += 1
                if result['status'] == 'created':
                    created_indexes += 1
                elif result['status'] == 'exists':
                    existing_indexes += 1
                elif result['status'] == 'failed':
                    failed_indexes += 1
        
        print(f"   ç´¢å¼•ä¼˜åŒ–:")
        print(f"     æ€»ç´¢å¼•æ•°: {total_indexes}")
        print(f"     æ–°åˆ›å»º: {created_indexes}")
        print(f"     å·²å­˜åœ¨: {existing_indexes}")
        print(f"     åˆ›å»ºå¤±è´¥: {failed_indexes}")
        
        # æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡
        if performance_results:
            avg_speed = sum(r['speed'] for r in performance_results) / len(performance_results)
            max_speed = max(r['speed'] for r in performance_results)
            min_speed = min(r['speed'] for r in performance_results)
            
            print(f"   æŸ¥è¯¢æ€§èƒ½:")
            print(f"     å¹³å‡é€Ÿåº¦: {avg_speed:.1f} è®°å½•/ç§’")
            print(f"     æœ€é«˜é€Ÿåº¦: {max_speed:.1f} è®°å½•/ç§’")
            print(f"     æœ€ä½é€Ÿåº¦: {min_speed:.1f} è®°å½•/ç§’")
        
        # ä¼˜åŒ–å»ºè®®
        print(f"   ä¼˜åŒ–å»ºè®®:")
        if created_indexes > 0:
            print(f"     âœ… å·²åˆ›å»º {created_indexes} ä¸ªæ–°ç´¢å¼•ï¼ŒæŸ¥è¯¢æ€§èƒ½åº”æœ‰æ˜¾è‘—æå‡")
        if failed_indexes > 0:
            print(f"     âš ï¸ {failed_indexes} ä¸ªç´¢å¼•åˆ›å»ºå¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥")
        print(f"     ğŸ’¡ å»ºè®®é‡æ–°å¯åŠ¨åŒ¹é…ä»»åŠ¡ä»¥éªŒè¯æ€§èƒ½æå‡")
    
    def run_database_optimization(self):
        """è¿è¡Œæ•°æ®åº“ä¼˜åŒ–"""
        self.print_header("æ¶ˆé˜²å•ä½å»ºç­‘æ•°æ®å…³è”ç³»ç»Ÿ - æ•°æ®åº“ä¼˜åŒ–")
        
        print(f"ğŸ•’ å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ ç›®æ ‡: åˆ›å»ºå¿…è¦ç´¢å¼•ï¼Œä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½")
        
        if not self.client:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œä¼˜åŒ–")
            return False
        
        # åˆ†æé›†åˆç»“æ„
        collection_stats = self.analyze_collections()
        
        # æ£€æŸ¥ç°æœ‰ç´¢å¼•
        existing_indexes = self.check_existing_indexes()
        
        # åˆ›å»ºæ€§èƒ½ç´¢å¼•
        index_results = self.create_performance_indexes()
        
        # ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
        self.optimize_query_performance()
        
        # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
        performance_results = self.test_query_performance()
        
        # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        self.generate_optimization_report(index_results, performance_results)
        
        self.print_header("æ•°æ®åº“ä¼˜åŒ–å®Œæˆ")
        print(f"ğŸ•’ ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"âœ… æ•°æ®åº“ä¼˜åŒ–æ‰§è¡ŒæˆåŠŸ")
        print(f"ğŸ’¡ å»ºè®®é‡æ–°å¯åŠ¨åŒ¹é…ä»»åŠ¡ä»¥éªŒè¯æ€§èƒ½æå‡")
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    optimizer = DatabaseOptimizer()
    optimizer.run_database_optimization()

if __name__ == "__main__":
    main() 