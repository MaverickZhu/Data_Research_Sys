#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºŽåˆ‡ç‰‡ç´¢å¼•çš„å¢žå¼ºæ¨¡ç³ŠåŒ¹é…å™¨
åˆ©ç”¨N-gramåˆ‡ç‰‡å’Œå…³é”®è¯ç´¢å¼•å®žçŽ°è¶…é«˜é€Ÿæ¨¡ç³ŠåŒ¹é…
"""

import pymongo
import re
import jieba
import time
from typing import Dict, List, Set, Tuple, Optional
from rapidfuzz import fuzz, process
from collections import defaultdict, Counter
import logging

logger = logging.getLogger(__name__)

class SliceEnhancedMatcher:
    def __init__(self, db_manager=None):
        """åˆå§‹åŒ–åˆ‡ç‰‡å¢žå¼ºåŒ¹é…å™¨"""
        if db_manager:
            # ä½¿ç”¨get_db()æ–¹æ³•èŽ·å–æ•°æ®åº“å®žä¾‹
            if hasattr(db_manager, 'get_db'):
                self.db = db_manager.get_db()
            elif hasattr(db_manager, 'db'):
                self.db = db_manager.db
            else:
                # é™çº§åˆ°é»˜è®¤è¿žæŽ¥
                client = pymongo.MongoClient('mongodb://localhost:27017/')
                self.db = client['Unit_Info']
        else:
            client = pymongo.MongoClient('mongodb://localhost:27017/')
            self.db = client['Unit_Info']
        
        # ç¼“å­˜é›†åˆå¼•ç”¨
        self.source_slice_collection = self.db['xfaqpc_jzdwxx_name_slices']
        self.source_keyword_collection = self.db['xfaqpc_jzdwxx_name_keywords']
        self.target_slice_collection = self.db['xxj_shdwjbxx_name_slices']
        self.target_keyword_collection = self.db['xxj_shdwjbxx_name_keywords']
        self.target_collection = self.db['xxj_shdwjbxx']
        self.cache_collection = self.db['unit_name_similarity_cache']
        
        # é…ç½®å‚æ•°
        self.config = {
            'slice_match_threshold': 0.3,      # åˆ‡ç‰‡åŒ¹é…é˜ˆå€¼
            'keyword_match_threshold': 0.5,    # å…³é”®è¯åŒ¹é…é˜ˆå€¼
            'final_similarity_threshold': 0.75, # æœ€ç»ˆç›¸ä¼¼åº¦é˜ˆå€¼
            'max_candidates': 50,               # æœ€å¤§å€™é€‰æ•°é‡
            'use_cache': True,                  # å¯ç”¨ç¼“å­˜
            'cache_ttl': 7 * 24 * 3600         # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆ7å¤©ï¼‰
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'slice_queries': 0,
            'keyword_queries': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'total_matches': 0
        }
        
        logger.info("ðŸš€ åˆ‡ç‰‡å¢žå¼ºåŒ¹é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_ngram_slices(self, text: str, n: int = 3) -> Set[str]:
        """åˆ›å»ºN-gramåˆ‡ç‰‡"""
        if not text or len(text) < n:
            return set()
        
        # æ¸…ç†æ–‡æœ¬
        clean_text = re.sub(r'[^\u4e00-\u9fff\w]', '', text)
        
        # ç”ŸæˆN-gramåˆ‡ç‰‡
        slices = set()
        for i in range(len(clean_text) - n + 1):
            slice_text = clean_text[i:i+n]
            if len(slice_text) == n:
                slices.add(slice_text)
        
        return slices
    
    def extract_keywords(self, text: str) -> Set[str]:
        """æå–å…³é”®è¯"""
        if not text:
            return set()
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(text)
        keywords = set()
        
        for word in words:
            word = word.strip()
            # è¿‡æ»¤æŽ‰é•¿åº¦å°äºŽ2çš„è¯å’Œå¸¸è§åœç”¨è¯
            if len(word) >= 2 and word not in {'æœ‰é™', 'å…¬å¸', 'ä¼ä¸š', 'é›†å›¢', 'å·¥åŽ‚', 'å•†åº—', 'ä¸­å¿ƒ'}:
                keywords.add(word)
        
        return keywords
    
    def find_candidates_by_slices(self, source_name: str) -> List[Dict]:
        """é€šè¿‡åˆ‡ç‰‡æŸ¥æ‰¾å€™é€‰è®°å½•"""
        try:
            # ç”Ÿæˆ3-gramå’Œ2-gramåˆ‡ç‰‡
            slices_3 = self.create_ngram_slices(source_name, 3)
            slices_2 = self.create_ngram_slices(source_name, 2)
            all_slices = slices_3.union(slices_2)
            
            if not all_slices:
                return []
            
            # æŸ¥è¯¢åˆ‡ç‰‡ç´¢å¼•
            self.stats['slice_queries'] += 1
            
            # ä½¿ç”¨èšåˆç®¡é“ç»Ÿè®¡åˆ‡ç‰‡åŒ¹é…åº¦
            pipeline = [
                {'$match': {'slice': {'$in': list(all_slices)}}},
                {'$group': {
                    '_id': '$doc_id',
                    'unit_name': {'$first': '$unit_name'},
                    'match_count': {'$sum': 1},
                    'matched_slices': {'$push': '$slice'}
                }},
                {'$addFields': {
                    'slice_score': {'$divide': ['$match_count', len(all_slices)]}
                }},
                {'$match': {'slice_score': {'$gte': self.config['slice_match_threshold']}}},
                {'$sort': {'slice_score': -1}},
                {'$limit': self.config['max_candidates']}
            ]
            
            candidates = list(self.target_slice_collection.aggregate(pipeline))
            
            # èŽ·å–å®Œæ•´è®°å½•
            if candidates:
                doc_ids = [candidate['_id'] for candidate in candidates]
                full_records = list(self.target_collection.find(
                    {'_id': {'$in': doc_ids}},
                    {'dwmc': 1, 'dwdz': 1, 'tyshxydm': 1, 'fddbr': 1, 'xfaqglr': 1}
                ))
                
                # åˆå¹¶åˆ‡ç‰‡ä¿¡æ¯å’Œå®Œæ•´è®°å½•
                record_map = {record['_id']: record for record in full_records}
                result = []
                
                for candidate in candidates:
                    doc_id = candidate['_id']
                    if doc_id in record_map:
                        record = record_map[doc_id].copy()
                        record['slice_score'] = candidate['slice_score']
                        record['matched_slices'] = candidate['matched_slices']
                        result.append(record)
                
                return result
            
            return []
            
        except Exception as e:
            logger.error(f"åˆ‡ç‰‡æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def find_candidates_by_keywords(self, source_name: str) -> List[Dict]:
        """é€šè¿‡å…³é”®è¯æŸ¥æ‰¾å€™é€‰è®°å½•"""
        try:
            keywords = self.extract_keywords(source_name)
            if not keywords:
                return []
            
            self.stats['keyword_queries'] += 1
            
            # ä½¿ç”¨èšåˆç®¡é“ç»Ÿè®¡å…³é”®è¯åŒ¹é…åº¦
            pipeline = [
                {'$match': {'keyword': {'$in': list(keywords)}}},
                {'$group': {
                    '_id': '$doc_id',
                    'unit_name': {'$first': '$unit_name'},
                    'match_count': {'$sum': 1},
                    'matched_keywords': {'$push': '$keyword'}
                }},
                {'$addFields': {
                    'keyword_score': {'$divide': ['$match_count', len(keywords)]}
                }},
                {'$match': {'keyword_score': {'$gte': self.config['keyword_match_threshold']}}},
                {'$sort': {'keyword_score': -1}},
                {'$limit': self.config['max_candidates']}
            ]
            
            candidates = list(self.target_keyword_collection.aggregate(pipeline))
            
            # èŽ·å–å®Œæ•´è®°å½•
            if candidates:
                doc_ids = [candidate['_id'] for candidate in candidates]
                full_records = list(self.target_collection.find(
                    {'_id': {'$in': doc_ids}},
                    {'dwmc': 1, 'dwdz': 1, 'tyshxydm': 1, 'fddbr': 1, 'xfaqglr': 1}
                ))
                
                # åˆå¹¶å…³é”®è¯ä¿¡æ¯å’Œå®Œæ•´è®°å½•
                record_map = {record['_id']: record for record in full_records}
                result = []
                
                for candidate in candidates:
                    doc_id = candidate['_id']
                    if doc_id in record_map:
                        record = record_map[doc_id].copy()
                        record['keyword_score'] = candidate['keyword_score']
                        record['matched_keywords'] = candidate['matched_keywords']
                        result.append(record)
                
                return result
            
            return []
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def calculate_similarity(self, source_name: str, target_name: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            source_name: æºå­—ç¬¦ä¸²
            target_name: ç›®æ ‡å­—ç¬¦ä¸²
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0.0-1.0)
        """
        if not source_name or not target_name:
            return 0.0
        
        # ä½¿ç”¨å¤šç§ç®—æ³•è®¡ç®—ç›¸ä¼¼åº¦
        basic_similarity = fuzz.ratio(source_name, target_name) / 100.0
        token_similarity = fuzz.token_sort_ratio(source_name, target_name) / 100.0
        partial_similarity = fuzz.partial_ratio(source_name, target_name) / 100.0
        
        # åŠ æƒå¹³å‡
        weighted_score = (
            basic_similarity * 0.4 +
            token_similarity * 0.4 +
            partial_similarity * 0.2
        )
        
        return weighted_score
    
    def calculate_enhanced_similarity(self, source_name: str, target_record: Dict) -> float:
        """è®¡ç®—å¢žå¼ºç›¸ä¼¼åº¦"""
        target_name = target_record.get('dwmc', '')
        if not target_name:
            return 0.0
        
        # æ£€æŸ¥ç¼“å­˜
        if self.config['use_cache']:
            cache_key = f"{source_name}||{target_name}"
            cached_result = self.cache_collection.find_one({'source_name': source_name, 'target_name': target_name})
            
            if cached_result:
                self.stats['cache_hits'] += 1
                return cached_result.get('similarity_score', 0.0)
            else:
                self.stats['cache_misses'] += 1
        
        # è®¡ç®—å¤šç»´åº¦ç›¸ä¼¼åº¦
        similarities = {}
        weights = {}
        
        # 1. åŸºç¡€å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ (æƒé‡: 0.4)
        basic_similarity = fuzz.ratio(source_name, target_name) / 100.0
        similarities['basic'] = basic_similarity
        weights['basic'] = 0.4
        
        # 2. Tokenç›¸ä¼¼åº¦ (æƒé‡: 0.3)
        token_similarity = fuzz.token_sort_ratio(source_name, target_name) / 100.0
        similarities['token'] = token_similarity
        weights['token'] = 0.3
        
        # 3. åˆ‡ç‰‡ç›¸ä¼¼åº¦ (æƒé‡: 0.2)
        if 'slice_score' in target_record:
            similarities['slice'] = target_record['slice_score']
            weights['slice'] = 0.2
        else:
            # å®žæ—¶è®¡ç®—åˆ‡ç‰‡ç›¸ä¼¼åº¦
            source_slices = self.create_ngram_slices(source_name, 3)
            target_slices = self.create_ngram_slices(target_name, 3)
            
            if source_slices and target_slices:
                intersection = source_slices.intersection(target_slices)
                union = source_slices.union(target_slices)
                slice_similarity = len(intersection) / len(union) if union else 0.0
                similarities['slice'] = slice_similarity
                weights['slice'] = 0.2
        
        # 4. å…³é”®è¯ç›¸ä¼¼åº¦ (æƒé‡: 0.1)
        if 'keyword_score' in target_record:
            similarities['keyword'] = target_record['keyword_score']
            weights['keyword'] = 0.1
        else:
            # å®žæ—¶è®¡ç®—å…³é”®è¯ç›¸ä¼¼åº¦
            source_keywords = self.extract_keywords(source_name)
            target_keywords = self.extract_keywords(target_name)
            
            if source_keywords and target_keywords:
                intersection = source_keywords.intersection(target_keywords)
                union = source_keywords.union(target_keywords)
                keyword_similarity = len(intersection) / len(union) if union else 0.0
                similarities['keyword'] = keyword_similarity
                weights['keyword'] = 0.1
        
        # è®¡ç®—åŠ æƒå¹³å‡åˆ†æ•°
        total_weight = sum(weights.values())
        if total_weight > 0:
            final_score = sum(similarities[key] * weights[key] for key in similarities) / total_weight
        else:
            final_score = basic_similarity
        
        # ç¼“å­˜ç»“æžœ
        if self.config['use_cache']:
            try:
                self.cache_collection.insert_one({
                    'source_name': source_name,
                    'target_name': target_name,
                    'similarity_score': final_score,
                    'created_time': time.time()
                })
            except Exception:
                pass  # å¿½ç•¥ç¼“å­˜æ’å…¥é”™è¯¯
        
        return final_score
    
    def find_best_matches(self, source_record: Dict) -> List[Dict]:
        """æŸ¥æ‰¾æœ€ä½³åŒ¹é…"""
        source_name = source_record.get('UNIT_NAME', '')
        if not source_name:
            return []
        
        start_time = time.time()
        
        # 1. é€šè¿‡åˆ‡ç‰‡æŸ¥æ‰¾å€™é€‰
        slice_candidates = self.find_candidates_by_slices(source_name)
        
        # 2. é€šè¿‡å…³é”®è¯æŸ¥æ‰¾å€™é€‰
        keyword_candidates = self.find_candidates_by_keywords(source_name)
        
        # 3. åˆå¹¶å€™é€‰è®°å½•ï¼ˆåŽ»é‡ï¼‰
        all_candidates = {}
        
        for candidate in slice_candidates:
            doc_id = candidate['_id']
            all_candidates[doc_id] = candidate
        
        for candidate in keyword_candidates:
            doc_id = candidate['_id']
            if doc_id in all_candidates:
                # åˆå¹¶åˆ†æ•°ä¿¡æ¯
                all_candidates[doc_id]['keyword_score'] = candidate.get('keyword_score', 0)
                all_candidates[doc_id]['matched_keywords'] = candidate.get('matched_keywords', [])
            else:
                all_candidates[doc_id] = candidate
        
        # 4. è®¡ç®—ç›¸ä¼¼åº¦å¹¶æŽ’åº
        scored_matches = []
        for candidate in all_candidates.values():
            similarity = self.calculate_enhanced_similarity(source_name, candidate)
            
            if similarity >= self.config['final_similarity_threshold']:
                candidate['similarity_score'] = similarity
                scored_matches.append(candidate)
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæŽ’åº
        scored_matches.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        # ç»Ÿè®¡
        self.stats['total_matches'] += len(scored_matches)
        
        processing_time = time.time() - start_time
        logger.debug(f"åˆ‡ç‰‡å¢žå¼ºåŒ¹é…è€—æ—¶: {processing_time:.4f}ç§’, å€™é€‰: {len(all_candidates)}, åŒ¹é…: {len(scored_matches)}")
        
        return scored_matches[:10]  # è¿”å›žå‰10ä¸ªæœ€ä½³åŒ¹é…
    
    def get_performance_stats(self) -> Dict:
        """èŽ·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            'slice_queries': self.stats['slice_queries'],
            'keyword_queries': self.stats['keyword_queries'],
            'cache_hits': self.stats['cache_hits'],
            'cache_misses': self.stats['cache_misses'],
            'cache_hit_rate': self.stats['cache_hits'] / (self.stats['cache_hits'] + self.stats['cache_misses']) if (self.stats['cache_hits'] + self.stats['cache_misses']) > 0 else 0,
            'total_matches': self.stats['total_matches']
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        try:
            self.cache_collection.drop()
            logger.info("ç›¸ä¼¼åº¦ç¼“å­˜å·²æ¸…ç©º")
        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}") 