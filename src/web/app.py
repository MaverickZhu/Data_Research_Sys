"""
Flask Webåº”ç”¨ä¸»ç¨‹åº
æä¾›æ•°æ®åŒ¹é…è¿›åº¦ç®¡ç†çš„Webç•Œé¢
"""

from flask import Flask, render_template, jsonify, request, send_file
from flask_cors import CORS
import logging
import yaml
import os
from datetime import datetime
from typing import Dict, List
import pandas as pd
import io
import math
import json

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.database.connection import DatabaseManager
from src.matching.match_processor import MatchProcessor
from src.matching.multi_match_processor import MultiMatchProcessor
from src.matching.enhanced_association_processor import EnhancedAssociationProcessor, AssociationStrategy
from src.matching.optimized_match_processor import OptimizedMatchProcessor
from src.utils.logger import setup_logger
from src.utils.config import ConfigManager
from src.utils.helpers import safe_json_response, generate_match_id

# V2.0æ–°å¢æ¨¡å—å¯¼å…¥
from src.data_manager.csv_processor import CSVProcessor
from src.data_manager.data_analyzer import DataAnalyzer  
from src.data_manager.schema_detector import SchemaDetector
from src.data_manager.validation_engine import ValidationEngine

# çŸ¥è¯†å›¾è°±æ¨¡å—å¯¼å…¥
from src.knowledge_graph.kg_store import KnowledgeGraphStore
from src.knowledge_graph.kg_builder import KnowledgeGraphBuilder
from src.knowledge_graph.entity_extractor import EntityExtractor
from src.knowledge_graph.relation_extractor import RelationExtractor
from src.knowledge_graph.kg_quality_assessor import KnowledgeGraphQualityAssessor

# è®¾ç½®æ—¥å¿—
logger = setup_logger(__name__)

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
CORS(app, resources={
    r"/api/*": {
        "origins": "*",
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization"]
    }
})

# å…¨å±€å˜é‡
db_manager = None
match_processor = None
multi_match_processor = None

# çŸ¥è¯†å›¾è°±ç›¸å…³å…¨å±€å˜é‡
kg_store = None
kg_builder = None
entity_extractor = None
relation_extractor = None
kg_quality_assessor = None
enhanced_association_processor = None
optimized_match_processor = None
config_manager = None

# V2.0æ–°å¢å…¨å±€å˜é‡
csv_processor = None
data_analyzer = None
schema_detector = None
validation_engine = None


def create_app():
    """åˆ›å»ºå’Œé…ç½®Flaskåº”ç”¨"""
    global db_manager, match_processor, multi_match_processor, enhanced_association_processor, optimized_match_processor, config_manager
    global csv_processor, data_analyzer, schema_detector, validation_engine
    global kg_store, kg_builder, entity_extractor, relation_extractor, kg_quality_assessor
    
    try:
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        config_manager = ConfigManager()
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        db_manager = DatabaseManager(config=config_manager.get_database_config())
        
        # åˆå§‹åŒ–åŒ¹é…å¤„ç†å™¨
        match_processor = MatchProcessor(
            db_manager=db_manager,
            config=config_manager.get_matching_config()
        )
        
        # åˆå§‹åŒ–ä¸€å¯¹å¤šåŒ¹é…å¤„ç†å™¨
        multi_match_processor = MultiMatchProcessor(
            db_manager=db_manager,
            config=config_manager.get_matching_config()
        )
        
        # åˆå§‹åŒ–å¢å¼ºå…³è”å¤„ç†å™¨
        enhanced_association_processor = EnhancedAssociationProcessor(
            db_manager=db_manager,
            config=config_manager.get_matching_config()
        )
        
        # æ–°å¢ï¼šç»Ÿä¸€åˆå§‹åŒ–ä¼˜åŒ–çš„åŒ¹é…å¤„ç†å™¨
        optimized_match_processor = OptimizedMatchProcessor(
            db_manager=db_manager,
            config_manager=config_manager
        )
        
        # V2.0æ–°å¢ï¼šåˆå§‹åŒ–æ•°æ®ç®¡ç†å¤„ç†å™¨
        csv_processor = CSVProcessor()
        data_analyzer = DataAnalyzer()
        schema_detector = SchemaDetector()
        validation_engine = ValidationEngine()
        
        # çŸ¥è¯†å›¾è°±ç»„ä»¶åˆå§‹åŒ–
        # ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç»„ä»¶
        kg_store = KnowledgeGraphStore(
            db_manager=db_manager,
            config={
                'batch_size': 1000,
                'use_threading': True,
                'max_workers': 4,
                'enable_compression': True,
                'cache_size': 10000
            }
        )
        
        entity_extractor = EntityExtractor(
            config={
                'min_confidence': 0.6,
                'max_entities_per_record': 50,
                'enable_nlp': True
            }
        )
        
        relation_extractor = RelationExtractor(
            config={
                'min_confidence': 0.7,
                'max_relations_per_record': 20,
                'enable_semantic_reasoning': True
            }
        )
        
        kg_builder = KnowledgeGraphBuilder(
            kg_store=kg_store,
            config={
                'batch_size': 500,
                'enable_quality_check': True
            }
        )
        
        kg_quality_assessor = KnowledgeGraphQualityAssessor(
            kg_store=kg_store,
            config={
                'assessment_interval': 3600,  # 1 hour
                'quality_threshold': 0.8
            }
        )
        
        logger.info("Flaskåº”ç”¨åˆå§‹åŒ–æˆåŠŸï¼ˆåŒ…å«çŸ¥è¯†å›¾è°±ç»„ä»¶ï¼‰")
        return app
        
    except Exception as e:
        logger.error(f"Flaskåº”ç”¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise


@app.route('/')
def index():
    """é¦–é¡µ - ç³»ç»Ÿæ¦‚è§ˆ"""
    try:
        # æä¾›åŸºæœ¬çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…é˜»å¡
        basic_stats = {
            'data_sources': {
                'supervision_count': 'åŠ è½½ä¸­...',
                'inspection_count': 'åŠ è½½ä¸­...',
                'match_results_count': 'åŠ è½½ä¸­...'
            },
            'matching_stats': {
                'total_matches': 'åŠ è½½ä¸­...',
                'last_updated': 'åŠ è½½ä¸­...'
            },
            'system_info': {
                'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'status': 'running'
            }
        }
        return render_template('index.html', stats=basic_stats)
    except Exception as e:
        logger.error(f"é¦–é¡µåŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


@app.route('/matching')
def matching_page():
    """åŒ¹é…ç®¡ç†é¡µé¢"""
    try:
        # è·å–åŒ¹é…é…ç½®ä¿¡æ¯
        matching_config = config_manager.get_matching_config()
        return render_template('matching.html', config=matching_config)
    except Exception as e:
        logger.error(f"åŒ¹é…ç®¡ç†é¡µé¢åŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


@app.route('/results')
def results_page():
    """åŒ¹é…ç»“æœé¡µé¢"""
    try:
        return render_template('results.html')
    except Exception as e:
        logger.error(f"åŒ¹é…ç»“æœé¡µé¢åŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


@app.route('/statistics')
def statistics_page():
    """æ•°æ®ç»Ÿè®¡é¡µé¢"""
    try:
        return render_template('statistics.html')
    except Exception as e:
        logger.error(f"æ•°æ®ç»Ÿè®¡é¡µé¢åŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


@app.route('/test')
def test_page():
    """æ•°æ®åŠ è½½æµ‹è¯•é¡µé¢"""
    return '''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ•°æ®åŠ è½½æµ‹è¯•é¡µé¢</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .card { border: 1px solid #ddd; padding: 20px; margin: 10px 0; border-radius: 5px; }
        .success { color: green; }
        .error { color: red; }
        .loading { color: orange; }
        pre { background: #f5f5f5; padding: 10px; border-radius: 3px; overflow-x: auto; }
    </style>
</head>
<body>
    <h1>ğŸ§ª æ•°æ®åŠ è½½æµ‹è¯•é¡µé¢</h1>
    
    <div class="card">
        <h2>ğŸ“Š ç»Ÿè®¡æ•°æ®</h2>
        <div id="stats-display">
            <p class="loading">æ­£åœ¨åŠ è½½æ•°æ®...</p>
        </div>
    </div>
    
    <div class="card">
        <h2>ğŸ“¡ APIå“åº”</h2>
        <div id="api-response">
            <p class="loading">æ­£åœ¨è·å–APIå“åº”...</p>
        </div>
    </div>
    
    <div class="card">
        <h2>ğŸ”§ è°ƒè¯•ä¿¡æ¯</h2>
        <div id="debug-info">
            <p>ç­‰å¾…è°ƒè¯•ä¿¡æ¯...</p>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            console.log('ğŸš€ æµ‹è¯•é¡µé¢åŠ è½½å®Œæˆ');
            testDataLoading();
        });

        function testDataLoading() {
            const debugInfo = document.getElementById('debug-info');
            const statsDisplay = document.getElementById('stats-display');
            const apiResponse = document.getElementById('api-response');
            
            debugInfo.innerHTML = '<p class="loading">å¼€å§‹æµ‹è¯•æ•°æ®åŠ è½½...</p>';
            
            console.log('ğŸ“¡ å¼€å§‹æµ‹è¯•APIæ¥å£...');
            
            fetch('/api/stats')
                .then(response => {
                    console.log('ğŸ“Š APIå“åº”çŠ¶æ€:', response.status);
                    debugInfo.innerHTML += `<p>APIå“åº”çŠ¶æ€: ${response.status}</p>`;
                    
                    if (!response.ok) {
                        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
                    }
                    return response.json();
                })
                .then(data => {
                    console.log('âœ… æ”¶åˆ°æ•°æ®:', data);
                    
                    apiResponse.innerHTML = `
                        <p class="success">âœ… APIè°ƒç”¨æˆåŠŸ</p>
                        <pre>${JSON.stringify(data, null, 2)}</pre>
                    `;
                    
                    const supervisionCount = data.data_sources.supervision_count || 0;
                    const inspectionCount = data.data_sources.inspection_count || 0;
                    const matchResultsCount = data.data_sources.match_results_count || 0;
                    const totalMatches = data.matching_stats.total_matches || 0;
                    const totalUnits = supervisionCount + inspectionCount;
                    const matchRate = totalUnits > 0 ? (totalMatches / totalUnits * 100).toFixed(1) : 0;
                    
                    statsDisplay.innerHTML = `
                        <div class="success">
                            <h3>ğŸ“ˆ æ•°æ®ç»Ÿè®¡</h3>
                            <p><strong>ç›‘ç£ç®¡ç†ç³»ç»Ÿ:</strong> ${supervisionCount.toLocaleString()} æ¡</p>
                            <p><strong>å®‰å…¨æ’æŸ¥ç³»ç»Ÿ:</strong> ${inspectionCount.toLocaleString()} æ¡</p>
                            <p><strong>æ€»å•ä½æ•°:</strong> ${totalUnits.toLocaleString()} æ¡</p>
                            <p><strong>åŒ¹é…ç»“æœ:</strong> ${matchResultsCount.toLocaleString()} æ¡</p>
                            <p><strong>æ€»åŒ¹é…æ•°:</strong> ${totalMatches.toLocaleString()} æ¡</p>
                            <p><strong>åŒ¹é…ç‡:</strong> ${matchRate}%</p>
                        </div>
                    `;
                    
                    debugInfo.innerHTML += '<p class="success">âœ… æ•°æ®åŠ è½½æˆåŠŸ</p>';
                    
                })
                .catch(error => {
                    console.error('âŒ æ•°æ®åŠ è½½å¤±è´¥:', error);
                    
                    apiResponse.innerHTML = `
                        <p class="error">âŒ APIè°ƒç”¨å¤±è´¥</p>
                        <p>é”™è¯¯ä¿¡æ¯: ${error.message}</p>
                    `;
                    
                    statsDisplay.innerHTML = `
                        <p class="error">âŒ æ•°æ®åŠ è½½å¤±è´¥: ${error.message}</p>
                    `;
                    
                    debugInfo.innerHTML += `<p class="error">âŒ é”™è¯¯: ${error.message}</p>`;
                });
        }
    </script>
</body>
</html>'''


@app.route('/progress')
def progress_page():
    """è¿›åº¦ç›‘æ§é¡µé¢"""
    try:
        return render_template('progress.html')
    except Exception as e:
        logger.error(f"åŠ è½½è¿›åº¦ç›‘æ§é¡µé¢å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e)), 500


@app.route('/api/stats')
def api_get_stats():
    """API: è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = get_system_stats()
        # ç¡®ä¿æ•°æ®èƒ½æ­£ç¡®åºåˆ—åŒ–
        safe_stats = safe_json_response(stats)
        return jsonify(safe_stats)
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/start_matching', methods=['POST'])
def api_start_matching():
    """API: å¯åŠ¨åŒ¹é…ä»»åŠ¡"""
    try:
        request_data = request.get_json() or {}
        
        # è·å–åŒ¹é…å‚æ•°
        match_type = request_data.get('match_type', 'both')  # exact, fuzzy, both
        batch_size = request_data.get('batch_size', 100)
        
        # å¯åŠ¨åŒ¹é…ä»»åŠ¡
        task_id = match_processor.start_matching_task(
            match_type=match_type,
            batch_size=batch_size
        )
        
        logger.info(f"åŒ¹é…ä»»åŠ¡å¯åŠ¨æˆåŠŸï¼Œä»»åŠ¡ID: {task_id}")
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'åŒ¹é…ä»»åŠ¡å·²å¯åŠ¨'
        })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/task_progress/<task_id>')
def api_get_task_progress(task_id):
    """API: è·å–ä»»åŠ¡è¿›åº¦"""
    try:
        progress = match_processor.get_task_progress(task_id)
        # ç¡®ä¿æ•°æ®èƒ½æ­£ç¡®åºåˆ—åŒ–
        safe_progress = safe_json_response(progress)
        return jsonify(safe_progress)
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/stop_matching/<task_id>', methods=['POST'])
def api_stop_matching(task_id):
    """API: åœæ­¢åŒ¹é…ä»»åŠ¡"""
    try:
        # å¢å¼ºè¯Šæ–­ï¼šè®°å½•APIè¯·æ±‚æ¥æº
        logger.info(f"æ”¶åˆ°æ¥è‡ªIP '{request.remote_addr}' çš„APIè¯·æ±‚ï¼Œè¦æ±‚åœæ­¢ä»»åŠ¡ '{task_id}'")

        success = match_processor.stop_matching_task(task_id)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'åŒ¹é…ä»»åŠ¡å·²åœæ­¢'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'åœæ­¢ä»»åŠ¡å¤±è´¥'
            })
            
    except Exception as e:
        logger.error(f"åœæ­¢åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


def _format_match_results(raw_results: List[Dict]) -> List[Dict]:
    """å°†ä»æ•°æ®åº“è·å–çš„åŸå§‹åŒ¹é…ç»“æœæ ¼å¼åŒ–ä¸ºç»Ÿä¸€çš„ç»“æ„"""
    new_results = []
    for row in raw_results:
        # å®‰å…¨è·å–å­—æ®µå€¼çš„è¾…åŠ©å‡½æ•°
        def safe_get(key, default=None):
            value = row.get(key, default)
            # ç»Ÿä¸€å¤„ç†å„ç§å½¢å¼çš„ç©ºå€¼
            return value if value not in [None, '', '-', 'null'] else default
        
        # ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„ã€ç¨³å®šçš„match_id
        # è¿™æ˜¯ä¿®å¤å†³ç­–åˆ†æåŠŸèƒ½404é—®é¢˜çš„å…³é”®
        match_id = safe_get('match_id')
        if not match_id:
            # ä»…åœ¨match_idä¸å­˜åœ¨æ—¶æ‰ç”Ÿæˆï¼Œä½œä¸ºåå¤‡æ–¹æ¡ˆ
            # (éœ€è¦ç¡®ä¿helperå‡½æ•°å¯ç”¨)
            try:
                from src.utils.helpers import generate_match_id
                primary_id = safe_get('primary_record_id') or safe_get('_id')
                matched_id = safe_get('matched_record_id', 'no_match')
                match_id = generate_match_id(str(primary_id), str(matched_id))
            except ImportError:
                # å¦‚æœå¸®åŠ©å‡½æ•°ä¸å¯ç”¨ï¼Œåˆ™ä½¿ç”¨_idä½œä¸ºå¤‡ç”¨
                match_id = str(safe_get('_id'))

        # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨åœ°å°†IDè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        def to_str(val):
            return str(val) if val is not None else None
            
        new_results.append({
            # åŸºæœ¬ä¿¡æ¯
            'primary_unit_name': safe_get('primary_unit_name') or safe_get('unit_name'),
            'matched_unit_name': safe_get('matched_unit_name'),
            'match_type': safe_get('match_type', 'none'),
            'similarity_score': safe_get('similarity_score', 0),
            'match_time': safe_get('match_time') or safe_get('matching_time') or safe_get('process_time'),
            
            # åœ°å€ä¿¡æ¯
            'primary_unit_address': safe_get('primary_unit_address') or safe_get('unit_address') or safe_get('address'),
            'matched_unit_address': safe_get('matched_unit_address'),
            
            # æ³•å®šä»£è¡¨äººä¿¡æ¯
            'primary_legal_person': safe_get('primary_legal_person') or safe_get('legal_person') or safe_get('contact_person'),
            'matched_legal_person': safe_get('matched_legal_person'),
            
            # æ¶ˆé˜²å®‰å…¨ç®¡ç†äººä¿¡æ¯
            'primary_security_manager': safe_get('primary_security_manager') or safe_get('SECURITY_PEOPLE') or safe_get('security_manager') or safe_get('fire_safety_manager'),
            'matched_security_manager': safe_get('matched_security_manager') or safe_get('xfaqglr'),
            
            # è”ç³»ç”µè¯
            'primary_phone': safe_get('primary_phone') or safe_get('contact_phone') or safe_get('phone'),
            'matched_phone': safe_get('matched_phone') or safe_get('matched_contact_phone'),
            
            # å…¶ä»–ä¿¡æ¯ (ç¡®ä¿IDä¸ºå­—ç¬¦ä¸²)
            'primary_credit_code': to_str(safe_get('primary_credit_code') or safe_get('credit_code')),
            'matched_credit_code': to_str(safe_get('matched_credit_code')),
                
            # ç³»ç»Ÿä¿¡æ¯
            '_id': str(row.get('_id')),
            'match_id': match_id, # ç¡®ä¿ä½¿ç”¨ç”Ÿæˆçš„match_id
            'xfaqpc_jzdwxx_id': to_str(safe_get('xfaqpc_jzdwxx_id')),
            'xxj_shdwjbxx_id': to_str(safe_get('xxj_shdwjbxx_id')),
            'review_status': safe_get('review_status', 'pending'),
            'review_notes': safe_get('review_notes', ''),
            'reviewer': safe_get('reviewer', ''),
            'review_time': safe_get('review_time'),
            'created_time': safe_get('created_time'),
            'updated_time': safe_get('updated_time'),
            
            # åŒ¹é…è¯¦æƒ…
            'match_details': safe_get('match_details', {}),
            'match_confidence': safe_get('match_confidence'),
            'match_reason': safe_get('match_reason')
        })
    return new_results


@app.route('/api/match_results')
def api_get_match_results():
    """API: è·å–åŒ¹é…ç»“æœ"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        page = int(request.args.get('page', 1))
        per_page = int(request.args.get('per_page', 20))
        match_type = request.args.get('match_type')
        search_term = request.args.get('search_term') # è·å–æœç´¢å…³é”®è¯
        
        # æŸ¥è¯¢åŒ¹é…ç»“æœ
        results_data = db_manager.get_match_results(
            page=page,
            per_page=per_page,
            match_type=match_type,
            search_term=search_term # ä¼ é€’æœç´¢å…³é”®è¯
        )
        
        # ä½¿ç”¨æ–°çš„è¾…åŠ©å‡½æ•°æ ¼å¼åŒ–ç»“æœ
        new_results = _format_match_results(results_data.get('results', []))
        
        results_data['results'] = new_results
        return jsonify(results_data)
        
    except Exception as e:
        logger.error(f"è·å–åŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
        return jsonify({
            'results': [],
            'total': 0,
            'page': page,
            'per_page': per_page,
            'pages': 0,
            'error': str(e)
        })


@app.route('/api/match_statistics')
def api_get_match_statistics():
    """API: è·å–åŒ¹é…ç»Ÿè®¡ä¿¡æ¯"""
    try:
        statistics = db_manager.get_match_statistics()
        # ç¡®ä¿æ•°æ®èƒ½æ­£ç¡®åºåˆ—åŒ–
        safe_statistics = safe_json_response(statistics)
        return jsonify(safe_statistics)
    except Exception as e:
        logger.error(f"è·å–åŒ¹é…ç»Ÿè®¡å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/running_tasks')
def api_get_running_tasks():
    """API: è·å–å½“å‰è¿è¡Œçš„ä»»åŠ¡"""
    try:
        # è·å–è¿è¡Œä¸­çš„ä»»åŠ¡
        running_tasks = match_processor.get_running_tasks() if hasattr(match_processor, 'get_running_tasks') else []
        
        # ç¡®ä¿æ•°æ®èƒ½æ­£ç¡®åºåˆ—åŒ–
        safe_tasks = safe_json_response(running_tasks)
        return jsonify({
            'success': True,
            'tasks': safe_tasks,
            'count': len(safe_tasks)
        })
    except Exception as e:
        logger.error(f"è·å–è¿è¡Œä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'tasks': [],
            'count': 0
        }), 500


@app.route('/api/task_history')
def api_get_task_history():
    """API: è·å–ä»»åŠ¡å†å²"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        page = int(request.args.get('page', 1))
        per_page = int(request.args.get('per_page', 10))
        
        # è·å–ä»»åŠ¡å†å²ï¼ˆè¿™é‡Œéœ€è¦å®ç°å®é™…çš„å†å²è®°å½•åŠŸèƒ½ï¼‰
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
        mock_history = [
            {
                'id': 'ab2b5b43-8970-429f-aa9b-115273276637',
                'type': 'both',
                'status': 'completed',
                'start_time': '2025-06-13 10:34:50',
                'end_time': '2025-06-13 10:45:30',
                'processed': 1659320,
                'matches': 28,
                'progress': 100,
                'duration': 640  # ç§’
            },
            {
                'id': '13170dc0-353b-4d07-ae63-691c9090a66c',
                'type': 'both',
                'status': 'completed',
                'start_time': '2025-06-13 11:21:21',
                'end_time': '2025-06-13 11:25:15',
                'processed': 1659320,
                'matches': 32,
                'progress': 100,
                'duration': 234  # ç§’
            }
        ]
        
        # åˆ†é¡µå¤„ç†
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        page_tasks = mock_history[start_idx:end_idx]
        
        return jsonify({
            'success': True,
            'tasks': page_tasks,
            'total': len(mock_history),
            'page': page,
            'per_page': per_page,
            'pages': (len(mock_history) + per_page - 1) // per_page
        })
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡å†å²å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'tasks': [],
            'total': 0
        }), 500


@app.route('/api/export_results')
def api_export_results():
    """API: å¯¼å‡ºåŒ¹é…ç»“æœä¸ºCSVæ–‡ä»¶"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        match_type = request.args.get('match_type')
        search_term = request.args.get('search_term')

        # è·å–æ‰€æœ‰åŒ¹é…ç»“æœï¼ˆä¸åˆ†é¡µï¼‰
        # æˆ‘ä»¬é€šè¿‡è®¾ç½®ä¸€ä¸ªæå¤§çš„per_pageå€¼æ¥è·å–æ‰€æœ‰æ•°æ®
        # æ›´å¥½çš„æ–¹æ³•æ˜¯ä¿®æ”¹get_match_resultsä»¥æ”¯æŒå¯¼å‡ºæ¨¡å¼
        results_data = db_manager.get_match_results(
            page=1,
            per_page=1000000, # è·å–æ‰€æœ‰æ•°æ®
            match_type=match_type,
            search_term=search_term
        )

        # ä½¿ç”¨è¾…åŠ©å‡½æ•°æ ¼å¼åŒ–ç»“æœ
        formatted_results = _format_match_results(results_data.get('results', []))

        if not formatted_results:
            return jsonify({'success': False, 'message': 'æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®'}), 404

        # ä½¿ç”¨pandasåˆ›å»ºDataFrame
        df = pd.DataFrame(formatted_results)

        # é€‰æ‹©å¹¶é‡å‘½åå­—æ®µ
        export_df = df[[
            'primary_unit_name', 'matched_unit_name', 'match_type', 'similarity_score',
            'review_status', 'primary_unit_address', 'matched_unit_address',
            'primary_legal_person', 'matched_legal_person', 'primary_credit_code', 'matched_credit_code',
            'xfaqpc_jzdwxx_id', 'xxj_shdwjbxx_id'
        ]].rename(columns={
            'primary_unit_name': 'æºå•ä½åç§°',
            'matched_unit_name': 'åŒ¹é…å•ä½åç§°',
            'match_type': 'åŒ¹é…ç±»å‹',
            'similarity_score': 'ç›¸ä¼¼åº¦',
            'review_status': 'å®¡æ ¸çŠ¶æ€',
            'primary_unit_address': 'æºå•ä½åœ°å€',
            'matched_unit_address': 'åŒ¹é…å•ä½åœ°å€',
            'primary_legal_person': 'æºå•ä½æ³•äºº',
            'matched_legal_person': 'åŒ¹é…å•ä½æ³•äºº',
            'primary_credit_code': 'æºå•ä½ä¿¡ç”¨ä»£ç ',
            'matched_credit_code': 'åŒ¹é…å•ä½ä¿¡ç”¨ä»£ç ',
            'xfaqpc_jzdwxx_id': 'æºç³»ç»ŸåŸå§‹ID',
            'xxj_shdwjbxx_id': 'ç›®æ ‡ç³»ç»ŸåŸå§‹ID'
        })
        
        # å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        export_df['ç›¸ä¼¼åº¦'] = export_df['ç›¸ä¼¼åº¦'].apply(lambda x: f"{x*100:.1f}%" if isinstance(x, float) else x)

        # ä¸ºä¿¡ç”¨ä»£ç å’ŒåŸå§‹IDæ·»åŠ ç‰¹æ®Šæ ¼å¼ï¼Œé˜²æ­¢Excelç­‰è½¯ä»¶è‡ªåŠ¨è½¬æ¢ä¸ºç§‘å­¦è®¡æ•°æ³•
        def format_id_for_excel(value):
            """Safely formats a value that might be an ID to prevent Excel's conversion."""
            if value is None or not pd.notna(value) or str(value).strip() == '':
                return ''
            
            # The value is now guaranteed to be a string from the source.
            return f'="{str(value).strip()}"'

        for col in ['æºå•ä½ä¿¡ç”¨ä»£ç ', 'åŒ¹é…å•ä½ä¿¡ç”¨ä»£ç ', 'æºç³»ç»ŸåŸå§‹ID', 'ç›®æ ‡ç³»ç»ŸåŸå§‹ID']:
            export_df[col] = export_df[col].apply(format_id_for_excel)

        # åˆ›å»ºä¸€ä¸ªå†…å­˜ä¸­çš„CSVæ–‡ä»¶
        output = io.BytesIO()
        export_df.to_csv(output, index=False, encoding='utf-8-sig')
        output.seek(0)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"match_results_{timestamp}.csv"

        return send_file(
            output,
            mimetype='text/csv',
            as_attachment=True,
            download_name=filename
        )
        
    except Exception as e:
        logger.error(f"å¯¼å‡ºç»“æœå¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/update_config', methods=['POST'])
def api_update_config():
    """API: æ›´æ–°åŒ¹é…é…ç½®"""
    try:
        config_data = request.get_json()
        
        # æ›´æ–°é…ç½®
        success = config_manager.update_matching_config(config_data)
        
        if success:
            # é‡æ–°åˆå§‹åŒ–åŒ¹é…å¤„ç†å™¨
            global match_processor
            match_processor = MatchProcessor(
                db_manager=db_manager,
                config=config_manager.get_matching_config()
            )
            
            return jsonify({
                'success': True,
                'message': 'é…ç½®æ›´æ–°æˆåŠŸ'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'é…ç½®æ›´æ–°å¤±è´¥'
            })
            
    except Exception as e:
        logger.error(f"æ›´æ–°é…ç½®å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/start_optimized_matching', methods=['POST'])
def api_start_optimized_matching():
    """API: å¯åŠ¨ä¼˜åŒ–åŒ¹é…ä»»åŠ¡"""
    try:
        request_data = request.get_json() or {}
        
        # è·å–åŒ¹é…å‚æ•°
        match_type = request_data.get('match_type', 'both')  # exact, fuzzy, both
        mode = request_data.get('mode', 'incremental')  # incremental, update, full
        batch_size = request_data.get('batch_size', 100)
        clear_existing = request_data.get('clear_existing', False)
        
        # éªŒè¯æ¨¡å¼å‚æ•°
        from src.matching.optimized_match_processor import MatchingMode
        valid_modes = [MatchingMode.INCREMENTAL, MatchingMode.UPDATE, MatchingMode.FULL]
        if mode not in valid_modes:
            return jsonify({
                'success': False,
                'error': f'æ— æ•ˆçš„åŒ¹é…æ¨¡å¼: {mode}ï¼Œæ”¯æŒçš„æ¨¡å¼: {valid_modes}'
            }), 400
        
        # ç›´æ¥ä½¿ç”¨å·²åˆå§‹åŒ–çš„å¤„ç†å™¨
        task_id = optimized_match_processor.start_optimized_matching_task(
            match_type=match_type,
            mode=mode,
            batch_size=batch_size,
            clear_existing=clear_existing
        )
        
        logger.info(f"ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å¯åŠ¨æˆåŠŸï¼Œä»»åŠ¡ID: {task_id}, æ¨¡å¼: {mode}")
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'mode': mode,
            'message': f'ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å·²å¯åŠ¨ (æ¨¡å¼: {mode})'
        })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/optimized_task_progress/<task_id>')
def api_get_optimized_task_progress(task_id):
    """API: è·å–ä¼˜åŒ–ä»»åŠ¡è¿›åº¦"""
    try:
        if not optimized_match_processor:
            return jsonify({'error': 'ä¼˜åŒ–åŒ¹é…å¤„ç†å™¨æœªåˆå§‹åŒ–'}), 500
        
        progress = optimized_match_processor.get_optimized_task_progress(task_id)
        # ç¡®ä¿æ•°æ®èƒ½æ­£ç¡®åºåˆ—åŒ–
        safe_progress = safe_json_response(progress)
        return jsonify(safe_progress)
    except Exception as e:
        logger.error(f"è·å–ä¼˜åŒ–ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/stop_optimized_matching/<task_id>', methods=['POST'])
def api_stop_optimized_matching(task_id):
    """API: åœæ­¢ä¼˜åŒ–åŒ¹é…ä»»åŠ¡"""
    try:
        # å¢å¼ºè¯Šæ–­ï¼šè®°å½•APIè¯·æ±‚æ¥æº
        logger.info(f"æ”¶åˆ°æ¥è‡ªIP '{request.remote_addr}' çš„APIè¯·æ±‚ï¼Œè¦æ±‚åœæ­¢ä»»åŠ¡ '{task_id}'")

        if not optimized_match_processor:
            return jsonify({
                'success': False,
                'message': 'ä¼˜åŒ–åŒ¹é…å¤„ç†å™¨æœªåˆå§‹åŒ–'
            }), 500
        
        success = optimized_match_processor.stop_optimized_matching_task(task_id)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å·²åœæ­¢'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'åœæ­¢ä¼˜åŒ–ä»»åŠ¡å¤±è´¥'
            })
            
    except Exception as e:
        logger.error(f"åœæ­¢ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/optimized_match_statistics')
def api_get_optimized_match_statistics():
    """API: è·å–ä¼˜åŒ–åŒ¹é…ç»Ÿè®¡ä¿¡æ¯"""
    try:
        if not optimized_match_processor:
            return jsonify({'error': 'ä¼˜åŒ–åŒ¹é…å¤„ç†å™¨æœªåˆå§‹åŒ–'}), 500
        
        statistics = optimized_match_processor.get_optimized_matching_statistics()
        # ç¡®ä¿æ•°æ®èƒ½æ­£ç¡®åºåˆ—åŒ–
        safe_statistics = safe_json_response(statistics)
        return jsonify(safe_statistics)
    except Exception as e:
        logger.error(f"è·å–ä¼˜åŒ–åŒ¹é…ç»Ÿè®¡å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/match_result_details/<match_id>')
def api_get_match_result_details(match_id):
    """API: è·å–åŒ¹é…ç»“æœè¯¦æƒ…"""
    try:
        if not optimized_match_processor:
            return jsonify({'error': 'ä¼˜åŒ–åŒ¹é…å¤„ç†å™¨æœªåˆå§‹åŒ–'}), 500
        
        details = optimized_match_processor.get_match_result_details(match_id)
        
        if details:
            return jsonify({
                'success': True,
                'details': details
            })
        else:
            return jsonify({
                'success': False,
                'message': 'æœªæ‰¾åˆ°åŒ¹é…ç»“æœ'
            }), 404
            
    except Exception as e:
        logger.error(f"è·å–åŒ¹é…ç»“æœè¯¦æƒ…å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/update_review_status', methods=['POST'])
def api_update_review_status():
    """API: æ›´æ–°å®¡æ ¸çŠ¶æ€"""
    try:
        request_data = request.get_json() or {}
        
        match_id = request_data.get('match_id')
        review_status = request_data.get('review_status')  # pending, approved, rejected
        review_notes = request_data.get('review_notes', '')
        reviewer = request_data.get('reviewer', '')
        
        if not match_id or not review_status:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: match_id å’Œ review_status'
            }), 400
        
        # éªŒè¯å®¡æ ¸çŠ¶æ€
        valid_statuses = ['pending', 'approved', 'rejected']
        if review_status not in valid_statuses:
            return jsonify({
                'success': False,
                'error': f'æ— æ•ˆçš„å®¡æ ¸çŠ¶æ€: {review_status}ï¼Œæ”¯æŒçš„çŠ¶æ€: {valid_statuses}'
            }), 400
        
        if not optimized_match_processor:
            return jsonify({'error': 'ä¼˜åŒ–åŒ¹é…å¤„ç†å™¨æœªåˆå§‹åŒ–'}), 500
        
        success = optimized_match_processor.update_review_status(
            match_id=match_id,
            review_status=review_status,
            review_notes=review_notes,
            reviewer=reviewer
        )
        
        if success:
            return jsonify({
                'success': True,
                'message': 'å®¡æ ¸çŠ¶æ€æ›´æ–°æˆåŠŸ'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'å®¡æ ¸çŠ¶æ€æ›´æ–°å¤±è´¥'
            })
            
    except Exception as e:
        logger.error(f"æ›´æ–°å®¡æ ¸çŠ¶æ€å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/clear_match_results', methods=['POST'])
def api_clear_match_results():
    """API: æ¸…é™¤åŒ¹é…ç»“æœè¡¨"""
    try:
        # è·å–åŒ¹é…ç»“æœé›†åˆ
        collection = db_manager.get_collection('unit_match_results')
        
        # åˆ é™¤æ‰€æœ‰è®°å½•
        result = collection.delete_many({})
        
        logger.info(f"æ¸…é™¤åŒ¹é…ç»“æœè¡¨æˆåŠŸï¼Œåˆ é™¤è®°å½•æ•°: {result.deleted_count}")
        
        return jsonify({
            'success': True,
            'deleted_count': result.deleted_count,
            'message': f'æˆåŠŸæ¸…é™¤ {result.deleted_count} æ¡åŒ¹é…ç»“æœè®°å½•'
        })
        
    except Exception as e:
        logger.error(f"æ¸…é™¤åŒ¹é…ç»“æœè¡¨å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'æ¸…é™¤åŒ¹é…ç»“æœè¡¨å¤±è´¥'
        }), 500


@app.route('/api/fuzzy_review_results')
def api_fuzzy_review_results():
    """API: è·å–å¾…äººå·¥å®¡æ ¸çš„æ¨¡ç³ŠåŒ¹é…ç»“æœ"""
    try:
        # è·å–é›†åˆ
        collection = db_manager.get_collection('unit_match_results')
        
        # æŸ¥è¯¢æ¡ä»¶ï¼šæ¨¡ç³ŠåŒ¹é…ä¸”å¾…å®¡æ ¸
        query = {
            'match_type': 'fuzzy',
            '$or': [
                {'review_status': {'$exists': False}},
                {'review_status': 'pending'}
            ]
        }
        
        cursor = collection.find(query).sort('created_time', -1).limit(100)
        results = []
        
        for doc in cursor:
            # è¿”å›å‰ç«¯éœ€è¦çš„å­—æ®µï¼Œç¡®ä¿å­—æ®µååŒ¹é…
            result = {
                'primary_unit_name': doc.get('unit_name', ''),  # ä¸»ç³»ç»Ÿå•ä½åç§°
                'matched_unit_name': doc.get('matched_unit_name', ''),  # åŒ¹é…åˆ°çš„å•ä½åç§°
                'similarity_score': doc.get('similarity_score', 0),  # ç›¸ä¼¼åº¦åˆ†æ•°
                'field_similarities': doc.get('match_details', {}).get('field_similarities', {}),  # å­—æ®µç›¸ä¼¼åº¦è¯¦æƒ…
                'match_id': str(doc.get('_id', '')),  # åŒ¹é…è®°å½•ID
                
                # æ·»åŠ æ›´å¤šè¯¦ç»†ä¿¡æ¯ç”¨äºæ˜¾ç¤º
                'primary_unit_address': doc.get('unit_address', ''),
                'matched_unit_address': doc.get('matched_unit_address', ''),
                'primary_legal_person': doc.get('contact_person', ''),
                'matched_legal_person': doc.get('matched_legal_person', ''),
                'primary_security_manager': doc.get('security_manager', ''),
                'matched_security_manager': doc.get('matched_security_manager', ''),
                'match_confidence': doc.get('match_confidence', ''),
                'match_fields': doc.get('match_fields', []),
                'created_time': doc.get('created_time', '').strftime('%Y-%m-%d %H:%M:%S') if doc.get('created_time') else '',
            }
            results.append(result)
        
        return jsonify({'results': results})
        
    except Exception as e:
        logger.error(f"è·å–å¾…äººå·¥å®¡æ ¸æ¨¡ç³ŠåŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
        return jsonify({'results': [], 'error': str(e)})


@app.route('/api/confirm_association', methods=['POST'])
def api_confirm_association():
    """API: ç¡®è®¤å…³è”"""
    try:
        request_data = request.get_json() or {}
        match_id = request_data.get('match_id')
        
        if not match_id:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: match_id'
            }), 400
        
        # è·å–é›†åˆ
        collection = db_manager.get_collection('unit_match_results')
        
        # æ›´æ–°å®¡æ ¸çŠ¶æ€
        from bson import ObjectId
        from datetime import datetime
        
        update_result = collection.update_one(
            {'_id': ObjectId(match_id)},
            {
                '$set': {
                    'review_status': 'approved',
                    'review_notes': 'äººå·¥å®¡æ ¸ç¡®è®¤å…³è”',
                    'reviewer': 'manual_reviewer',
                    'review_time': datetime.now()
                }
            }
        )
        
        if update_result.modified_count > 0:
            logger.info(f"ç¡®è®¤å…³è”æˆåŠŸ: {match_id}")
            return jsonify({
                'success': True,
                'message': 'å…³è”ç¡®è®¤æˆåŠŸ'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'æœªæ‰¾åˆ°åŒ¹é…è®°å½•æˆ–æ›´æ–°å¤±è´¥'
            })
            
    except Exception as e:
        logger.error(f"ç¡®è®¤å…³è”å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/reject_association', methods=['POST'])
def api_reject_association():
    """API: æ‹’ç»å…³è”"""
    try:
        request_data = request.get_json() or {}
        match_id = request_data.get('match_id')
        reason = request_data.get('reason', 'äººå·¥å®¡æ ¸æ‹’ç»å…³è”')
        
        if not match_id:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: match_id'
            }), 400
        
        # è·å–é›†åˆ
        collection = db_manager.get_collection('unit_match_results')
        
        # æ›´æ–°å®¡æ ¸çŠ¶æ€
        from bson import ObjectId
        from datetime import datetime
        
        update_result = collection.update_one(
            {'_id': ObjectId(match_id)},
            {
                '$set': {
                    'review_status': 'rejected',
                    'review_notes': reason,
                    'reviewer': 'manual_reviewer',
                    'review_time': datetime.now()
                }
            }
        )
        
        if update_result.modified_count > 0:
            logger.info(f"æ‹’ç»å…³è”æˆåŠŸ: {match_id}")
            return jsonify({
                'success': True,
                'message': 'å…³è”æ‹’ç»æˆåŠŸ'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'æœªæ‰¾åˆ°åŒ¹é…è®°å½•æˆ–æ›´æ–°å¤±è´¥'
            })
            
    except Exception as e:
        logger.error(f"æ‹’ç»å…³è”å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


def get_system_stats() -> Dict:
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–åŸå§‹åŒ¹é…ç»Ÿè®¡
        raw_matching_stats = db_manager.get_match_statistics()
        
        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        matching_stats = {
            'total_matches': raw_matching_stats.get('total_matches', 0),
            'exact': 0,
            'fuzzy': 0, 
            'none': 0
        }
        
        # ä»stats_by_typeä¸­æå–æ•°æ®
        for stat in raw_matching_stats.get('stats_by_type', []):
            match_type = stat.get('_id', '')
            count = stat.get('count', 0)
            
            if match_type == 'exact':
                matching_stats['exact'] = count
            elif match_type == 'fuzzy':
                matching_stats['fuzzy'] = count
            elif match_type == 'none':
                matching_stats['none'] = count
        
        # å¦‚æœstats_by_typeä¸ºç©ºï¼Œå°è¯•ä»raw_stats_by_typeä¸­æå–
        if not raw_matching_stats.get('stats_by_type'):
            for stat in raw_matching_stats.get('raw_stats_by_type', []):
                match_type = stat.get('_id', '')
                count = stat.get('count', 0)
                
                if match_type and match_type.startswith('exact'):
                    matching_stats['exact'] += count
                elif match_type and match_type.startswith('fuzzy'):
                    matching_stats['fuzzy'] += count
                elif match_type == 'none':
                    matching_stats['none'] += count
        
        stats = {
            'data_sources': {
                'supervision_count': db_manager.get_collection_count('xxj_shdwjbxx'),
                'inspection_count': db_manager.get_collection_count('xfaqpc_jzdwxx'),
                'match_results_count': db_manager.get_collection_count('unit_match_results')  # ä¿®æ­£é›†åˆåç§°
            },
            'matching_stats': matching_stats,
            'raw_matching_stats': raw_matching_stats,  # ä¿ç•™åŸå§‹æ•°æ®ä¾›è°ƒè¯•
            'system_info': {
                'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'status': 'running'
            }
        }
        
        return stats
        
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        return {
            'error': str(e),
            'data_sources': {},
            'matching_stats': {},
            'system_info': {'status': 'error'}
        }


@app.errorhandler(404)
def not_found_error(error):
    """404é”™è¯¯å¤„ç†"""
    return render_template('error.html', 
                          error='é¡µé¢æœªæ‰¾åˆ°', 
                          error_code=404), 404


@app.errorhandler(500)
def internal_error(error):
    """500é”™è¯¯å¤„ç†"""
    logger.error(f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(error)}")
    return render_template('error.html', 
                          error='å†…éƒ¨æœåŠ¡å™¨é”™è¯¯', 
                          error_code=500), 500


@app.route('/api/start_multi_matching', methods=['POST'])
def api_start_multi_matching():
    """å¯åŠ¨ä¸€å¯¹å¤šåŒ¹é…å¤„ç†"""
    global multi_match_processor
    
    try:
        data = request.get_json() or {}
        limit = data.get('limit', None)
        
        logger.info(f"å¼€å§‹ä¸€å¯¹å¤šåŒ¹é…å¤„ç†ï¼Œé™åˆ¶: {limit}")
        
        # æ‰§è¡Œä¸€å¯¹å¤šåŒ¹é…
        statistics = multi_match_processor.process_all_records(limit=limit)
        
        return safe_json_response({
            'success': True,
            'message': 'ä¸€å¯¹å¤šåŒ¹é…å¤„ç†å®Œæˆ',
            'statistics': statistics
        })
        
    except Exception as e:
        logger.error(f"ä¸€å¯¹å¤šåŒ¹é…å¤„ç†å¤±è´¥: {str(e)}")
        return safe_json_response({
            'success': False,
            'error': str(e)
        }, status_code=500)


@app.route('/api/multi_match_statistics')
def api_get_multi_match_statistics():
    """è·å–ä¸€å¯¹å¤šåŒ¹é…ç»Ÿè®¡ä¿¡æ¯"""
    global multi_match_processor
    
    try:
        statistics = multi_match_processor.get_statistics()
        
        return safe_json_response({
            'success': True,
            'statistics': statistics
        })
        
    except Exception as e:
        logger.error(f"è·å–ä¸€å¯¹å¤šåŒ¹é…ç»Ÿè®¡å¤±è´¥: {str(e)}")
        return safe_json_response({
            'success': False,
            'error': str(e)
        }, status_code=500)


@app.route('/api/multi_match_results')
def api_get_multi_match_results():
    """è·å–ä¸€å¯¹å¤šåŒ¹é…ç»“æœ"""
    global multi_match_processor
    
    try:
        page = int(request.args.get('page', 1))
        per_page = int(request.args.get('per_page', 20))
        
        skip = (page - 1) * per_page
        results = multi_match_processor.get_match_results(limit=per_page, skip=skip)
        
        # æ ¼å¼åŒ–ç»“æœç”¨äºå‰ç«¯æ˜¾ç¤º
        formatted_results = []
        for result in results:
            formatted_result = {
                'id': str(result.get('_id', '')),
                'unit_name': result.get('unit_name', ''),
                'address': result.get('address', ''),
                'legal_representative': result.get('legal_representative', ''),
                'credit_code': result.get('credit_code', ''),
                'match_status': result.get('match_status', ''),
                'total_matched_records': result.get('total_matched_records', 0),
                'match_summary': result.get('match_summary', {}),
                'primary_matched_record': result.get('primary_matched_record', {}),
                'matched_records': result.get('matched_records', []),
                'created_at': result.get('created_at', '').isoformat() if result.get('created_at') else ''
            }
            formatted_results.append(formatted_result)
        
        return safe_json_response({
            'success': True,
            'results': formatted_results,
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total': len(formatted_results)
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–ä¸€å¯¹å¤šåŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
        return safe_json_response({
            'success': False,
            'error': str(e)
        }, status_code=500)


@app.route('/api/unit_inspection_history/<unit_id>')
def api_get_unit_inspection_history(unit_id):
    """è·å–å•ä½çš„æ£€æŸ¥å†å²è®°å½•"""
    global multi_match_processor
    
    try:
        # æŸ¥æ‰¾æŒ‡å®šå•ä½çš„åŒ¹é…ç»“æœ
        result_collection = db_manager.get_collection('unit_multi_match_results')
        unit_result = result_collection.find_one({'_id': unit_id})
        
        if not unit_result:
            return safe_json_response({
                'success': False,
                'error': 'æœªæ‰¾åˆ°æŒ‡å®šå•ä½'
            }, status_code=404)
        
        # æ ¼å¼åŒ–æ£€æŸ¥å†å²è®°å½•
        inspection_history = []
        for record in unit_result.get('matched_records', []):
            match_info = record.get('match_info', {})
            inspection_record = {
                'inspection_id': str(record.get('_id', '')),
                'inspection_date': match_info.get('inspection_date', ''),
                'unit_name': record.get('dwmc', ''),
                'address': record.get('dwdz', ''),
                'legal_representative': record.get('fddbr', ''),
                'contact_phone': record.get('frlxdh', ''),
                'credit_code': record.get('tyshxydm', ''),
                'match_type': match_info.get('match_type', ''),
                'similarity_score': match_info.get('similarity_score', 0),
                'is_primary': match_info.get('is_primary', False),
                'match_rank': match_info.get('match_rank', 0),
                'data_completeness': match_info.get('data_completeness', 0)
            }
            inspection_history.append(inspection_record)
        
        # æŒ‰æ£€æŸ¥æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        inspection_history.sort(key=lambda x: x['inspection_date'], reverse=True)
        
        # æ„å»ºå“åº”æ•°æ®
        unit_info = {
            'unit_id': str(unit_result.get('_id', '')),
            'unit_name': unit_result.get('unit_name', ''),
            'address': unit_result.get('address', ''),
            'legal_representative': unit_result.get('legal_representative', ''),
            'credit_code': unit_result.get('credit_code', ''),
            'total_inspections': len(inspection_history),
            'match_summary': unit_result.get('match_summary', {}),
            'inspection_history': inspection_history
        }
        
        return safe_json_response({
            'success': True,
            'unit_info': unit_info
        })
        
    except Exception as e:
        logger.error(f"è·å–å•ä½æ£€æŸ¥å†å²å¤±è´¥: {str(e)}")
        return safe_json_response({
            'success': False,
            'error': str(e)
        }, status_code=500)


@app.route('/multi_results')
def multi_results_page():
    """ä¸€å¯¹å¤šåŒ¹é…ç»“æœé¡µé¢"""
    try:
        return render_template('multi_results.html')
    except Exception as e:
        logger.error(f"ä¸€å¯¹å¤šåŒ¹é…ç»“æœé¡µé¢åŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


@app.route('/unit_detail/<unit_id>')
def unit_detail_page(unit_id):
    """å•ä½è¯¦æƒ…é¡µé¢"""
    try:
        return render_template('unit_detail.html', unit_id=unit_id)
    except Exception as e:
        logger.error(f"å•ä½è¯¦æƒ…é¡µé¢åŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


# ===== å¢å¼ºå…³è”åŠŸèƒ½APIæ¥å£ =====

@app.route('/enhanced_association')
def enhanced_association_page():
    """å¢å¼ºå…³è”é¡µé¢"""
    try:
        return render_template('enhanced_association.html')
    except Exception as e:
        logger.error(f"å¢å¼ºå…³è”é¡µé¢åŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


@app.route('/csv_upload')
def csv_upload_page():
    """V2.0 CSVæ–‡ä»¶ä¸Šä¼ é¡µé¢"""
    try:
        return render_template('csv_upload.html')
    except Exception as e:
        logger.error(f"CSVä¸Šä¼ é¡µé¢åŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


@app.route('/data_analysis')
def data_analysis_page():
    """V2.0 æ•°æ®åˆ†æé¡µé¢"""
    try:
        file_id = request.args.get('file_id')
        if not file_id:
            return render_template('error.html', error_message='ç¼ºå°‘æ–‡ä»¶IDå‚æ•°'), 400
        return render_template('data_analysis.html', file_id=file_id)
    except Exception as e:
        logger.error(f"æ•°æ®åˆ†æé¡µé¢åŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


@app.route('/field_mapping')
def field_mapping_page():
    """V2.0 å­—æ®µæ˜ å°„é…ç½®é¡µé¢"""
    try:
        file_id = request.args.get('file_id')
        if not file_id:
            return render_template('error.html', error_message='ç¼ºå°‘æ–‡ä»¶IDå‚æ•°'), 400
        return render_template('field_mapping.html', file_id=file_id)
    except Exception as e:
        logger.error(f"å­—æ®µæ˜ å°„é¡µé¢åŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


@app.route('/kg_builder')
def kg_builder_page():
    """V2.0 çŸ¥è¯†å›¾è°±æ„å»ºé¡µé¢"""
    try:
        file_id = request.args.get('file_id')
        if not file_id:
            return render_template('error.html', error_message='ç¼ºå°‘æ–‡ä»¶IDå‚æ•°'), 400
        return render_template('kg_builder.html', file_id=file_id)
    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±æ„å»ºé¡µé¢åŠ è½½å¤±è´¥: {str(e)}")
        return render_template('error.html', error=str(e))


@app.route('/api/start_enhanced_association', methods=['POST'])
def api_start_enhanced_association():
    """å¯åŠ¨å¢å¼ºå…³è”ä»»åŠ¡"""
    try:
        data = request.get_json() or {}
        
        # è·å–å‚æ•°
        strategy = data.get('strategy', AssociationStrategy.HYBRID)
        clear_existing = data.get('clear_existing', False)
        
        # éªŒè¯ç­–ç•¥å‚æ•°
        valid_strategies = [AssociationStrategy.BUILDING_BASED, AssociationStrategy.UNIT_BASED, AssociationStrategy.HYBRID]
        if strategy not in valid_strategies:
            return jsonify({
                'success': False,
                'error': f'æ— æ•ˆçš„å…³è”ç­–ç•¥: {strategy}',
                'valid_strategies': valid_strategies
            }), 400
        
        # å¯åŠ¨ä»»åŠ¡
        task_id = enhanced_association_processor.start_enhanced_association_task(
            strategy=strategy,
            clear_existing=clear_existing
        )
        
        logger.info(f"å¢å¼ºå…³è”ä»»åŠ¡å¯åŠ¨æˆåŠŸ: {task_id}")
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': f'å¢å¼ºå…³è”ä»»åŠ¡å·²å¯åŠ¨ (ç­–ç•¥: {strategy})',
            'parameters': {
                'strategy': strategy,
                'clear_existing': clear_existing
            }
        })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨å¢å¼ºå…³è”ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/enhanced_association_progress/<task_id>')
def api_get_enhanced_association_progress(task_id):
    """API: è·å–å¢å¼ºå…³è”ä»»åŠ¡è¿›åº¦"""
    try:
        progress = enhanced_association_processor.get_association_task_progress(task_id)
        if progress:
            return jsonify({'success': True, 'progress': progress})
        else:
            return jsonify({'success': False, 'error': 'æœªæ‰¾åˆ°ä»»åŠ¡'}), 404
    except Exception as e:
        logger.error(f"è·å–å¢å¼ºå…³è”ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/enhanced_association_statistics')
def api_get_enhanced_association_statistics():
    """è·å–å¢å¼ºå…³è”ç»Ÿè®¡ä¿¡æ¯ - [ä¿®å¤] ç›´æ¥ä»ç»“æœè¡¨èšåˆ"""
    try:
        if not enhanced_association_processor:
            return jsonify({'error': 'å¢å¼ºå…³è”å¤„ç†å™¨æœªåˆå§‹åŒ–'}), 500
        
        # ç›´æ¥è°ƒç”¨é‡æ„åçš„ç»Ÿè®¡æ–¹æ³•
        statistics = enhanced_association_processor.get_association_statistics()
        
        return jsonify({
            'success': True,
            'statistics': statistics,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"è·å–å¢å¼ºå…³è”ç»Ÿè®¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/enhanced_association_results')
def api_get_enhanced_association_results():
    """è·å–å¢å¼ºå…³è”ç»“æœ - [ä¿®å¤] ç›´æ¥ä»ç»“æœè¡¨æŸ¥è¯¢"""
    try:
        page = int(request.args.get('page', 1))
        page_size = int(request.args.get('page_size', 20))
        
        # æ„å»ºæŸ¥è¯¢ï¼ˆè¿™é‡Œå¯ä»¥ä¿æŒä¸å˜ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯ç›´æ¥æŸ¥è¯¢æœ€ç»ˆç»“æœï¼‰
        query = {}
        strategy = request.args.get('strategy')
        if strategy:
            query['association_type'] = strategy # æ³¨æ„å­—æ®µåå¯èƒ½å·²åœ¨èšåˆä¸­æ›´æ”¹
        
        collection = db_manager.get_collection('enhanced_association_results')
        
        total_count = collection.count_documents(query)
        cursor = collection.find(query).skip((page - 1) * page_size).limit(page_size)
        
        results = safe_json_response(list(cursor))
        
        return jsonify({
            'success': True,
            'results': results,
            'pagination': {
                'page': page,
                'page_size': page_size,
                'total_count': total_count,
                'total_pages': (total_count + page_size - 1) // page_size if page_size > 0 else 0
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–å¢å¼ºå…³è”ç»“æœå¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/association_result_detail/<association_id>')
def api_get_association_result_detail(association_id):
    """è·å–å…³è”ç»“æœè¯¦æƒ…"""
    try:
        from bson import ObjectId
        
        collection = db_manager.get_collection('enhanced_association_results')
        
        # æŸ¥æ‰¾è®°å½•
        result = collection.find_one({'association_id': association_id})
        
        if not result:
            return jsonify({
                'success': False,
                'error': 'æœªæ‰¾åˆ°æŒ‡å®šçš„å…³è”ç»“æœ'
            }), 404
        
        # ä½¿ç”¨ safe_json_response ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½èƒ½è¢«æ­£ç¡®åºåˆ—åŒ–
        return jsonify({
            'success': True,
            'result': safe_json_response(result)
        })
        
    except Exception as e:
        logger.error(f"è·å–å…³è”ç»“æœè¯¦æƒ…å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/clear_enhanced_association_results', methods=['POST'])
def api_clear_enhanced_association_results():
    """API: æ¸…ç©ºå¢å¼ºå…³è”ç»“æœ"""
    try:
        if not enhanced_association_processor:
             return jsonify({'success': False, 'error': 'å¤„ç†å™¨æœªåˆå§‹åŒ–'}), 500

        success = enhanced_association_processor._clear_association_results()
        
        if success:
            return jsonify({'success': True, 'message': 'å¢å¼ºå…³è”ç»“æœå·²æˆåŠŸæ¸…é™¤'})
        else:
            return jsonify({'success': False, 'error': 'æ¸…é™¤å¢å¼ºå…³è”ç»“æœå¤±è´¥'})
            
    except Exception as e:
        logger.error(f"æ¸…é™¤å¢å¼ºå…³è”ç»“æœå¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ===== V2.0 CSVæ•°æ®ç®¡ç†APIæ¥å£ =====

@app.route('/api/upload_csv', methods=['POST'])
def api_upload_csv():
    """API: ä¸Šä¼ CSVæ–‡ä»¶"""
    logger.info(f"æ”¶åˆ°CSVä¸Šä¼ è¯·æ±‚ï¼ŒContent-Type: {request.content_type}")
    logger.info(f"è¯·æ±‚æ–‡ä»¶: {list(request.files.keys())}")
    try:
        # æ£€æŸ¥å¤„ç†å™¨æ˜¯å¦åˆå§‹åŒ–
        if not csv_processor:
            return jsonify({'success': False, 'error': 'CSVå¤„ç†å™¨æœªåˆå§‹åŒ–'}), 500
            
        # è¯»å–ä¸Šä¼ é…ç½®
        upload_config = {}
        try:
            upload_config = (config_manager.get_web_config() or {}).get('upload', {})
        except Exception:
            upload_config = {}

        def _parse_size_to_bytes(size_str: str) -> int:
            try:
                s = (size_str or '').strip().lower()
                if s.endswith('gb'):
                    return int(float(s[:-2].strip()) * 1024 * 1024 * 1024)
                if s.endswith('mb'):
                    return int(float(s[:-2].strip()) * 1024 * 1024)
                if s.endswith('kb'):
                    return int(float(s[:-2].strip()) * 1024)
                if s.isdigit():
                    return int(s)
            except Exception:
                pass
            # é»˜è®¤100MB
            return 100 * 1024 * 1024

        allowed_extensions = upload_config.get('allowed_extensions', ['.csv', '.txt', '.xlsx', '.xls'])
        max_size_bytes = _parse_size_to_bytes(upload_config.get('max_file_size', '100MB'))
        upload_folder = upload_config.get('upload_folder', 'uploads')  # å…¼å®¹æ—§é…ç½®ä½†ä¸å†æœ¬åœ°è½ç›˜

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400
            
        file = request.files['file']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'æ–‡ä»¶åä¸ºç©º'}), 400
            
        # éªŒè¯æ–‡ä»¶ç±»å‹
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in allowed_extensions:
            return jsonify({'success': False, 'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼'}), 400
            
        # æ£€æŸ¥æ–‡ä»¶å¤§å° (100MBé™åˆ¶)
        file.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
        file_size = file.tell()
        file.seek(0)  # é‡ç½®åˆ°æ–‡ä»¶å¼€å¤´
        
        if file_size > max_size_bytes:
            max_mb = int(round(max_size_bytes / (1024 * 1024)))
            return jsonify({'success': False, 'error': f'æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆæœ€å¤§{max_mb}MBï¼‰'}), 400
            
        # ç”Ÿæˆæ—¶é—´æˆ³ä¸åŸå§‹æ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        original_filename = file.filename

        # è¯»å–å†…å­˜å†…å®¹
        file_bytes = file.read()

        # æŒ‰ç±»å‹è§£æä¸ºDataFrameï¼ˆä¸è½ç›˜ï¼‰
        from io import BytesIO
        import pandas as _pd

        if file_ext in ['.xlsx', '.xls']:
            df = _pd.read_excel(BytesIO(file_bytes))
            df = csv_processor._basic_cleaning(df)
            parse_meta = {
                'encoding': 'binary',
                'delimiter': ','
            }
            result = {
                'success': True,
                'dataframe': df,
                'metadata': {
                    'file_name': original_filename,
                    'total_rows': len(df),
                    'total_columns': len(df.columns),
                    'columns': df.columns.tolist(),
                },
                'encoding': 'binary',
                'delimiter': ',',
                'row_count': len(df),
                'column_count': len(df.columns),
                'sample_data': csv_processor.get_sample_data(df),
                'warnings': []
            }
        else:
            # CSV/TXT èµ°è§£æå™¨ï¼Œä¸è½ç›˜
            # å…ˆæ ¡éªŒï¼ˆç”¨æ–‡ä»¶ååšè·¯å¾„å‚æ•°ä»¥å¤ç”¨æ‰©å±•åæ ¡éªŒï¼‰
            validation_result = csv_processor.validate_file(original_filename, file_bytes)
            if not validation_result.get('valid'):
                return jsonify({'success': False, 'message': f"æ–‡ä»¶éªŒè¯å¤±è´¥: {', '.join(validation_result.get('errors', []))}"}), 400

            result = csv_processor.parse_csv(file_bytes, original_filename)
            if result.get('success'):
                # è¡¥å…¨ç»Ÿä¸€è¿”å›ç»“æ„
                df = result['data']
                result = {
                    'success': True,
                    'dataframe': df,
                    'metadata': result.get('metadata', {}),
                    'encoding': result.get('metadata', {}).get('encoding', 'utf-8'),
                    'delimiter': result.get('metadata', {}).get('delimiter', ','),
                    'row_count': len(df),
                    'column_count': len(df.columns),
                    'sample_data': csv_processor.get_sample_data(df),
                    'warnings': result.get('warnings', [])
                }
        
        # æ£€æŸ¥å¤„ç†ç»“æœ
        if not result.get('success', False):
            return jsonify({
                'success': False,
                'message': result.get('error', 'CSVæ–‡ä»¶å¤„ç†å¤±è´¥')
            }), 400
        
        # è¿›è¡Œæ•°æ®åˆ†æ
        df = result['dataframe']
        if df is None:
            return jsonify({
                'success': False,
                'message': 'CSVæ–‡ä»¶è§£æå¤±è´¥ï¼Œæ— æ³•è·å–æ•°æ®'
            }), 400
            
        analysis_result = data_analyzer.analyze_dataframe(df)
        
        # è¿›è¡Œæ¨¡å¼æ£€æµ‹
        schema_result = schema_detector.detect_schema(df)
        
        # ç”Ÿæˆé›†åˆåï¼ˆæŒ‰æ–‡ä»¶åï¼‰ä¸æ–‡ä»¶ID
        def derive_collection_name(name: str) -> str:
            import re
            base = os.path.splitext(os.path.basename(name))[0]
            base = base.lower()
            base = re.sub(r'[^a-z0-9_]+', '_', base)
            base = re.sub(r'_+', '_', base).strip('_')
            if not base:
                base = f"csv_{timestamp}"
            return base

        collection_name = derive_collection_name(original_filename)
        file_id = f"csv_{timestamp}_{hash(original_filename) % 10000}"

        # å†™å…¥MongoDBï¼ˆæŒ‰æ–‡ä»¶åå»ºè¡¨/é›†åˆï¼‰ï¼Œè‹¥å·²å­˜åœ¨åˆ™ä½¿ç”¨åˆ«å
        try:
            db = db_manager.get_db()
            existing_names = set(db.list_collection_names())
            requested_collection = collection_name
            alias_used = False
            alias_notice = None
            if collection_name in existing_names:
                # ä½¿ç”¨æ—¶é—´æˆ³åˆ«åï¼Œç¡®ä¿å”¯ä¸€
                alias_candidate = f"{collection_name}_{timestamp}"
                idx = 1
                while alias_candidate in existing_names:
                    alias_candidate = f"{collection_name}_{timestamp}_{idx}"
                    idx += 1
                collection_name = alias_candidate
                alias_used = True
                alias_notice = f"é›†åˆ {requested_collection} å·²å­˜åœ¨ï¼Œå·²ä½¿ç”¨åˆ«å {collection_name}"
            # å­—æ®µåæ¸…æ´—ï¼ˆMongoä¸å…è®¸åŒ…å«'.'æˆ–ä»¥'$'å¼€å¤´ï¼‰
            def _sanitize_field_name(name: str) -> str:
                import re
                n = str(name)
                if n.startswith('$'):
                    n = f"f_{n[1:]}"  # å‰ç¼€æ›¿æ¢
                n = n.replace('.', '_')
                n = re.sub(r'\s+', '_', n).strip('_')
                return n or 'field'

            column_mapping = {col: _sanitize_field_name(col) for col in df.columns}
            if list(column_mapping.values()) != list(df.columns):
                df = df.rename(columns=column_mapping)

            # å°†NaNè½¬ä¸ºNone
            safe_df = df.where(pd.notna(df), None)
            records = safe_df.to_dict('records')
            if records:
                db[collection_name].insert_many(records)
        except Exception as e:
            logger.error(f"å†™å…¥MongoDBé›†åˆå¤±è´¥: {collection_name}, é”™è¯¯: {e}")
            return jsonify({'success': False, 'error': f'å†™å…¥æ•°æ®åº“å¤±è´¥: {str(e)}'}), 500
        
        # æ„å»ºæ–‡ä»¶ä¿¡æ¯ï¼Œé’ˆå¯¹å¤§æ•°æ®é›†ä¼˜åŒ–å“åº”å¤§å°
        is_large_dataset = len(df) > 50000
        
        file_info = {
            'file_id': file_id,
            'filename': original_filename,
            'collection_name': collection_name,
            'requested_collection': requested_collection,
            'alias_used': alias_used,
            'alias_notice': alias_notice,
            'file_size': file_size,
            'upload_time': datetime.now().isoformat(),
            'encoding': result.get('encoding', 'utf-8'),
            'delimiter': result.get('delimiter', ','),
            'row_count': len(df),
            'column_count': len(df.columns),
            'quality_score': analysis_result.get('quality_score', 0),
            'is_large_dataset': is_large_dataset
        }
        
        # å¯¹äºå¤§æ•°æ®é›†ï¼Œå‡å°‘è¯¦ç»†ä¿¡æ¯ä»¥é¿å…å“åº”è¿‡å¤§
        if is_large_dataset:
            file_info.update({
                'columns': df.columns.tolist()[:20],  # åªæ˜¾ç¤ºå‰20åˆ—
                'columns_truncated': len(df.columns) > 20,
                'sample_data': df.head(2).to_dict('records'),  # åªæ˜¾ç¤ºå‰2è¡Œ
                'message': f'å¤§æ•°æ®é›†ï¼ˆ{len(df):,}è¡Œï¼‰ï¼Œéƒ¨åˆ†ä¿¡æ¯å·²çœç•¥ä»¥ä¼˜åŒ–å“åº”é€Ÿåº¦'
            })
        else:
            file_info.update({
                'columns': df.columns.tolist(),
                'data_types': analysis_result.get('field_types', {}),
                'schema_detection': schema_result,
                'sample_data': df.head(5).to_dict('records'),
                'column_mapping': column_mapping
            })
        
        # å¯é€‰ï¼šå­˜å‚¨åˆ°MongoDB
        if db_manager and db_manager.mongo_client:
            try:
                db_manager.mongo_client.get_database().csv_files.insert_one({
                    'file_id': file_id,
                    'file_info': file_info,
                    'created_at': datetime.now()
                })
            except Exception as e:
                logger.warning(f"å­˜å‚¨æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
        
        # è¿”å›å¤„ç†ç»“æœ
        success_message = 'æ–‡ä»¶ä¸Šä¼ å’Œå¯¼å…¥æˆåŠŸ'
        if alias_notice:
            success_message += f'ï¼ˆ{alias_notice}ï¼‰'
        return jsonify({'success': True, 'message': success_message, 'data': file_info})
        
    except Exception as e:
        logger.error(f"CSVæ–‡ä»¶ä¸Šä¼ å¤„ç†å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': f'æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}'}), 500


@app.route('/api/get_file_analysis/<file_id>')
def api_get_file_analysis(file_id):
    """API: è·å–æ–‡ä»¶åˆ†æç»“æœ"""
    try:
        # ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500
            
        file_record = db_manager.mongo_client.get_database().csv_files.find_one(
            {'file_id': file_id}
        )
        
        if not file_record:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
            
        return jsonify({
            'success': True,
            'data': file_record['file_info']
        })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ†æç»“æœå¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/validate_csv_data', methods=['POST'])
def api_validate_csv_data():
    """API: éªŒè¯CSVæ•°æ®"""
    try:
        data = request.get_json() or {}
        file_id = data.get('file_id')
        
        if not file_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶ID'}), 400
            
        if not validation_engine:
            return jsonify({'success': False, 'error': 'éªŒè¯å¼•æ“æœªåˆå§‹åŒ–'}), 500
            
        # è·å–æ–‡ä»¶ä¿¡æ¯
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500
            
        file_record = db_manager.mongo_client.get_database().csv_files.find_one(
            {'file_id': file_id}
        )
        
        if not file_record:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
            
        # ä¼˜å…ˆä»Mongoé›†åˆè¯»å–æ•°æ®
        file_info = file_record.get('file_info', {})
        collection_name = file_info.get('collection_name')
        df = None
        if collection_name:
            try:
                import pandas as _pd
                db = db_manager.get_db()
                docs = list(db[collection_name].find({}))
                if not docs:
                    return jsonify({'success': False, 'error': 'é›†åˆä¸­æ— æ•°æ®'}), 404
                # å»é™¤Mongoçš„_idå­—æ®µ
                for d in docs:
                    d.pop('_id', None)
                df = _pd.DataFrame(docs)
            except Exception as e:
                return jsonify({'success': False, 'error': f'ä»é›†åˆè¯»å–æ•°æ®å¤±è´¥: {str(e)}'}), 500
        else:
            # å…¼å®¹æ—§æ•°æ®ï¼šä»æœ¬åœ°æ–‡ä»¶è·¯å¾„å›é€€
            file_path = file_info.get('file_path')
            if not file_path or not os.path.exists(file_path):
                return jsonify({'success': False, 'error': 'æ— æ³•è¯»å–æ•°æ®ï¼ˆæ— é›†åˆä¸”æ–‡ä»¶ä¸å­˜åœ¨ï¼‰'}), 404
            result = csv_processor.process_file(file_path)
            df = result['dataframe']
        
        # æ‰§è¡Œæ•°æ®éªŒè¯
        validation_result = validation_engine.validate_dataframe(
            df, 
            validation_rules=data.get('validation_rules', {})
        )
        
        return jsonify({
            'success': True,
            'data': validation_result
        })
        
    except Exception as e:
        logger.error(f"CSVæ•°æ®éªŒè¯å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/build_knowledge_graph', methods=['POST'])
def api_build_knowledge_graph():
    """API: æ„å»ºçŸ¥è¯†å›¾è°±"""
    try:
        data = request.get_json() or {}
        file_id = data.get('file_id')
        config = data.get('config', {})
        
        if not file_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶ID'}), 400
            
        # æ£€æŸ¥çŸ¥è¯†å›¾è°±å¤„ç†å™¨ï¼ˆéœ€è¦ä»æ˜¨å¤©çš„æ¨¡å—å¯¼å…¥ï¼‰
        try:
            from src.knowledge_graph.kg_builder import KnowledgeGraphBuilder
            kg_builder = KnowledgeGraphBuilder(db_manager)
        except ImportError as e:
            return jsonify({'success': False, 'error': f'çŸ¥è¯†å›¾è°±æ¨¡å—æœªæ‰¾åˆ°: {str(e)}'}), 500
            
        # è·å–æ–‡ä»¶ä¿¡æ¯
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500
            
        file_record = db_manager.mongo_client.get_database().csv_files.find_one(
            {'file_id': file_id}
        )
        
        if not file_record:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
            
        # è·å–æ–‡ä»¶æ•°æ®
        file_path = file_record['file_info']['file_path']
        if not os.path.exists(file_path):
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å·²ä¸å­˜åœ¨'}), 404
            
        # å¤„ç†æ–‡ä»¶
        result = csv_processor.process_file(file_path)
        df = result['dataframe']
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"kg_build_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(file_id) % 10000}"
        
        # åˆ›å»ºæ„å»ºä»»åŠ¡è®°å½•
        build_task = {
            'task_id': task_id,
            'file_id': file_id,
            'config': config,
            'status': 'starting',
            'progress': 0,
            'current_step': 1,
            'steps': ['æ•°æ®åŠ è½½', 'å®ä½“æŠ½å–', 'å…³ç³»å‘ç°', 'å›¾è°±æ„å»º', 'è´¨é‡éªŒè¯'],
            'processed_records': 0,
            'extracted_entities': 0,
            'discovered_relations': 0,
            'logs': [],
            'created_at': datetime.now(),
            'updated_at': datetime.now()
        }
        
        # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
        db_manager.mongo_client.get_database().kg_build_tasks.insert_one(build_task)
        
        # è¿™é‡Œåº”è¯¥å¯åŠ¨å¼‚æ­¥æ„å»ºä»»åŠ¡ï¼Œç°åœ¨å…ˆè¿”å›æˆåŠŸ
        # åœ¨å®é™…å®ç°ä¸­ï¼Œåº”è¯¥ä½¿ç”¨Celeryæˆ–ç±»ä¼¼çš„ä»»åŠ¡é˜Ÿåˆ—
        
        return jsonify({
            'success': True,
            'message': 'çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡å·²å¯åŠ¨',
            'task_id': task_id
        })
        
    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±æ„å»ºå¯åŠ¨å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500





@app.route('/api/database_tables')
def api_get_database_tables():
    """API: è·å–æ•°æ®åº“ä¸­æ‰€æœ‰å¯ç”¨çš„æ•°æ®è¡¨åˆ—è¡¨"""
    try:
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        db = db_manager.mongo_client.get_database()
        
        # è·å–æ‰€æœ‰é›†åˆåç§°
        collection_names = db.list_collection_names()
        
        # è¿‡æ»¤æ‰ç³»ç»Ÿé›†åˆå’Œä¸´æ—¶é›†åˆ
        excluded_prefixes = ['system.', 'tmp_', 'temp_']
        excluded_collections = ['csv_files', 'match_results', 'tokenization_cache', 'unit_name_cache', 'address_cache', 'match_result_cache']
        
        filtered_collections = []
        for name in collection_names:
            # è·³è¿‡ç³»ç»Ÿé›†åˆå’Œç¼“å­˜é›†åˆ
            if any(name.startswith(prefix) for prefix in excluded_prefixes):
                continue
            if name in excluded_collections:
                continue
                
            try:
                # è·å–é›†åˆåŸºæœ¬ä¿¡æ¯
                collection = db[name]
                doc_count = collection.count_documents({})
                
                # è·å–æ ·æœ¬æ–‡æ¡£æ¥åˆ†æå­—æ®µ
                sample_doc = collection.find_one()
                field_count = len(sample_doc.keys()) if sample_doc else 0
                
                # è·å–é›†åˆåˆ›å»ºä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                collection_info = {
                    'name': name,
                    'display_name': name.replace('_', ' ').title(),
                    'count': doc_count,  # å‰ç«¯æœŸæœ›çš„å­—æ®µå
                    'fields': field_count,  # å‰ç«¯æœŸæœ›çš„å­—æ®µå
                    'document_count': doc_count,  # ä¿ç•™åŸå­—æ®µåå…¼å®¹æ€§
                    'field_count': field_count,  # ä¿ç•™åŸå­—æ®µåå…¼å®¹æ€§
                    'sample_fields': list(sample_doc.keys())[:10] if sample_doc else [],
                    'has_data': doc_count > 0,
                    'type': 'user_data'  # ç”¨æˆ·æ•°æ®è¡¨
                }
                
                # å°è¯•ä»csv_filesé›†åˆè·å–æ›´è¯¦ç»†çš„ä¿¡æ¯
                try:
                    csv_file_info = db.csv_files.find_one({'file_info.collection_name': name})
                    if csv_file_info and 'file_info' in csv_file_info:
                        file_info = csv_file_info['file_info']
                        collection_info.update({
                            'source_filename': file_info.get('filename', name),
                            'upload_time': file_info.get('upload_time'),
                            'file_size': file_info.get('file_size'),
                            'encoding': file_info.get('encoding'),
                            'type': 'imported_csv'
                        })
                except:
                    pass  # å¦‚æœè·å–ä¸åˆ°ä¹Ÿæ²¡å…³ç³»
                
                filtered_collections.append(collection_info)
                
            except Exception as e:
                logger.warning(f"è·å–é›†åˆ {name} ä¿¡æ¯å¤±è´¥: {str(e)}")
                continue
        
        # æŒ‰æ–‡æ¡£æ•°é‡æ’åº
        filtered_collections.sort(key=lambda x: x['count'], reverse=True)
        
        return jsonify({
            'success': True,
            'tables': filtered_collections,
            'total_count': len(filtered_collections)
        })
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/analyze_table', methods=['POST'])
def api_analyze_table():
    """API: åˆ†ææŒ‡å®šæ•°æ®è¡¨"""
    try:
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        data = request.get_json()
        table_name = data.get('table_name')
        options = {
            'include_sample_data': data.get('include_sample_data', True),
            'include_quality_check': data.get('include_quality_check', True),
            'include_field_analysis': data.get('include_field_analysis', True),
            'include_statistics': data.get('include_statistics', True),
            'sample_size': data.get('sample_size', 5000)
        }
        
        if not table_name:
            return jsonify({'success': False, 'error': 'æœªæŒ‡å®šæ•°æ®è¡¨å'}), 400
        
        db = db_manager.mongo_client.get_database()
        collection = db[table_name]
        
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        if table_name not in db.list_collection_names():
            return jsonify({'success': False, 'error': f'æ•°æ®è¡¨ {table_name} ä¸å­˜åœ¨'}), 404
        
        analysis_result = {
            'table_name': table_name,
            'total_records': collection.count_documents({}),
            'field_count': 0,
            'quality_score': 0,
            'completeness': 0
        }
        
        # è·å–æ ·æœ¬æ•°æ®è¿›è¡Œåˆ†æ
        sample_size = min(options['sample_size'], analysis_result['total_records'])
        pipeline = [{'$sample': {'size': sample_size}}] if sample_size > 0 else []
        sample_docs = list(collection.aggregate(pipeline)) if sample_size > 0 else []
        
        if sample_docs:
            # åˆ†æå­—æ®µ
            all_fields = set()
            for doc in sample_docs:
                all_fields.update(doc.keys())
            
            all_fields.discard('_id')  # æ’é™¤MongoDBçš„_idå­—æ®µ
            analysis_result['field_count'] = len(all_fields)
            
            # å­—æ®µåˆ†æ
            if options['include_field_analysis'] and all_fields:
                field_analysis = []
                
                for field in all_fields:
                    field_info = analyze_field(sample_docs, field)
                    field_analysis.append(field_info)
                
                analysis_result['field_analysis'] = field_analysis
                
                # è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°
                if field_analysis:
                    avg_quality = sum(f['quality_score'] for f in field_analysis) / len(field_analysis)
                    analysis_result['quality_score'] = round(avg_quality)
            
            # è®¡ç®—å®Œæ•´æ€§
            if options['include_quality_check']:
                total_fields = len(all_fields) * len(sample_docs)
                non_null_fields = 0
                
                for doc in sample_docs:
                    for field in all_fields:
                        if field in doc and doc[field] is not None and doc[field] != '':
                            non_null_fields += 1
                
                analysis_result['completeness'] = round((non_null_fields / total_fields) * 100) if total_fields > 0 else 0
            
            # æ ·æœ¬æ•°æ®
            if options['include_sample_data']:
                # æ¸…ç†æ ·æœ¬æ•°æ®ï¼Œç§»é™¤_idå­—æ®µ
                clean_sample = []
                for doc in sample_docs[:10]:  # åªè¿”å›å‰10æ¡
                    clean_doc = {k: v for k, v in doc.items() if k != '_id'}
                    clean_sample.append(clean_doc)
                analysis_result['sample_data'] = clean_sample
        
        # æ¸…ç†æ‰€æœ‰NaNå€¼ä»¥ç¡®ä¿JSONåºåˆ—åŒ–æˆåŠŸ
        analysis_result = clean_nan_values(analysis_result)
        
        return jsonify({
            'success': True,
            'analysis': analysis_result
        })
        
    except Exception as e:
        logger.error(f"æ•°æ®è¡¨åˆ†æå¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


def analyze_field(sample_docs, field_name):
    """åˆ†æå•ä¸ªå­—æ®µçš„è¯¦ç»†ä¿¡æ¯"""
    values = []
    non_null_count = 0
    
    for doc in sample_docs:
        value = doc.get(field_name)
        if value is not None and value != '' and not (isinstance(value, float) and math.isnan(value)):
            values.append(value)
            non_null_count += 1
    
    total_count = len(sample_docs)
    non_null_rate = round((non_null_count / total_count) * 100) if total_count > 0 else 0
    unique_count = len(set(str(v) for v in values))
    
    # æ¨æ–­æ•°æ®ç±»å‹
    data_type = infer_data_type(values)
    
    # è®¡ç®—è´¨é‡åˆ†æ•°
    quality_score = calculate_field_quality(values, non_null_rate, unique_count, total_count)
    
    return {
        'field_name': field_name,
        'data_type': data_type,
        'non_null_rate': non_null_rate,
        'unique_count': unique_count,
        'quality_score': quality_score,
        'total_values': total_count
    }


def infer_data_type(values):
    """æ¨æ–­å­—æ®µçš„æ•°æ®ç±»å‹"""
    if not values:
        return 'unknown'
    
    # æ£€æŸ¥æ•°å€¼ç±»å‹
    numeric_count = 0
    date_count = 0
    boolean_count = 0
    
    for value in values[:100]:  # åªæ£€æŸ¥å‰100ä¸ªå€¼
        # è·³è¿‡NaNå€¼
        if isinstance(value, float) and math.isnan(value):
            continue
        if pd.isna(value):
            continue
            
        str_value = str(value).strip()
        
        # æ£€æŸ¥æ•°å­—
        try:
            float(str_value)
            numeric_count += 1
            continue
        except (ValueError, TypeError):
            pass
        
        # æ£€æŸ¥å¸ƒå°”å€¼
        if str_value.lower() in ['true', 'false', '1', '0', 'yes', 'no', 'y', 'n']:
            boolean_count += 1
            continue
        
        # æ£€æŸ¥æ—¥æœŸ
        try:
            import dateutil.parser
            dateutil.parser.parse(str_value)
            date_count += 1
            continue
        except:
            pass
    
    total_checked = min(len(values), 100)
    
    if numeric_count / total_checked > 0.8:
        return 'number'
    elif date_count / total_checked > 0.8:
        return 'date'
    elif boolean_count / total_checked > 0.8:
        return 'boolean'
    else:
        return 'string'


def calculate_field_quality(values, non_null_rate, unique_count, total_count):
    """è®¡ç®—å­—æ®µè´¨é‡åˆ†æ•°"""
    # åŸºç¡€åˆ†æ•°åŸºäºéç©ºç‡
    quality = non_null_rate
    
    # æ ¹æ®å”¯ä¸€æ€§è°ƒæ•´åˆ†æ•°
    if total_count > 0:
        uniqueness_ratio = unique_count / total_count
        
        # å¦‚æœæ‰€æœ‰å€¼éƒ½ç›¸åŒï¼Œé™ä½è´¨é‡åˆ†æ•°
        if uniqueness_ratio < 0.01:
            quality *= 0.7
        # å¦‚æœæœ‰åˆç†çš„é‡å¤ï¼Œè¿™æ˜¯å¥½çš„
        elif 0.01 <= uniqueness_ratio <= 0.8:
            quality *= 1.1
        # å¦‚æœå‡ ä¹æ‰€æœ‰å€¼éƒ½å”¯ä¸€ï¼Œå¯èƒ½æ˜¯IDå­—æ®µï¼Œä¿æŒåŸåˆ†æ•°
    
    return min(100, round(quality))


def clean_nan_values(obj):
    """æ¸…ç†å¯¹è±¡ä¸­çš„NaNå€¼ï¼Œä½¿å…¶å¯ä»¥JSONåºåˆ—åŒ–"""
    if isinstance(obj, dict):
        return {k: clean_nan_values(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_nan_values(item) for item in obj]
    elif isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):
        return None
    elif pd.isna(obj):  # å¤„ç†pandasçš„NaNå€¼
        return None
    else:
        return obj


@app.route('/standalone_data_analysis')
def standalone_data_analysis():
    """ç‹¬ç«‹æ•°æ®åˆ†æé¡µé¢"""
    return render_template('standalone_data_analysis.html')


@app.route('/user_driven_field_mapping')
def user_driven_field_mapping():
    """ç”¨æˆ·ä¸»å¯¼çš„å­—æ®µæ˜ å°„é¡µé¢"""
    return render_template('user_driven_field_mapping.html')


@app.route('/api/get_table_fields', methods=['POST'])
def api_get_table_fields():
    """API: è·å–æŒ‡å®šæ•°æ®è¡¨çš„å­—æ®µä¿¡æ¯"""
    try:
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        data = request.get_json()
        
        # æ”¯æŒå•è¡¨å’Œå¤šè¡¨æŸ¥è¯¢
        table_name = data.get('table_name')
        table_names = data.get('table_names', [])
        
        if table_name:
            table_names = [table_name]
        
        if not table_names:
            return jsonify({'success': False, 'error': 'æœªæŒ‡å®šæ•°æ®è¡¨å'}), 400
        
        db = db_manager.mongo_client.get_database()
        tables_info = []
        
        for table_name in table_names:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if table_name not in db.list_collection_names():
                return jsonify({'success': False, 'error': f'æ•°æ®è¡¨ {table_name} ä¸å­˜åœ¨'}), 404
            
            collection = db[table_name]
            
            # è·å–æ ·æœ¬æ–‡æ¡£æ¥åˆ†æå­—æ®µ
            sample_docs = list(collection.find().limit(100))
            if not sample_docs:
                tables_info.append({
                    'table_name': table_name,
                    'fields': [],
                    'sample_count': 0,
                    'total_count': 0
                })
                continue
            
            # åˆ†æå­—æ®µä¿¡æ¯
            all_fields = set()
            for doc in sample_docs:
                all_fields.update(doc.keys())
            
            all_fields.discard('_id')  # æ’é™¤MongoDBçš„_idå­—æ®µ
            
            # è¯¦ç»†åˆ†ææ¯ä¸ªå­—æ®µ
            field_info = []
            for field in sorted(all_fields):
                field_analysis = analyze_field_details(sample_docs, field)
                field_info.append(field_analysis)
            
            tables_info.append({
                'table_name': table_name,
                'display_name': table_name.replace('_', ' ').title(),
                'fields': field_info,
                'sample_count': len(sample_docs),
                'total_count': collection.count_documents({})
            })
        
        # å¦‚æœæ˜¯å•è¡¨æŸ¥è¯¢ï¼Œç›´æ¥è¿”å›å­—æ®µä¿¡æ¯
        if len(tables_info) == 1 and data.get('table_name'):
            return jsonify({
                'success': True,
                'fields': tables_info[0]['fields'],
                'table_info': tables_info[0]
            })
        else:
            return jsonify({
                'success': True,
                'tables': tables_info
            })
        
    except Exception as e:
        logger.error(f"è·å–è¡¨å­—æ®µä¿¡æ¯å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/suggest_field_mappings', methods=['POST'])
def api_suggest_field_mappings():
    """API: æ™ºèƒ½å­—æ®µæ˜ å°„å»ºè®®"""
    try:
        data = request.get_json()
        source_table = data.get('source_table')
        target_table = data.get('target_table')
        source_fields = data.get('source_fields', [])
        target_fields = data.get('target_fields', [])
        
        if not all([source_table, target_table, source_fields, target_fields]):
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        # æ™ºèƒ½å­—æ®µåŒ¹é…ç®—æ³•
        suggestions = generate_field_mapping_suggestions(
            source_fields, target_fields, source_table, target_table
        )
        
        return jsonify({
            'success': True,
            'suggestions': suggestions,
            'source_table': source_table,
            'target_table': target_table
        })
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆå­—æ®µæ˜ å°„å»ºè®®å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/save_field_mapping', methods=['POST'])
def api_save_field_mapping():
    """API: ä¿å­˜å­—æ®µæ˜ å°„é…ç½®"""
    try:
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        data = request.get_json()
        source_table = data.get('source_table')
        target_tables = data.get('target_tables', [])
        mappings = data.get('mappings', [])
        
        if not all([source_table, target_tables, mappings]):
            return jsonify({'success': False, 'error': 'æ˜ å°„é…ç½®ä¸å®Œæ•´'}), 400
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        db = db_manager.mongo_client.get_database()
        config_collection = db['field_mapping_configs']
        
        # åˆ›å»ºé…ç½®æ–‡æ¡£
        config_doc = {
            'config_id': f"{source_table}_mapping_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'source_table': source_table,
            'target_tables': target_tables,
            'mappings': mappings,
            'created_at': datetime.now().isoformat(),  # ä½¿ç”¨ISOæ ¼å¼å­—ç¬¦ä¸²
            'status': 'active'
        }
        
        result = config_collection.insert_one(config_doc)
        
        if result.inserted_id:
            return jsonify({
                'success': True,
                'config_id': config_doc['config_id'],
                'message': 'å­—æ®µæ˜ å°„é…ç½®ä¿å­˜æˆåŠŸ'
            })
        else:
            return jsonify({'success': False, 'error': 'é…ç½®ä¿å­˜å¤±è´¥'}), 500
            
    except Exception as e:
        logger.error(f"ä¿å­˜å­—æ®µæ˜ å°„é…ç½®å¤±è´¥: {str(e)}")
        logger.error(f"è¯·æ±‚æ•°æ®: {data}")
        import traceback
        logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/preview_field_mapping', methods=['POST'])
def api_preview_field_mapping():
    """API: é¢„è§ˆå­—æ®µæ˜ å°„ç»“æœ"""
    try:
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        data = request.get_json()
        source_table = data.get('source_table')
        target_table = data.get('target_table')
        field_mappings = data.get('field_mappings', [])
        preview_limit = data.get('preview_limit', 10)
        
        if not all([source_table, target_table, field_mappings]):
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        db = db_manager.mongo_client.get_database()
        
        # è·å–æºè¡¨å’Œç›®æ ‡è¡¨çš„æ ·æœ¬æ•°æ®
        source_collection = db[source_table]
        target_collection = db[target_table]
        
        source_docs = list(source_collection.find().limit(preview_limit))
        target_docs = list(target_collection.find().limit(preview_limit))
        
        # æ‰§è¡Œå­—æ®µæ˜ å°„é¢„è§ˆ
        preview_result = execute_field_mapping_preview(
            source_docs, target_docs, field_mappings, preview_limit
        )
        
        return jsonify({
            'success': True,
            'preview': preview_result,
            'source_table': source_table,
            'target_table': target_table,
            'mapping_count': len(field_mappings)
        })
        
    except Exception as e:
        logger.error(f"é¢„è§ˆå­—æ®µæ˜ å°„å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500





def analyze_field_details(sample_docs, field_name):
    """åˆ†æå­—æ®µçš„è¯¦ç»†ä¿¡æ¯"""
    import math
    
    values = []
    non_null_count = 0
    
    for doc in sample_docs:
        value = doc.get(field_name)
        if value is not None and value != '':
            # å¤„ç†NaNå€¼
            if isinstance(value, float) and (math.isnan(value) or math.isinf(value)):
                continue
            values.append(value)
            non_null_count += 1
    
    total_count = len(sample_docs)
    non_null_rate = round((non_null_count / total_count) * 100) if total_count > 0 else 0
    unique_count = len(set(str(v) for v in values))
    
    # æ¨æ–­æ•°æ®ç±»å‹
    data_type = infer_data_type(values)
    
    # è·å–ç¤ºä¾‹å€¼ï¼Œç¡®ä¿åºåˆ—åŒ–å®‰å…¨
    sample_values = []
    for v in values[:5]:
        if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):
            continue
        sample_values.append(str(v))
    
    return {
        'name': field_name,  # å‰ç«¯æœŸæœ›çš„å­—æ®µå
        'type': data_type,   # å‰ç«¯æœŸæœ›çš„å­—æ®µå
        'field_name': field_name,  # ä¿ç•™åŸå­—æ®µåå…¼å®¹æ€§
        'data_type': data_type,    # ä¿ç•™åŸå­—æ®µåå…¼å®¹æ€§
        'non_null_rate': non_null_rate,
        'unique_count': unique_count,
        'total_values': total_count,
        'sample_values': sample_values,
        'is_key_field': unique_count == non_null_count if non_null_count > 0 else False  # åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯ä¸»é”®å­—æ®µ
    }


def generate_field_mapping_suggestions(source_fields, target_fields, source_table, target_table):
    """ç”Ÿæˆæ™ºèƒ½å­—æ®µæ˜ å°„å»ºè®®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    try:
        # å¯¼å…¥å¢å¼ºå­—æ®µæ˜ å°„å™¨
        from src.matching.enhanced_field_mapper import EnhancedFieldMapper
        
        # åˆå§‹åŒ–å¢å¼ºæ˜ å°„å™¨
        enhanced_mapper = EnhancedFieldMapper()
        
        # ä½¿ç”¨å¢å¼ºç®—æ³•ç”Ÿæˆå»ºè®®
        suggestions = enhanced_mapper.generate_intelligent_mapping_suggestions(
            source_fields, target_fields, max_suggestions=3
        )
        
        return suggestions
        
    except ImportError as e:
        logger.warning(f"å¢å¼ºå­—æ®µæ˜ å°„å™¨å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ç®—æ³•: {str(e)}")
        # å›é€€åˆ°åŸå§‹ç®—æ³•
        return _generate_basic_field_mapping_suggestions(source_fields, target_fields, source_table, target_table)
    except Exception as e:
        logger.error(f"å¢å¼ºå­—æ®µæ˜ å°„å¤±è´¥: {str(e)}")
        # å›é€€åˆ°åŸå§‹ç®—æ³•
        return _generate_basic_field_mapping_suggestions(source_fields, target_fields, source_table, target_table)


def _generate_basic_field_mapping_suggestions(source_fields, target_fields, source_table, target_table):
    """ç”ŸæˆåŸºç¡€å­—æ®µæ˜ å°„å»ºè®®ï¼ˆåŸå§‹ç®—æ³•ä½œä¸ºå›é€€ï¼‰"""
    suggestions = []
    
    for source_field in source_fields:
        source_name = source_field['field_name'].lower()
        source_type = source_field['data_type']
        
        best_matches = []
        
        for target_field in target_fields:
            target_name = target_field['field_name'].lower()
            target_type = target_field['data_type']
            
            # è®¡ç®—å­—æ®µåç§°ç›¸ä¼¼åº¦
            name_similarity = calculate_field_name_similarity(source_name, target_name)
            
            # æ•°æ®ç±»å‹å…¼å®¹æ€§æ£€æŸ¥
            type_compatibility = check_data_type_compatibility(source_type, target_type)
            
            # ç»¼åˆè¯„åˆ†
            overall_score = (name_similarity * 0.7) + (type_compatibility * 0.3)
            
            if overall_score > 0.3:  # åªä¿ç•™ç›¸ä¼¼åº¦è¾ƒé«˜çš„å»ºè®®
                best_matches.append({
                    'target_field': target_field['field_name'],
                    'target_type': target_type,
                    'name_similarity': round(name_similarity, 3),
                    'type_compatibility': round(type_compatibility, 3),
                    'overall_score': round(overall_score, 3),
                    'confidence': 'high' if overall_score > 0.8 else 'medium' if overall_score > 0.6 else 'low'
                })
        
        # æŒ‰è¯„åˆ†æ’åºï¼Œå–å‰3ä¸ªå»ºè®®
        best_matches.sort(key=lambda x: x['overall_score'], reverse=True)
        
        suggestions.append({
            'source_field': source_field['field_name'],
            'source_type': source_type,
            'suggested_mappings': best_matches[:3]
        })
    
    return suggestions


def calculate_field_name_similarity(name1, name2):
    """è®¡ç®—å­—æ®µåç§°ç›¸ä¼¼åº¦"""
    # ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦è®¡ç®—
    if name1 == name2:
        return 1.0
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åŒçš„å…³é”®è¯
    name1_parts = name1.replace('_', ' ').split()
    name2_parts = name2.replace('_', ' ').split()
    
    common_parts = set(name1_parts) & set(name2_parts)
    total_parts = set(name1_parts) | set(name2_parts)
    
    if total_parts:
        return len(common_parts) / len(total_parts)
    
    return 0.0


def check_data_type_compatibility(type1, type2):
    """æ£€æŸ¥æ•°æ®ç±»å‹å…¼å®¹æ€§"""
    if type1 == type2:
        return 1.0
    
    # æ•°å€¼ç±»å‹å…¼å®¹æ€§
    numeric_types = {'number', 'integer', 'float'}
    if type1 in numeric_types and type2 in numeric_types:
        return 0.8
    
    # å­—ç¬¦ä¸²ç±»å‹å¯ä»¥ä¸å¤§éƒ¨åˆ†ç±»å‹å…¼å®¹
    if type1 == 'string' or type2 == 'string':
        return 0.6
    
    # æ—¥æœŸç±»å‹å…¼å®¹æ€§
    date_types = {'date', 'datetime', 'timestamp'}
    if type1 in date_types and type2 in date_types:
        return 0.9
    
    return 0.2  # ä¸å…¼å®¹çš„ç±»å‹


def execute_field_mapping_preview(source_docs, target_docs, field_mappings, limit):
    """æ‰§è¡Œå­—æ®µæ˜ å°„é¢„è§ˆ"""
    preview_results = []
    
    # åˆ›å»ºæ˜ å°„å­—å…¸
    mapping_dict = {}
    for mapping in field_mappings:
        mapping_dict[mapping['source_field']] = mapping['target_field']
    
    # é¢„è§ˆæ˜ å°„ç»“æœ
    for i, source_doc in enumerate(source_docs[:limit]):
        mapped_doc = {}
        mapping_info = {}
        
        for source_field, target_field in mapping_dict.items():
            source_value = source_doc.get(source_field)
            mapped_doc[target_field] = source_value
            mapping_info[target_field] = {
                'source_field': source_field,
                'source_value': source_value,
                'mapped_successfully': source_value is not None
            }
        
        preview_results.append({
            'index': i + 1,
            'source_doc': {k: v for k, v in source_doc.items() if k != '_id'},
            'mapped_doc': mapped_doc,
            'mapping_info': mapping_info
        })
    
    # è®¡ç®—æ˜ å°„ç»Ÿè®¡ä¿¡æ¯
    mapping_stats = calculate_mapping_statistics(preview_results, field_mappings)
    
    return {
        'preview_data': preview_results,
        'statistics': mapping_stats,
        'total_previewed': len(preview_results)
    }


def calculate_mapping_statistics(preview_results, field_mappings):
    """è®¡ç®—æ˜ å°„ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        'total_mappings': len(field_mappings),
        'successful_mappings': 0,
        'field_coverage': {},
        'data_quality': {}
    }
    
    if not preview_results:
        return stats
    
    # ç»Ÿè®¡æ¯ä¸ªå­—æ®µçš„æ˜ å°„æˆåŠŸç‡
    for mapping in field_mappings:
        target_field = mapping['target_field']
        successful_count = 0
        
        for result in preview_results:
            if result['mapping_info'].get(target_field, {}).get('mapped_successfully'):
                successful_count += 1
        
        success_rate = successful_count / len(preview_results)
        stats['field_coverage'][target_field] = {
            'success_rate': round(success_rate, 3),
            'successful_count': successful_count,
            'total_count': len(preview_results)
        }
        
        if success_rate > 0.8:
            stats['successful_mappings'] += 1
    
    # è®¡ç®—æ•´ä½“æ•°æ®è´¨é‡è¯„åˆ†
    if stats['total_mappings'] > 0:
        overall_success_rate = stats['successful_mappings'] / stats['total_mappings']
        stats['data_quality']['overall_score'] = round(overall_success_rate * 100)
        stats['data_quality']['quality_level'] = (
            'excellent' if overall_success_rate > 0.9 else
            'good' if overall_success_rate > 0.7 else
            'fair' if overall_success_rate > 0.5 else
            'poor'
        )
    
    return stats


@app.route('/standalone_field_mapping')
def standalone_field_mapping():
    """ç‹¬ç«‹å­—æ®µæ˜ å°„é¡µé¢"""
    return render_template('standalone_field_mapping.html')


@app.route('/api/kg_build_config', methods=['POST'])
def api_kg_build_config():
    """API: çŸ¥è¯†å›¾è°±æ„å»ºé…ç½®"""
    try:
        data = request.get_json()
        table_names = data.get('table_names', [])
        build_options = data.get('build_options', {})
        
        if not table_names:
            return jsonify({'success': False, 'error': 'æœªæŒ‡å®šæ•°æ®è¡¨'}), 400
        
        # åˆ†ææ•°æ®è¡¨ï¼Œç”Ÿæˆæ„å»ºé…ç½®å»ºè®®
        config_suggestions = generate_kg_build_config(table_names, build_options)
        
        return jsonify({
            'success': True,
            'config_suggestions': config_suggestions,
            'table_names': table_names
        })
        
    except Exception as e:
        logger.error(f"ç”ŸæˆçŸ¥è¯†å›¾è°±æ„å»ºé…ç½®å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/start_kg_build', methods=['POST'])
def api_start_kg_build():
    """API: å¯åŠ¨çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡"""
    try:
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        data = request.get_json()
        table_names = data.get('table_names', [])
        build_config = data.get('build_config', {})
        project_name = data.get('project_name', f'KG_Project_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
        
        if not table_names:
            return jsonify({'success': False, 'error': 'æœªæŒ‡å®šæ•°æ®è¡¨'}), 400
        
        # åˆ›å»ºçŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡
        task_id = start_kg_build_task(table_names, build_config, project_name)
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'project_name': project_name,
            'table_count': len(table_names),
            'status': 'started'
        })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/kg_build_progress/<task_id>')
def api_kg_build_progress(task_id):
    """API: æŸ¥è¯¢çŸ¥è¯†å›¾è°±æ„å»ºè¿›åº¦"""
    try:
        progress_info = get_kg_build_progress(task_id)
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'progress': progress_info
        })
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ„å»ºè¿›åº¦å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/kg_build_result/<task_id>')
def api_kg_build_result(task_id):
    """API: è·å–çŸ¥è¯†å›¾è°±æ„å»ºç»“æœ"""
    try:
        build_result = get_kg_build_result(task_id)
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'result': build_result
        })
        
    except Exception as e:
        logger.error(f"è·å–æ„å»ºç»“æœå¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


def generate_kg_build_config(table_names, build_options):
    """ç”ŸæˆçŸ¥è¯†å›¾è°±æ„å»ºé…ç½®å»ºè®®"""
    try:
        db = db_manager.mongo_client.get_database()
        config_suggestions = {
            'entity_extraction': {
                'enabled': True,
                'methods': ['field_based', 'pattern_based'],
                'confidence_threshold': 0.6
            },
            'relation_extraction': {
                'enabled': True,
                'methods': ['field_mapping', 'semantic_analysis'],
                'confidence_threshold': 0.5
            },
            'build_options': {
                'batch_size': 1000,
                'enable_validation': True,
                'auto_merge_entities': True,
                'create_indexes': True
            },
            'table_analysis': []
        }
        
        # åˆ†ææ¯ä¸ªæ•°æ®è¡¨
        for table_name in table_names:
            if table_name not in db.list_collection_names():
                continue
                
            collection = db[table_name]
            sample_docs = list(collection.find().limit(100))
            
            if not sample_docs:
                continue
            
            # åˆ†æå­—æ®µç‰¹å¾
            all_fields = set()
            for doc in sample_docs:
                all_fields.update(doc.keys())
            all_fields.discard('_id')
            
            # è¯†åˆ«æ½œåœ¨çš„å®ä½“å­—æ®µ
            entity_fields = []
            relation_fields = []
            
            for field in all_fields:
                field_analysis = analyze_field_for_kg(sample_docs, field)
                
                if field_analysis['is_entity_candidate']:
                    entity_fields.append({
                        'field_name': field,
                        'entity_type': field_analysis['suggested_entity_type'],
                        'confidence': field_analysis['confidence']
                    })
                
                if field_analysis['is_relation_candidate']:
                    relation_fields.append({
                        'field_name': field,
                        'relation_type': field_analysis['suggested_relation_type'],
                        'confidence': field_analysis['confidence']
                    })
            
            config_suggestions['table_analysis'].append({
                'table_name': table_name,
                'total_records': collection.count_documents({}),
                'total_fields': len(all_fields),
                'entity_fields': entity_fields,
                'relation_fields': relation_fields,
                'recommended_config': {
                    'primary_entity_field': entity_fields[0]['field_name'] if entity_fields else None,
                    'key_relation_fields': [f['field_name'] for f in relation_fields[:3]]
                }
            })
        
        return config_suggestions
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ„å»ºé…ç½®å¤±è´¥: {str(e)}")
        return {}


def analyze_field_for_kg(sample_docs, field_name):
    """åˆ†æå­—æ®µæ˜¯å¦é€‚åˆä½œä¸ºå®ä½“æˆ–å…³ç³»"""
    values = [doc.get(field_name) for doc in sample_docs if doc.get(field_name) is not None]
    
    if not values:
        return {
            'is_entity_candidate': False,
            'is_relation_candidate': False,
            'confidence': 0.0
        }
    
    # åŸºæœ¬ç»Ÿè®¡
    unique_count = len(set(str(v) for v in values))
    total_count = len(values)
    uniqueness_ratio = unique_count / total_count if total_count > 0 else 0
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºå®ä½“å€™é€‰
    is_entity_candidate = False
    suggested_entity_type = 'Generic'
    entity_confidence = 0.0
    
    # é«˜å”¯ä¸€æ€§å­—æ®µå¯èƒ½æ˜¯å®ä½“
    if uniqueness_ratio > 0.7:
        is_entity_candidate = True
        entity_confidence = min(uniqueness_ratio, 0.9)
        
        # æ ¹æ®å­—æ®µåæ¨æ–­å®ä½“ç±»å‹
        field_lower = field_name.lower()
        if any(keyword in field_lower for keyword in ['name', 'åç§°', 'å§“å']):
            suggested_entity_type = 'Person' if 'å§“å' in field_lower else 'Organization'
            entity_confidence = min(entity_confidence + 0.1, 1.0)
        elif any(keyword in field_lower for keyword in ['id', 'ç¼–å·', 'ä»£ç ']):
            suggested_entity_type = 'Identifier'
        elif any(keyword in field_lower for keyword in ['åœ°å€', 'address', 'ä½ç½®']):
            suggested_entity_type = 'Location'
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºå…³ç³»å€™é€‰
    is_relation_candidate = False
    suggested_relation_type = 'hasProperty'
    relation_confidence = 0.0
    
    # ä¸­ç­‰å”¯ä¸€æ€§å­—æ®µå¯èƒ½è¡¨ç¤ºå…³ç³»
    if 0.1 < uniqueness_ratio < 0.7:
        is_relation_candidate = True
        relation_confidence = 1.0 - abs(uniqueness_ratio - 0.4) * 2
        
        # æ ¹æ®å­—æ®µåæ¨æ–­å…³ç³»ç±»å‹
        field_lower = field_name.lower()
        if any(keyword in field_lower for keyword in ['ç±»å‹', 'type', 'åˆ†ç±»']):
            suggested_relation_type = 'hasType'
        elif any(keyword in field_lower for keyword in ['çŠ¶æ€', 'status', 'æƒ…å†µ']):
            suggested_relation_type = 'hasStatus'
        elif any(keyword in field_lower for keyword in ['å±äº', 'belongs', 'éš¶å±']):
            suggested_relation_type = 'belongsTo'
    
    return {
        'is_entity_candidate': is_entity_candidate,
        'suggested_entity_type': suggested_entity_type,
        'is_relation_candidate': is_relation_candidate,
        'suggested_relation_type': suggested_relation_type,
        'confidence': max(entity_confidence, relation_confidence),
        'uniqueness_ratio': uniqueness_ratio,
        'sample_values': values[:5]
    }


def start_kg_build_task(table_names, build_config, project_name):
    """å¯åŠ¨çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡"""
    import uuid
    import threading
    
    task_id = str(uuid.uuid4())
    
    # åˆ›å»ºä»»åŠ¡è®°å½•
    task_record = {
        'task_id': task_id,
        'project_name': project_name,
        'table_names': table_names,
        'build_config': build_config,
        'status': 'started',
        'progress': 0,
        'start_time': datetime.now().isoformat(),
        'current_stage': 'initializing',
        'results': {}
    }
    
    # ä¿å­˜ä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“
    db = db_manager.mongo_client.get_database()
    db.kg_build_tasks.insert_one(task_record)
    
    # å¯åŠ¨åå°æ„å»ºä»»åŠ¡
    build_thread = threading.Thread(
        target=execute_kg_build_task,
        args=(task_id, table_names, build_config, project_name)
    )
    build_thread.daemon = True
    build_thread.start()
    
    return task_id


def execute_kg_build_task(task_id, table_names, build_config, project_name):
    """æ‰§è¡ŒçŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡"""
    try:
        db = db_manager.mongo_client.get_database()
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        def update_task_progress(stage, progress, details=None):
            update_data = {
                'current_stage': stage,
                'progress': progress,
                'updated_time': datetime.now().isoformat()
            }
            if details:
                update_data['stage_details'] = details
            
            db.kg_build_tasks.update_one(
                {'task_id': task_id},
                {'$set': update_data}
            )
        
        # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±æ„å»ºå™¨
        from src.knowledge_graph.kg_store import KnowledgeGraphStore
        from src.knowledge_graph.kg_builder import KnowledgeGraphBuilder
        
        kg_store = KnowledgeGraphStore(db_manager)
        kg_builder = KnowledgeGraphBuilder(kg_store, build_config)
        
        total_tables = len(table_names)
        build_results = []
        
        for i, table_name in enumerate(table_names):
            current_progress = int((i / total_tables) * 100)
            update_task_progress(f'processing_table_{i+1}', current_progress, {
                'current_table': table_name,
                'table_index': i + 1,
                'total_tables': total_tables
            })
            
            # è·å–æ•°æ®è¡¨æ•°æ®
            collection = db[table_name]
            documents = list(collection.find())
            
            if not documents:
                logger.warning(f"æ•°æ®è¡¨ {table_name} ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                continue
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(documents)
            if '_id' in df.columns:
                df = df.drop('_id', axis=1)
            
            # æ„å»ºçŸ¥è¯†å›¾è°±
            table_result = kg_builder.build_knowledge_graph_from_dataframe(
                df, table_name, project_name
            )
            
            build_results.append(table_result)
        
        # å®Œæˆæ„å»º
        final_result = {
            'project_name': project_name,
            'total_tables_processed': len(build_results),
            'total_entities': sum(r.get('entities_created', 0) for r in build_results),
            'total_triples': sum(r.get('triples_created', 0) for r in build_results),
            'table_results': build_results,
            'build_summary': generate_build_summary(build_results)
        }
        
        # æ›´æ–°ä»»åŠ¡å®ŒæˆçŠ¶æ€
        db.kg_build_tasks.update_one(
            {'task_id': task_id},
            {'$set': {
                'status': 'completed',
                'progress': 100,
                'current_stage': 'completed',
                'end_time': datetime.now().isoformat(),
                'results': final_result
            }}
        )
        
        logger.info(f"çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡å®Œæˆ: {task_id}")
        
    except Exception as e:
        error_msg = f"çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        
        # æ›´æ–°ä»»åŠ¡å¤±è´¥çŠ¶æ€
        db.kg_build_tasks.update_one(
            {'task_id': task_id},
            {'$set': {
                'status': 'failed',
                'current_stage': 'error',
                'error_message': error_msg,
                'end_time': datetime.now().isoformat()
            }}
        )


def generate_build_summary(build_results):
    """ç”Ÿæˆæ„å»ºæ‘˜è¦"""
    if not build_results:
        return {}
    
    total_entities = sum(r.get('entities_created', 0) for r in build_results)
    total_triples = sum(r.get('triples_created', 0) for r in build_results)
    successful_tables = len([r for r in build_results if r.get('status') == 'completed'])
    
    return {
        'total_entities_created': total_entities,
        'total_triples_created': total_triples,
        'successful_tables': successful_tables,
        'total_tables': len(build_results),
        'success_rate': (successful_tables / len(build_results)) * 100 if build_results else 0,
        'avg_entities_per_table': total_entities / len(build_results) if build_results else 0,
        'avg_triples_per_table': total_triples / len(build_results) if build_results else 0
    }


def get_kg_build_progress(task_id):
    """è·å–çŸ¥è¯†å›¾è°±æ„å»ºè¿›åº¦"""
    try:
        db = db_manager.mongo_client.get_database()
        task_record = db.kg_build_tasks.find_one({'task_id': task_id})
        
        if not task_record:
            return {'error': 'Task not found'}
        
        return {
            'task_id': task_id,
            'status': task_record.get('status', 'unknown'),
            'progress': task_record.get('progress', 0),
            'current_stage': task_record.get('current_stage', 'unknown'),
            'stage_details': task_record.get('stage_details', {}),
            'start_time': task_record.get('start_time'),
            'updated_time': task_record.get('updated_time'),
            'error_message': task_record.get('error_message')
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")
        return {'error': str(e)}


def get_kg_build_result(task_id):
    """è·å–çŸ¥è¯†å›¾è°±æ„å»ºç»“æœ"""
    try:
        db = db_manager.mongo_client.get_database()
        task_record = db.kg_build_tasks.find_one({'task_id': task_id})
        
        if not task_record:
            return {'error': 'Task not found'}
        
        if task_record.get('status') != 'completed':
            return {
                'error': 'Task not completed',
                'current_status': task_record.get('status', 'unknown')
            }
        
        return task_record.get('results', {})
        
    except Exception as e:
        logger.error(f"è·å–æ„å»ºç»“æœå¤±è´¥: {str(e)}")
        return {'error': str(e)}


@app.route('/standalone_kg_builder')
def standalone_kg_builder():
    """ç‹¬ç«‹çŸ¥è¯†å›¾è°±æ„å»ºé¡µé¢"""
    return render_template('standalone_kg_builder.html')


@app.route('/dynamic_matching')
def dynamic_matching():
    """åŠ¨æ€åŒ¹é…é¡µé¢ - åŸºäºç”¨æˆ·å­—æ®µæ˜ å°„é…ç½®çš„æ™ºèƒ½åŒ¹é…"""
    return render_template('dynamic_matching.html')


@app.route('/user_data_matching')
def user_data_matching():
    """ç”¨æˆ·æ•°æ®æ™ºèƒ½åŒ¹é…é¡µé¢ - ä¸“é—¨å¤„ç†ç”¨æˆ·ä¸Šä¼ æ•°æ®çš„åŒ¹é…"""
    return render_template('user_data_matching.html')


@app.route('/api/analyze_multi_table_relationships', methods=['POST'])
def api_analyze_multi_table_relationships():
    """API: åˆ†æå¤šè¡¨å…³ç³»"""
    try:
        data = request.get_json()
        
        if not data or 'table_names' not in data:
            return jsonify({'success': False, 'error': 'è¯·æ±‚æ•°æ®æ— æ•ˆï¼Œéœ€è¦table_nameså­—æ®µ'}), 400
        
        table_names = data['table_names']
        if not isinstance(table_names, list) or len(table_names) < 2:
            return jsonify({'success': False, 'error': 'éœ€è¦è‡³å°‘2ä¸ªè¡¨è¿›è¡Œå…³ç³»åˆ†æ'}), 400
        
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        db = db_manager.mongo_client.get_database()
        
        # è·å–è¡¨ç»“æ„ä¿¡æ¯
        table_schemas = []
        for table_name in table_names:
            if table_name not in db.list_collection_names():
                return jsonify({'success': False, 'error': f'è¡¨ {table_name} ä¸å­˜åœ¨'}), 400
            
            # è·å–å­—æ®µä¿¡æ¯
            sample_docs = list(db[table_name].find().limit(100))
            if not sample_docs:
                continue
            
            fields = []
            for field_name in sample_docs[0].keys():
                if field_name == '_id':
                    continue
                
                field_info = analyze_field_details(sample_docs, field_name)
                fields.append({
                    'field_name': field_name,
                    'data_type': field_info['data_type'],
                    'non_null_rate': field_info['non_null_rate'],
                    'unique_count': field_info['unique_count'],
                    'is_key_field': field_info['is_key_field']
                })
            
            table_schemas.append({
                'table_name': table_name,
                'fields': fields,
                'record_count': len(sample_docs)
            })
        
        # ä½¿ç”¨å¢å¼ºå­—æ®µæ˜ å°„å™¨åˆ†æå¤šè¡¨å…³ç³»
        try:
            from src.matching.enhanced_field_mapper import EnhancedFieldMapper
            enhanced_mapper = EnhancedFieldMapper()
            
            relationships = enhanced_mapper.analyze_multi_table_relationships(table_schemas)
            
            return jsonify({
                'success': True,
                'table_count': len(table_schemas),
                'relationships': relationships,
                'analysis_summary': {
                    'potential_joins': len(relationships['suggested_join_conditions']),
                    'similar_field_pairs': sum(len(item['similar_fields']) for item in relationships['similar_fields_across_tables']),
                    'analyzed_tables': [schema['table_name'] for schema in table_schemas]
                }
            })
            
        except ImportError:
            # å›é€€åˆ°åŸºç¡€åˆ†æ
            basic_relationships = _analyze_basic_table_relationships(table_schemas)
            return jsonify({
                'success': True,
                'table_count': len(table_schemas),
                'relationships': basic_relationships,
                'analysis_type': 'basic',
                'message': 'ä½¿ç”¨åŸºç¡€ç®—æ³•è¿›è¡Œåˆ†æ'
            })
        
    except Exception as e:
        logger.error(f"å¤šè¡¨å…³ç³»åˆ†æå¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


def _analyze_basic_table_relationships(table_schemas):
    """åŸºç¡€å¤šè¡¨å…³ç³»åˆ†æï¼ˆå›é€€ç®—æ³•ï¼‰"""
    relationships = {
        'similar_fields_across_tables': [],
        'suggested_join_conditions': []
    }
    
    # ç®€å•çš„å­—æ®µååŒ¹é…
    for i, table1 in enumerate(table_schemas):
        for j, table2 in enumerate(table_schemas):
            if i >= j:
                continue
            
            similar_fields = []
            for field1 in table1['fields']:
                for field2 in table2['fields']:
                    name_similarity = calculate_field_name_similarity(
                        field1['field_name'].lower(),
                        field2['field_name'].lower()
                    )
                    
                    if name_similarity > 0.7:
                        similar_fields.append({
                            'field1': field1['field_name'],
                            'field2': field2['field_name'],
                            'similarity': name_similarity
                        })
            
            if similar_fields:
                relationships['similar_fields_across_tables'].append({
                    'table1': table1['table_name'],
                    'table2': table2['table_name'],
                    'similar_fields': similar_fields
                })
    
    return relationships


# ====== çŸ¥è¯†å›¾è°±å¢å¼ºAPIç«¯ç‚¹ ======

@app.route('/api/kg_entity_extract', methods=['POST'])
def api_kg_entity_extract():
    """å®ä½“æŠ½å–API"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'}), 400
        
        # æ”¯æŒä¸¤ç§æ¨¡å¼ï¼šä»è¡¨æ ¼æŠ½å–æˆ–ä»æ–‡æœ¬æŠ½å–
        if 'table_name' in data:
            # ä»æ•°æ®è¡¨æŠ½å–å®ä½“
            table_name = data['table_name']
            
            # è·å–è¡¨æ•°æ®
            collection = db_manager.get_collection(table_name)
            if not collection:
                return jsonify({'error': f'æ•°æ®è¡¨ {table_name} ä¸å­˜åœ¨'}), 404
            
            # è½¬æ¢ä¸ºDataFrame
            documents = list(collection.find().limit(1000))  # é™åˆ¶å¤„ç†æ•°é‡
            if not documents:
                return jsonify({'error': 'æ•°æ®è¡¨ä¸ºç©º'}), 400
            
            df = pd.DataFrame(documents)
            
            # æŠ½å–å®ä½“
            entities = entity_extractor.extract_entities_from_dataframe(df, table_name)
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = entity_extractor.get_entity_extraction_statistics()
            
            return jsonify({
                'success': True,
                'entities_count': len(entities),
                'entities': [entity.to_dict() for entity in entities[:50]],  # é™åˆ¶è¿”å›æ•°é‡
                'statistics': stats,
                'table_name': table_name
            })
        
        elif 'text' in data:
            # ä»æ–‡æœ¬æŠ½å–å®ä½“
            text = data['text']
            context = data.get('context', {})
            
            entities = entity_extractor.extract_entities_from_text(text, context)
            
            return jsonify({
                'success': True,
                'entities_count': len(entities),
                'entities': [entity.to_dict() for entity in entities],
                'text_length': len(text)
            })
        
        else:
            return jsonify({'error': 'è¯·æä¾› table_name æˆ– text å‚æ•°'}), 400
    
    except Exception as e:
        logger.error(f"å®ä½“æŠ½å–APIå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/kg_relation_extract', methods=['POST'])
def api_kg_relation_extract():
    """å…³ç³»æŠ½å–API"""
    try:
        data = request.get_json()
        
        if not data or 'table_name' not in data:
            return jsonify({'error': 'è¯·æä¾› table_name å‚æ•°'}), 400
        
        table_name = data['table_name']
        
        # è·å–è¡¨æ•°æ®
        collection = db_manager.get_collection(table_name)
        if not collection:
            return jsonify({'error': f'æ•°æ®è¡¨ {table_name} ä¸å­˜åœ¨'}), 404
        
        documents = list(collection.find().limit(1000))
        if not documents:
            return jsonify({'error': 'æ•°æ®è¡¨ä¸ºç©º'}), 400
        
        df = pd.DataFrame(documents)
        
        # é¦–å…ˆæŠ½å–å®ä½“
        entities = entity_extractor.extract_entities_from_dataframe(df, table_name)
        
        if not entities:
            return jsonify({'error': 'æœªæ‰¾åˆ°å®ä½“ï¼Œæ— æ³•æŠ½å–å…³ç³»'}), 400
        
        # æŠ½å–å…³ç³»
        triples = relation_extractor.extract_relations_from_dataframe(df, entities, table_name)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = relation_extractor.get_relation_extraction_statistics()
        
        return jsonify({
            'success': True,
            'entities_count': len(entities),
            'relations_count': len(triples),
            'relations': [triple.to_dict() for triple in triples[:50]],
            'statistics': stats,
            'table_name': table_name
        })
    
    except Exception as e:
        logger.error(f"å…³ç³»æŠ½å–APIå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/kg_quality_assessment', methods=['GET', 'POST'])
def api_kg_quality_assessment():
    """çŸ¥è¯†å›¾è°±è´¨é‡è¯„ä¼°API"""
    try:
        if request.method == 'POST':
            # æ‰§è¡Œå®Œæ•´è´¨é‡è¯„ä¼°
            quality_report = kg_quality_assessor.assess_overall_quality()
            return jsonify({
                'success': True,
                'assessment_type': 'full_assessment',
                'report': quality_report
            })
        
        else:
            # GETè¯·æ±‚ï¼šè·å–è´¨é‡ç»Ÿè®¡ä¿¡æ¯
            stats = kg_quality_assessor.get_quality_statistics()
            
            # è·å–è´¨é‡è¶‹åŠ¿ï¼ˆè¿‡å»7å¤©ï¼‰
            trends = kg_quality_assessor.get_quality_trends(days=7)
            
            return jsonify({
                'success': True,
                'assessment_type': 'statistics',
                'statistics': stats,
                'trends': trends
            })
    
    except Exception as e:
        logger.error(f"è´¨é‡è¯„ä¼°APIå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/kg_export', methods=['GET'])
def api_kg_export():
    """çŸ¥è¯†å›¾è°±å¯¼å‡ºAPI"""
    try:
        export_format = request.args.get('format', 'json').lower()
        limit = int(request.args.get('limit', 1000))
        
        # è·å–å®ä½“å’Œå…³ç³»
        entities = kg_store.find_entities(limit=limit)
        triples = kg_store.find_triples(limit=limit)
        
        if export_format == 'json':
            export_data = {
                'metadata': {
                    'export_time': datetime.now().isoformat(),
                    'entities_count': len(entities),
                    'triples_count': len(triples),
                    'format': 'json'
                },
                'entities': [entity.to_dict() for entity in entities],
                'triples': [triple.to_dict() for triple in triples]
            }
            
            return jsonify({
                'success': True,
                'export_data': export_data
            })
        
        elif export_format == 'rdf':
            # ç®€åŒ–çš„RDFå¯¼å‡º
            rdf_triples = []
            for triple in triples:
                rdf_triple = {
                    'subject': triple.subject.label if triple.subject else 'unknown',
                    'predicate': triple.predicate.type.value if triple.predicate else 'unknown',
                    'object': triple.object.label if triple.object else 'unknown'
                }
                rdf_triples.append(rdf_triple)
            
            return jsonify({
                'success': True,
                'format': 'rdf',
                'triples': rdf_triples
            })
        
        elif export_format == 'csv':
            # CSVæ ¼å¼å¯¼å‡ºä¸‰å…ƒç»„
            csv_data = []
            for triple in triples:
                csv_data.append({
                    'subject': triple.subject.label if triple.subject else '',
                    'subject_type': triple.subject.type.value if triple.subject else '',
                    'predicate': triple.predicate.type.value if triple.predicate else '',
                    'object': triple.object.label if triple.object else '',
                    'object_type': triple.object.type.value if triple.object else '',
                    'confidence': triple.confidence or 0.0,
                    'source': triple.source or ''
                })
            
            return jsonify({
                'success': True,
                'format': 'csv',
                'data': csv_data
            })
        
        else:
            return jsonify({'error': f'ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {export_format}'}), 400
    
    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±å¯¼å‡ºAPIå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/kg_search', methods=['GET'])
def api_kg_search():
    """çŸ¥è¯†å›¾è°±æœç´¢API"""
    try:
        query = request.args.get('q', '').strip()
        search_type = request.args.get('type', 'entity').lower()
        limit = int(request.args.get('limit', 50))
        
        if not query:
            return jsonify({'error': 'è¯·æä¾›æœç´¢æŸ¥è¯¢å‚æ•° q'}), 400
        
        if search_type == 'entity':
            # æœç´¢å®ä½“
            entities = kg_store.find_entities_by_text(query, limit=limit)
            
            return jsonify({
                'success': True,
                'search_type': 'entity',
                'query': query,
                'results_count': len(entities),
                'results': [entity.to_dict() for entity in entities]
            })
        
        elif search_type == 'relation':
            # æœç´¢é«˜ç½®ä¿¡åº¦å…³ç³»
            triples = kg_store.find_high_confidence_triples(min_confidence=0.7, limit=limit)
            
            # è¿‡æ»¤åŒ…å«æŸ¥è¯¢è¯çš„å…³ç³»
            filtered_triples = []
            query_lower = query.lower()
            
            for triple in triples:
                if (triple.subject and query_lower in triple.subject.label.lower()) or \
                   (triple.object and query_lower in triple.object.label.lower()) or \
                   (triple.predicate and query_lower in triple.predicate.type.value.lower()):
                    filtered_triples.append(triple)
            
            return jsonify({
                'success': True,
                'search_type': 'relation',
                'query': query,
                'results_count': len(filtered_triples),
                'results': [triple.to_dict() for triple in filtered_triples[:limit]]
            })
        
        else:
            return jsonify({'error': f'ä¸æ”¯æŒçš„æœç´¢ç±»å‹: {search_type}'}), 400
    
    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±æœç´¢APIå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/kg_statistics', methods=['GET'])
def api_kg_statistics():
    """çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯API"""
    try:
        # è·å–å­˜å‚¨ç»Ÿè®¡
        storage_stats = kg_store.get_statistics()
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        performance_stats = kg_store.get_performance_stats()
        
        # è·å–è´¨é‡ç»Ÿè®¡
        quality_stats = kg_quality_assessor.get_quality_statistics()
        
        return jsonify({
            'success': True,
            'timestamp': datetime.now().isoformat(),
            'storage_statistics': storage_stats,
            'performance_statistics': performance_stats,
            'quality_statistics': quality_stats
        })
    
    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±ç»Ÿè®¡APIå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/kg_optimize', methods=['POST'])
def api_kg_optimize():
    """çŸ¥è¯†å›¾è°±ä¼˜åŒ–API"""
    try:
        data = request.get_json() or {}
        operation = data.get('operation', 'collections').lower()
        
        if operation == 'collections':
            # ä¼˜åŒ–é›†åˆæ€§èƒ½
            results = kg_store.optimize_collections()
            
            return jsonify({
                'success': True,
                'operation': 'collections_optimization',
                'results': results
            })
        
        elif operation == 'cache':
            # æ¸…ç†ç¼“å­˜
            kg_store.clear_cache()
            
            return jsonify({
                'success': True,
                'operation': 'cache_cleared',
                'message': 'ç¼“å­˜å·²æ¸…ç†'
            })
        
        else:
            return jsonify({'error': f'ä¸æ”¯æŒçš„ä¼˜åŒ–æ“ä½œ: {operation}'}), 400
    
    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±ä¼˜åŒ–APIå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500


# ====== åŠ¨æ€åŒ¹é…ç›¸å…³API ======

@app.route('/api/get_field_mapping_config', methods=['GET'])
def api_get_field_mapping_config():
    """API: è·å–å­—æ®µæ˜ å°„é…ç½®"""
    try:
        config_id = request.args.get('config_id')
        if not config_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘é…ç½®IDå‚æ•°'}), 400
        
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        db = db_manager.mongo_client.get_database()
        config_collection = db['field_mapping_configs']
        
        config = config_collection.find_one({'config_id': config_id})
        
        if not config:
            return jsonify({'success': False, 'error': 'é…ç½®ä¸å­˜åœ¨'}), 404
        
        # ç§»é™¤MongoDBçš„_idå­—æ®µ
        if '_id' in config:
            del config['_id']
        
        return jsonify({
            'success': True,
            'config': config
        })
        
    except Exception as e:
        logger.error(f"è·å–å­—æ®µæ˜ å°„é…ç½®å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/start_dynamic_matching', methods=['POST'])
def api_start_dynamic_matching():
    """API: å¯åŠ¨åŸºäºé…ç½®çš„åŠ¨æ€åŒ¹é…ä»»åŠ¡"""
    try:
        data = request.get_json()
        config_id = data.get('config_id')
        match_type = data.get('match_type', 'both')
        batch_size = data.get('batch_size', 100)
        similarity_threshold = data.get('similarity_threshold', 0.7)
        max_results = data.get('max_results', 10)
        
        if not config_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘é…ç½®ID'}), 400
        
        # è·å–å­—æ®µæ˜ å°„é…ç½®
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        db = db_manager.mongo_client.get_database()
        config_collection = db['field_mapping_configs']
        config = config_collection.find_one({'config_id': config_id})
        
        if not config:
            return jsonify({'success': False, 'error': 'æ˜ å°„é…ç½®ä¸å­˜åœ¨'}), 404
        
        # åˆ›å»ºåŒ¹é…ä»»åŠ¡
        task_id = f"dynamic_match_{config_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        task_doc = {
            'task_id': task_id,
            'config_id': config_id,
            'source_table': config['source_table'],
            'target_tables': config['target_tables'],
            'mappings': config['mappings'],
            'match_type': match_type,
            'batch_size': batch_size,
            'similarity_threshold': similarity_threshold,
            'max_results': max_results,
            'status': 'running',
            'progress': {
                'percentage': 0,
                'processed': 0,
                'total': 0,
                'matches': 0
            },
            'created_at': datetime.now().isoformat(),
            'started_at': datetime.now().isoformat()
        }
        
        # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
        task_collection = db['dynamic_matching_tasks']
        task_collection.insert_one(task_doc)
        
        # å¯åŠ¨åå°åŒ¹é…ä»»åŠ¡ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ç”¨çº¿ç¨‹æ± ï¼‰
        import threading
        thread = threading.Thread(
            target=execute_dynamic_matching_task, 
            args=(task_id, config)
        )
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'åŠ¨æ€åŒ¹é…ä»»åŠ¡å·²å¯åŠ¨'
        })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨åŠ¨æ€åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/matching_progress', methods=['GET'])
def api_matching_progress():
    """API: è·å–åŒ¹é…ä»»åŠ¡è¿›åº¦"""
    try:
        task_id = request.args.get('task_id')
        if not task_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ä»»åŠ¡ID'}), 400
        
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        db = db_manager.mongo_client.get_database()
        task_collection = db['dynamic_matching_tasks']
        
        task = task_collection.find_one({'task_id': task_id})
        
        if not task:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
        
        return jsonify({
            'success': True,
            'progress': task['progress'],
            'status': task['status']
        })
        
    except Exception as e:
        logger.error(f"è·å–åŒ¹é…è¿›åº¦å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ====== ç”¨æˆ·æ•°æ®æ™ºèƒ½åŒ¹é…API ======

@app.route('/api/start_user_data_matching', methods=['POST'])
def api_start_user_data_matching():
    """API: å¯åŠ¨ç”¨æˆ·æ•°æ®æ™ºèƒ½åŒ¹é…ä»»åŠ¡"""
    try:
        data = request.get_json()
        config_id = data.get('config_id')
        algorithm_type = data.get('algorithm_type', 'enhanced')
        similarity_threshold = data.get('similarity_threshold', 0.7)
        batch_size = data.get('batch_size', 100)
        max_results = data.get('max_results', 10)
        
        if not config_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘é…ç½®ID'}), 400
        
        # è·å–å­—æ®µæ˜ å°„é…ç½®
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        db = db_manager.mongo_client.get_database()
        config_collection = db['field_mapping_configs']
        config = config_collection.find_one({'config_id': config_id})
        
        if not config:
            return jsonify({'success': False, 'error': 'æ˜ å°„é…ç½®ä¸å­˜åœ¨'}), 404
        
        # åˆ›å»ºåŒ¹é…ä»»åŠ¡
        task_id = f"user_match_{config_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        task_doc = {
            'task_id': task_id,
            'config_id': config_id,
            'source_table': config['source_table'],
            'target_tables': config['target_tables'],
            'mappings': config['mappings'],
            'algorithm_type': algorithm_type,
            'similarity_threshold': similarity_threshold,
            'batch_size': batch_size,
            'max_results': max_results,
            'status': 'running',
            'progress': {
                'percentage': 0,
                'processed': 0,
                'total': 0,
                'matches': 0,
                'current_operation': 'å‡†å¤‡å¯åŠ¨...'
            },
            'created_at': datetime.now().isoformat(),
            'started_at': datetime.now().isoformat()
        }
        
        # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
        task_collection = db['user_matching_tasks']
        task_collection.insert_one(task_doc)
        
        # åˆå§‹åŒ–ç”¨æˆ·æ•°æ®åŒ¹é…å™¨
        from src.matching.user_data_matcher import UserDataMatcher
        user_matcher = UserDataMatcher(db_manager=db_manager, config=config)
        
        # å¯åŠ¨åŒ¹é…ä»»åŠ¡
        user_matcher.start_matching_task(task_doc)
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'ç”¨æˆ·æ•°æ®æ™ºèƒ½åŒ¹é…ä»»åŠ¡å·²å¯åŠ¨'
        })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨ç”¨æˆ·æ•°æ®åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/user_matching_progress', methods=['GET'])
def api_user_matching_progress():
    """API: è·å–ç”¨æˆ·æ•°æ®åŒ¹é…ä»»åŠ¡è¿›åº¦"""
    try:
        task_id = request.args.get('task_id')
        if not task_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ä»»åŠ¡ID'}), 400
        
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        db = db_manager.mongo_client.get_database()
        task_collection = db['user_matching_tasks']
        
        task = task_collection.find_one({'task_id': task_id})
        
        if not task:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
        
        return jsonify({
            'success': True,
            'progress': task.get('progress', {}),
            'status': task.get('status', 'unknown'),
            'error': task.get('error'),
            'created_at': task.get('created_at'),
            'updated_at': task.get('updated_at')
        })
        
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ·åŒ¹é…è¿›åº¦å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/preview_user_data_matching', methods=['POST'])
def api_preview_user_data_matching():
    """API: é¢„è§ˆç”¨æˆ·æ•°æ®åŒ¹é…ç»“æœ"""
    try:
        data = request.get_json()
        config_id = data.get('config_id')
        preview_count = data.get('preview_count', 5)
        algorithm_type = data.get('algorithm_type', 'enhanced')
        similarity_threshold = data.get('similarity_threshold', 0.7)
        
        if not config_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘é…ç½®ID'}), 400
        
        # åˆå§‹åŒ–ç”¨æˆ·æ•°æ®åŒ¹é…å™¨
        from src.matching.user_data_matcher import UserDataMatcher
        user_matcher = UserDataMatcher(db_manager=db_manager)
        
        # æ‰§è¡Œé¢„è§ˆåŒ¹é…
        preview_results = user_matcher.preview_matching(
            config_id=config_id,
            preview_count=preview_count,
            algorithm_type=algorithm_type,
            similarity_threshold=similarity_threshold
        )
        
        return jsonify({
            'success': True,
            'preview_results': preview_results,
            'count': len(preview_results)
        })
        
    except Exception as e:
        logger.error(f"é¢„è§ˆç”¨æˆ·æ•°æ®åŒ¹é…å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/stop_user_matching_task', methods=['POST'])
def api_stop_user_matching_task():
    """API: åœæ­¢ç”¨æˆ·æ•°æ®åŒ¹é…ä»»åŠ¡"""
    try:
        task_id = request.args.get('task_id')
        if not task_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ä»»åŠ¡ID'}), 400
        
        # åˆå§‹åŒ–ç”¨æˆ·æ•°æ®åŒ¹é…å™¨
        from src.matching.user_data_matcher import UserDataMatcher
        user_matcher = UserDataMatcher(db_manager=db_manager)
        
        # åœæ­¢ä»»åŠ¡
        success = user_matcher.stop_task(task_id)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'ä»»åŠ¡åœæ­¢è¯·æ±‚å·²å‘é€'
            })
        else:
            return jsonify({'success': False, 'error': 'åœæ­¢ä»»åŠ¡å¤±è´¥'}), 500
        
    except Exception as e:
        logger.error(f"åœæ­¢ç”¨æˆ·åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/get_user_matching_results', methods=['GET'])
def api_get_user_matching_results():
    """API: è·å–ç”¨æˆ·æ•°æ®åŒ¹é…ç»“æœ"""
    try:
        task_id = request.args.get('task_id')
        page = int(request.args.get('page', 1))
        page_size = int(request.args.get('page_size', 10))
        
        if not task_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ä»»åŠ¡ID'}), 400
        
        if not db_manager or not db_manager.mongo_client:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–'}), 500
        
        db = db_manager.mongo_client.get_database()
        
        # è·å–ç»“æœé›†åˆ
        result_collection_name = f'user_match_results_{task_id}'
        if result_collection_name not in db.list_collection_names():
            return jsonify({
                'success': True,
                'results': [],
                'total': 0,
                'statistics': {
                    'total_processed': 0,
                    'total_matches': 0,
                    'match_rate': 0,
                    'avg_similarity': 0,
                    'high_confidence_count': 0,
                    'execution_time': 0
                }
            })
        
        result_collection = db[result_collection_name]
        
        # è·å–æ€»æ•°
        total_count = result_collection.count_documents({})
        
        # åˆ†é¡µæŸ¥è¯¢ç»“æœ
        skip = (page - 1) * page_size
        results = list(result_collection.find({})
                      .sort([('similarity_score', -1)])
                      .skip(skip)
                      .limit(page_size))
        
        # æ¸…ç†MongoDBçš„ObjectId
        for result in results:
            if '_id' in result:
                del result['_id']
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = _calculate_matching_statistics(result_collection, task_id)
        
        return jsonify({
            'success': True,
            'results': results,
            'total': total_count,
            'page': page,
            'page_size': page_size,
            'statistics': statistics
        })
        
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ·åŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


def _calculate_matching_statistics(result_collection, task_id: str) -> Dict:
    """è®¡ç®—åŒ¹é…ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # åŸºç¡€ç»Ÿè®¡
        total_matches = result_collection.count_documents({})
        
        if total_matches == 0:
            return {
                'total_processed': 0,
                'total_matches': 0,
                'match_rate': 0,
                'avg_similarity': 0,
                'high_confidence_count': 0,
                'execution_time': 0
            }
        
        # èšåˆæŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
        pipeline = [
            {
                '$group': {
                    '_id': None,
                    'avg_similarity': {'$avg': '$similarity_score'},
                    'max_similarity': {'$max': '$similarity_score'},
                    'min_similarity': {'$min': '$similarity_score'},
                    'high_confidence_count': {
                        '$sum': {
                            '$cond': [
                                {'$gte': ['$similarity_score', 0.8]},
                                1, 0
                            ]
                        }
                    }
                }
            }
        ]
        
        stats_result = list(result_collection.aggregate(pipeline))
        stats = stats_result[0] if stats_result else {}
        
        # è·å–ä»»åŠ¡ä¿¡æ¯è®¡ç®—å¤„ç†æ•°é‡å’Œæ‰§è¡Œæ—¶é—´
        db = db_manager.mongo_client.get_database()
        task_collection = db['user_matching_tasks']
        task = task_collection.find_one({'task_id': task_id})
        
        total_processed = 0
        execution_time = 0
        
        if task:
            progress = task.get('progress', {})
            total_processed = progress.get('processed', 0)
            
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            if task.get('created_at') and task.get('updated_at'):
                start_time = datetime.fromisoformat(task['created_at'])
                end_time = datetime.fromisoformat(task['updated_at'])
                execution_time = int((end_time - start_time).total_seconds())
        
        # è®¡ç®—åŒ¹é…ç‡
        match_rate = (total_matches / total_processed * 100) if total_processed > 0 else 0
        
        return {
            'total_processed': total_processed,
            'total_matches': total_matches,
            'match_rate': round(match_rate, 2),
            'avg_similarity': round((stats.get('avg_similarity', 0) * 100), 2),
            'high_confidence_count': stats.get('high_confidence_count', 0),
            'execution_time': execution_time
        }
        
    except Exception as e:
        logger.error(f"è®¡ç®—åŒ¹é…ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        return {
            'total_processed': 0,
            'total_matches': 0,
            'match_rate': 0,
            'avg_similarity': 0,
            'high_confidence_count': 0,
            'execution_time': 0
        }


@app.route('/user_matching_results')
def user_matching_results():
    """ç”¨æˆ·æ•°æ®åŒ¹é…ç»“æœé¡µé¢"""
    return render_template('user_matching_results.html')


def execute_dynamic_matching_task(task_id: str, config: dict):
    """æ‰§è¡ŒåŠ¨æ€åŒ¹é…ä»»åŠ¡ï¼ˆåå°çº¿ç¨‹ï¼‰"""
    try:
        logger.info(f"å¼€å§‹æ‰§è¡ŒåŠ¨æ€åŒ¹é…ä»»åŠ¡: {task_id}")
        
        db = db_manager.mongo_client.get_database()
        task_collection = db['dynamic_matching_tasks']
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        def update_progress(percentage, processed, total, matches, status='running'):
            task_collection.update_one(
                {'task_id': task_id},
                {
                    '$set': {
                        'progress': {
                            'percentage': percentage,
                            'processed': processed,
                            'total': total,
                            'matches': matches
                        },
                        'status': status,
                        'updated_at': datetime.now().isoformat()
                    }
                }
            )
        
        # æ¨¡æ‹ŸåŒ¹é…è¿‡ç¨‹
        source_table = config['source_table']
        target_tables = config['target_tables']
        mappings = config['mappings']
        
        # è·å–æºè¡¨æ•°æ®é‡
        source_collection = db[source_table]
        total_records = source_collection.count_documents({})
        
        update_progress(0, 0, total_records, 0, 'running')
        
        # æ¨¡æ‹Ÿæ‰¹é‡å¤„ç†
        batch_size = 100
        processed = 0
        matches = 0
        
        for i in range(0, total_records, batch_size):
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            import time
            time.sleep(1)
            
            processed += min(batch_size, total_records - i)
            matches += min(batch_size // 2, total_records - i)  # æ¨¡æ‹Ÿ50%åŒ¹é…ç‡
            
            percentage = int((processed / total_records) * 100)
            update_progress(percentage, processed, total_records, matches)
            
            # æ£€æŸ¥æ˜¯å¦è¢«åœæ­¢
            current_task = task_collection.find_one({'task_id': task_id})
            if current_task and current_task.get('status') == 'stopped':
                logger.info(f"åŠ¨æ€åŒ¹é…ä»»åŠ¡è¢«åœæ­¢: {task_id}")
                return
        
        # ä»»åŠ¡å®Œæˆ
        update_progress(100, total_records, total_records, matches, 'completed')
        
        # ä¿å­˜åŒ¹é…ç»“æœåˆ°ç»“æœè¡¨
        result_collection = db[f'dynamic_match_results_{task_id}']
        result_collection.create_index([('source_id', 1)])
        
        logger.info(f"åŠ¨æ€åŒ¹é…ä»»åŠ¡å®Œæˆ: {task_id}, åŒ¹é…æ•°: {matches}")
        
    except Exception as e:
        logger.error(f"æ‰§è¡ŒåŠ¨æ€åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        try:
            db = db_manager.mongo_client.get_database()
            task_collection = db['dynamic_matching_tasks']
            task_collection.update_one(
                {'task_id': task_id},
                {
                    '$set': {
                        'status': 'failed',
                        'error': str(e),
                        'updated_at': datetime.now().isoformat()
                    }
                }
            )
        except:
            pass


if __name__ == '__main__':
    # åˆå§‹åŒ–åº”ç”¨
    create_app()
    
    # å¯åŠ¨å¼€å‘æœåŠ¡å™¨
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=True,
        threaded=True
    )