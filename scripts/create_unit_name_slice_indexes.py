#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¶ˆé˜²å•ä½å»ºç­‘æ•°æ®å…³è”ç³»ç»Ÿ - å•ä½åç§°åˆ‡ç‰‡ç´¢å¼•ä¼˜åŒ–è„šæœ¬
é€šè¿‡åˆ›å»ºå•ä½åç§°çš„N-gramåˆ‡ç‰‡ç´¢å¼•ï¼Œå®ç°æ›´å¿«é€Ÿçš„æ¨¡ç³ŠåŒ¹é…
"""

import pymongo
import re
import jieba
from datetime import datetime
import time
from collections import defaultdict
import json

class UnitNameSliceIndexer:
    def __init__(self):
        """åˆå§‹åŒ–ç´¢å¼•å™¨"""
        self.client = pymongo.MongoClient('mongodb://localhost:27017/')
        self.db = self.client['Unit_Info']
        self.stats = {
            'slice_indexes_created': 0,
            'keyword_indexes_created': 0,
            'processing_time': 0,
            'total_slices': 0
        }
        
        print("ğŸ”§ å•ä½åç§°åˆ‡ç‰‡ç´¢å¼•å™¨åˆå§‹åŒ–å®Œæˆ")
        
    def create_ngram_slices(self, text, n: int = 3) -> set:
        """åˆ›å»ºN-gramåˆ‡ç‰‡"""
        # ç¡®ä¿textæ˜¯å­—ç¬¦ä¸²ç±»å‹å¹¶ä¸”æœ‰æ•ˆ
        if not text:
            return set()
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ£€æŸ¥é•¿åº¦
        text_str = str(text).strip()
        if not text_str or len(text_str) < n:
            return set()
        
        # æ¸…ç†æ–‡æœ¬
        clean_text = re.sub(r'[^\u4e00-\u9fff\w]', '', text_str)
        
        # ç”ŸæˆN-gramåˆ‡ç‰‡
        slices = set()
        for i in range(len(clean_text) - n + 1):
            slice_text = clean_text[i:i+n]
            if len(slice_text) == n:
                slices.add(slice_text)
        
        return slices
    
    def extract_keywords(self, text) -> set:
        """æå–å…³é”®è¯"""
        if not text:
            return set()
        
        # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
        text_str = str(text).strip()
        if not text_str:
            return set()
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(text_str)
        keywords = set()
        
        for word in words:
            word = word.strip()
            # è¿‡æ»¤æ‰é•¿åº¦å°äº2çš„è¯å’Œå¸¸è§åœç”¨è¯
            if len(word) >= 2 and word not in {'æœ‰é™', 'å…¬å¸', 'ä¼ä¸š', 'é›†å›¢', 'å·¥å‚', 'å•†åº—', 'ä¸­å¿ƒ'}:
                keywords.add(word)
        
        return keywords
    
    def create_slice_indexes_for_collection(self, collection_name: str, field_name: str):
        """ä¸ºæŒ‡å®šé›†åˆå’Œå­—æ®µåˆ›å»ºåˆ‡ç‰‡ç´¢å¼•"""
        print(f"\nğŸ“Š ä¸º {collection_name}.{field_name} åˆ›å»ºåˆ‡ç‰‡ç´¢å¼•...")
        
        collection = self.db[collection_name]
        slice_collection_name = f"{collection_name}_name_slices"
        keyword_collection_name = f"{collection_name}_name_keywords"
        
        # åˆ›å»ºåˆ‡ç‰‡ç´¢å¼•é›†åˆ
        slice_collection = self.db[slice_collection_name]
        keyword_collection = self.db[keyword_collection_name]
        
        # æ¸…ç©ºç°æœ‰ç´¢å¼•
        slice_collection.drop()
        keyword_collection.drop()
        
        # æ‰¹é‡å¤„ç†æ•°æ®
        batch_size = 1000
        total_processed = 0
        slice_data = []
        keyword_data = []
        
        cursor = collection.find({field_name: {"$exists": True, "$ne": ""}}, {field_name: 1})
        
        for doc in cursor:
            unit_name = doc.get(field_name, '')
            if not unit_name:
                continue
            
            doc_id = doc['_id']
            
            # ç”Ÿæˆ3-gramåˆ‡ç‰‡
            slices_3 = self.create_ngram_slices(unit_name, 3)
            # ç”Ÿæˆ2-gramåˆ‡ç‰‡ï¼ˆç”¨äºçŸ­åç§°ï¼‰
            slices_2 = self.create_ngram_slices(unit_name, 2)
            # æå–å…³é”®è¯
            keywords = self.extract_keywords(unit_name)
            
            # å‡†å¤‡åˆ‡ç‰‡æ•°æ®
            for slice_text in slices_3.union(slices_2):
                slice_data.append({
                    'slice': slice_text,
                    'doc_id': doc_id,
                    'unit_name': str(unit_name),
                    'slice_type': '3gram' if slice_text in slices_3 else '2gram'
                })
            
            # å‡†å¤‡å…³é”®è¯æ•°æ®
            for keyword in keywords:
                keyword_data.append({
                    'keyword': keyword,
                    'doc_id': doc_id,
                    'unit_name': str(unit_name)
                })
            
            total_processed += 1
            
            # æ‰¹é‡æ’å…¥
            if len(slice_data) >= batch_size:
                if slice_data:
                    slice_collection.insert_many(slice_data)
                slice_data = []
                
            if len(keyword_data) >= batch_size:
                if keyword_data:
                    keyword_collection.insert_many(keyword_data)
                keyword_data = []
            
            if total_processed % 10000 == 0:
                print(f"   å·²å¤„ç†: {total_processed:,} æ¡è®°å½•")
        
        # æ’å…¥å‰©ä½™æ•°æ®
        if slice_data:
            slice_collection.insert_many(slice_data)
        if keyword_data:
            keyword_collection.insert_many(keyword_data)
        
        # åˆ›å»ºç´¢å¼•
        print(f"   åˆ›å»ºåˆ‡ç‰‡ç´¢å¼•...")
        slice_collection.create_index([('slice', 1)])
        slice_collection.create_index([('slice', 1), ('slice_type', 1)])
        
        print(f"   åˆ›å»ºå…³é”®è¯ç´¢å¼•...")
        keyword_collection.create_index([('keyword', 1)])
        keyword_collection.create_index([('keyword', 'text')])
        
        # ç»Ÿè®¡ä¿¡æ¯
        slice_count = slice_collection.count_documents({})
        keyword_count = keyword_collection.count_documents({})
        
        print(f"   âœ… å®Œæˆ - åˆ‡ç‰‡: {slice_count:,} æ¡, å…³é”®è¯: {keyword_count:,} æ¡")
        
        self.stats['slice_indexes_created'] += 1
        self.stats['total_slices'] += slice_count
        
        return {
            'collection': collection_name,
            'field': field_name,
            'slice_count': slice_count,
            'keyword_count': keyword_count,
            'records_processed': total_processed
        }
    
    def create_fast_lookup_indexes(self):
        """åˆ›å»ºå¿«é€ŸæŸ¥æ‰¾ç´¢å¼•"""
        print(f"\nğŸš€ åˆ›å»ºå¿«é€ŸæŸ¥æ‰¾ç´¢å¼•...")
        
        # ä¸ºåŸå§‹é›†åˆåˆ›å»ºå‰ç¼€ç´¢å¼•
        collections_fields = [
            ('xfaqpc_jzdwxx', 'UNIT_NAME'),
            ('xxj_shdwjbxx', 'dwmc')
        ]
        
        for collection_name, field_name in collections_fields:
            collection = self.db[collection_name]
            
            try:
                # åˆ›å»ºå‰ç¼€ç´¢å¼•ï¼ˆç”¨äºå¿«é€Ÿå‰ç¼€åŒ¹é…ï¼‰
                collection.create_index([(field_name, 1)])
                
                # åˆ›å»ºæ–‡æœ¬ç´¢å¼•ï¼ˆç”¨äºå…¨æ–‡æœç´¢ï¼‰
                collection.create_index([(field_name, 'text')], 
                                      default_language='none',  # ç¦ç”¨è¯­è¨€å¤„ç†ä»¥æ”¯æŒä¸­æ–‡
                                      name=f"{field_name}_text_idx")
                
                print(f"   âœ… {collection_name}.{field_name} ç´¢å¼•åˆ›å»ºå®Œæˆ")
                
            except Exception as e:
                if "already exists" not in str(e).lower():
                    print(f"   âš ï¸ {collection_name}.{field_name} ç´¢å¼•åˆ›å»ºè­¦å‘Š: {e}")
    
    def create_similarity_cache_collection(self):
        """åˆ›å»ºç›¸ä¼¼åº¦ç¼“å­˜é›†åˆ"""
        print(f"\nğŸ’¾ åˆ›å»ºç›¸ä¼¼åº¦ç¼“å­˜é›†åˆ...")
        
        cache_collection = self.db['unit_name_similarity_cache']
        
        # åˆ›å»ºç¼“å­˜ç´¢å¼•
        try:
            cache_collection.create_index([('source_name', 1), ('target_name', 1)], unique=True)
            cache_collection.create_index([('similarity_score', -1)])
            cache_collection.create_index([('created_time', 1)], expireAfterSeconds=7*24*3600)  # 7å¤©è¿‡æœŸ
            
            print(f"   âœ… ç›¸ä¼¼åº¦ç¼“å­˜é›†åˆåˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            if "already exists" not in str(e).lower():
                print(f"   âš ï¸ ç¼“å­˜é›†åˆåˆ›å»ºè­¦å‘Š: {e}")
    
    def test_slice_index_performance(self):
        """æµ‹è¯•åˆ‡ç‰‡ç´¢å¼•æ€§èƒ½"""
        print(f"\nğŸ§ª æµ‹è¯•åˆ‡ç‰‡ç´¢å¼•æ€§èƒ½...")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_names = [
            "ä¸Šæµ·ç›å°”æ–¯åˆ¶å†·è®¾å¤‡æœ‰é™å…¬å¸",
            "ä¸Šæµ·ç”±é¹èµ„äº§ç®¡ç†æœ‰é™å…¬å¸", 
            "ä¸Šæµ·æ‰¬å‘é‡‘å±åˆ¶å“æœ‰é™å…¬å¸"
        ]
        
        for test_name in test_names:
            print(f"\n   æµ‹è¯•å•ä½: {test_name}")
            
            # æµ‹è¯•3-gramåˆ‡ç‰‡æŸ¥è¯¢
            slices = self.create_ngram_slices(test_name, 3)
            if slices:
                slice_collection = self.db['xxj_shdwjbxx_name_slices']
                
                start_time = time.time()
                results = list(slice_collection.find({'slice': {'$in': list(slices)}}).limit(10))
                query_time = time.time() - start_time
                
                print(f"     3-gramåˆ‡ç‰‡æŸ¥è¯¢: {len(results)} æ¡ç»“æœ, è€—æ—¶: {query_time:.4f}ç§’")
            
            # æµ‹è¯•å…³é”®è¯æŸ¥è¯¢
            keywords = self.extract_keywords(test_name)
            if keywords:
                keyword_collection = self.db['xxj_shdwjbxx_name_keywords']
                
                start_time = time.time()
                results = list(keyword_collection.find({'keyword': {'$in': list(keywords)}}).limit(10))
                query_time = time.time() - start_time
                
                print(f"     å…³é”®è¯æŸ¥è¯¢: {len(results)} æ¡ç»“æœ, è€—æ—¶: {query_time:.4f}ç§’")
    
    def run_optimization(self):
        """è¿è¡Œå®Œæ•´ä¼˜åŒ–"""
        start_time = time.time()
        
        print("ğŸ”¥" * 80)
        print("ğŸ”¥ æ¶ˆé˜²å•ä½å»ºç­‘æ•°æ®å…³è”ç³»ç»Ÿ - å•ä½åç§°åˆ‡ç‰‡ç´¢å¼•ä¼˜åŒ–")
        print("ğŸ”¥ é€šè¿‡N-gramåˆ‡ç‰‡å’Œå…³é”®è¯ç´¢å¼•å®ç°è¶…å¿«é€Ÿæ¨¡ç³ŠåŒ¹é…")
        print("ğŸ”¥" * 80)
        
        results = []
        
        # 1. åˆ›å»ºåˆ‡ç‰‡ç´¢å¼•
        print("\nğŸ“Š ç¬¬1æ­¥: åˆ›å»ºå•ä½åç§°åˆ‡ç‰‡ç´¢å¼•")
        collections_fields = [
            ('xfaqpc_jzdwxx', 'UNIT_NAME'),
            ('xxj_shdwjbxx', 'dwmc')
        ]
        
        for collection_name, field_name in collections_fields:
            result = self.create_slice_indexes_for_collection(collection_name, field_name)
            results.append(result)
        
        # 2. åˆ›å»ºå¿«é€ŸæŸ¥æ‰¾ç´¢å¼•
        print("\nğŸ“Š ç¬¬2æ­¥: åˆ›å»ºå¿«é€ŸæŸ¥æ‰¾ç´¢å¼•")
        self.create_fast_lookup_indexes()
        
        # 3. åˆ›å»ºç›¸ä¼¼åº¦ç¼“å­˜
        print("\nğŸ“Š ç¬¬3æ­¥: åˆ›å»ºç›¸ä¼¼åº¦ç¼“å­˜é›†åˆ")
        self.create_similarity_cache_collection()
        
        # 4. æµ‹è¯•æ€§èƒ½
        print("\nğŸ“Š ç¬¬4æ­¥: æµ‹è¯•ç´¢å¼•æ€§èƒ½")
        self.test_slice_index_performance()
        
        # ç»Ÿè®¡ç»“æœ
        self.stats['processing_time'] = time.time() - start_time
        
        print(f"\nğŸ‰ å•ä½åç§°åˆ‡ç‰‡ç´¢å¼•ä¼˜åŒ–å®Œæˆ!")
        print(f"ğŸ“Š ä¼˜åŒ–ç»Ÿè®¡:")
        print(f"   - åˆ‡ç‰‡ç´¢å¼•é›†åˆ: {self.stats['slice_indexes_created']} ä¸ª")
        print(f"   - æ€»åˆ‡ç‰‡æ•°é‡: {self.stats['total_slices']:,} æ¡")
        print(f"   - å¤„ç†æ—¶é—´: {self.stats['processing_time']:.2f} ç§’")
        
        # ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š
        report = {
            'optimization_time': datetime.now().isoformat(),
            'stats': self.stats,
            'results': results,
            'collections_created': [
                'xfaqpc_jzdwxx_name_slices',
                'xfaqpc_jzdwxx_name_keywords',
                'xxj_shdwjbxx_name_slices', 
                'xxj_shdwjbxx_name_keywords',
                'unit_name_similarity_cache'
            ]
        }
        
        with open('unit_name_slice_index_optimization_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: unit_name_slice_index_optimization_report.json")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    indexer = UnitNameSliceIndexer()
    return indexer.run_optimization()

if __name__ == "__main__":
    main() 