#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨ç´¢å¼•æ„å»ºå™¨
ä¸ºä»»æ„å­—æ®µç±»å‹è‡ªåŠ¨åˆ›å»ºé¢„å»ºç´¢å¼•è¡¨ï¼Œæ”¯æŒé«˜æ€§èƒ½æ–‡æœ¬è¯­ä¹‰åˆ†ææŸ¥è¯¢
"""

import logging
import time
from typing import Dict, List, Set, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import pymongo
from pymongo import ASCENDING, TEXT
from .universal_text_matcher import UniversalTextMatcher, FieldType

logger = logging.getLogger(__name__)


class UniversalIndexBuilder:
    """é€šç”¨ç´¢å¼•æ„å»ºå™¨"""
    
    def __init__(self, db_manager):
        """
        åˆå§‹åŒ–é€šç”¨ç´¢å¼•æ„å»ºå™¨
        
        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
        """
        self.db_manager = db_manager
        self.db = db_manager.get_db() if db_manager else None
        self.text_matcher = UniversalTextMatcher(db_manager)
        
        # æ„å»ºé…ç½®
        self.build_config = {
            'batch_size': 1000,
            'max_workers': 8,
            'enable_parallel': True,
            'clear_existing': True,  # æ˜¯å¦æ¸…ç©ºç°æœ‰ç´¢å¼•
            'create_compound_indexes': True,  # æ˜¯å¦åˆ›å»ºå¤åˆç´¢å¼•
            'enable_progress_logging': True
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        self.build_stats = {
            'total_records_processed': 0,
            'total_keywords_created': 0,
            'total_time_spent': 0.0,
            'tables_processed': 0,
            'fields_processed': 0
        }
        
        logger.info("ğŸ—ï¸ é€šç”¨ç´¢å¼•æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def build_field_index(self, table_name: str, field_name: str, 
                         field_type: FieldType = None, force_rebuild: bool = False) -> Dict[str, Any]:
        """
        ä¸ºæŒ‡å®šå­—æ®µæ„å»ºç´¢å¼•
        
        Args:
            table_name: è¡¨å
            field_name: å­—æ®µå
            field_type: å­—æ®µç±»å‹ï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨æ£€æµ‹ï¼‰
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»º
            
        Returns:
            Dict: æ„å»ºç»“æœç»Ÿè®¡
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ—ï¸ å¼€å§‹ä¸ºå­—æ®µæ„å»ºç´¢å¼•: {table_name}.{field_name}")
            
            # æ£€æŸ¥æºè¡¨æ˜¯å¦å­˜åœ¨
            if table_name not in self.db.list_collection_names():
                raise ValueError(f"æºè¡¨ä¸å­˜åœ¨: {table_name}")
            
            source_collection = self.db[table_name]
            
            # è‡ªåŠ¨æ£€æµ‹å­—æ®µç±»å‹
            if field_type is None:
                sample_values = self._get_sample_values(source_collection, field_name, 20)
                field_type = self.text_matcher.detect_field_type(field_name, sample_values)
            
            # æ„å»ºç´¢å¼•è¡¨å
            index_table_name = f"{table_name}_{field_name}_keywords"
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
            if not force_rebuild and self._index_exists_and_valid(index_table_name, table_name, field_name):
                logger.info(f"ç´¢å¼•å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè·³è¿‡æ„å»º: {index_table_name}")
                return {'status': 'skipped', 'reason': 'index_exists'}
            
            # æ¸…ç©ºç°æœ‰ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if self.build_config['clear_existing']:
                self._clear_existing_index(index_table_name, table_name, field_name)
            
            # è·å–å­—æ®µå¤„ç†é…ç½®
            config = self.text_matcher.field_configs.get(field_type, 
                                                       self.text_matcher.field_configs[FieldType.TEXT])
            
            # æ„å»ºç´¢å¼•
            build_result = self._build_index_for_field(
                source_collection, index_table_name, field_name, field_type, config
            )
            
            # åˆ›å»ºç´¢å¼•è¡¨çš„æ•°æ®åº“ç´¢å¼•
            self._create_database_indexes(index_table_name)
            
            # æ›´æ–°ç»Ÿè®¡
            build_time = time.time() - start_time
            self.build_stats['total_time_spent'] += build_time
            self.build_stats['fields_processed'] += 1
            
            result = {
                'status': 'success',
                'table_name': table_name,
                'field_name': field_name,
                'field_type': field_type.value,
                'index_table_name': index_table_name,
                'records_processed': build_result['records_processed'],
                'keywords_created': build_result['keywords_created'],
                'build_time': build_time,
                'processing_rate': build_result['records_processed'] / max(build_time, 0.001)
            }
            
            logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {index_table_name}, "
                       f"å¤„ç† {build_result['records_processed']} æ¡è®°å½•, "
                       f"ç”Ÿæˆ {build_result['keywords_created']} ä¸ªå…³é”®è¯, "
                       f"è€—æ—¶ {build_time:.2f}s, "
                       f"é€Ÿåº¦ {result['processing_rate']:.1f} æ¡/ç§’")
            
            return result
            
        except Exception as e:
            logger.error(f"ç´¢å¼•æ„å»ºå¤±è´¥: {table_name}.{field_name} - {str(e)}")
            return {
                'status': 'error',
                'table_name': table_name,
                'field_name': field_name,
                'error': str(e),
                'build_time': time.time() - start_time
            }
    
    def build_table_indexes(self, table_name: str, field_mappings: List[Dict], 
                           force_rebuild: bool = False) -> Dict[str, Any]:
        """
        ä¸ºè¡¨çš„å¤šä¸ªå­—æ®µæ‰¹é‡æ„å»ºç´¢å¼•
        
        Args:
            table_name: è¡¨å
            field_mappings: å­—æ®µæ˜ å°„åˆ—è¡¨
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»º
            
        Returns:
            Dict: æ‰¹é‡æ„å»ºç»“æœ
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ—ï¸ å¼€å§‹ä¸ºè¡¨æ„å»ºç´¢å¼•: {table_name}, å­—æ®µæ•°é‡: {len(field_mappings)}")
            
            results = {
                'table_name': table_name,
                'total_fields': len(field_mappings),
                'field_results': [],
                'success_count': 0,
                'error_count': 0,
                'skipped_count': 0,
                'total_build_time': 0.0
            }
            
            # æå–éœ€è¦æ„å»ºç´¢å¼•çš„å­—æ®µ
            target_fields = set()
            for mapping in field_mappings:
                if mapping.get('target_table') == table_name:
                    target_fields.add(mapping.get('target_field'))
            
            if not target_fields:
                logger.warning(f"è¡¨ {table_name} æ²¡æœ‰éœ€è¦æ„å»ºç´¢å¼•çš„å­—æ®µ")
                return results
            
            # å¹¶è¡Œæ„å»ºç´¢å¼•
            if self.build_config['enable_parallel'] and len(target_fields) > 1:
                field_results = self._build_indexes_parallel(table_name, target_fields, force_rebuild)
            else:
                field_results = self._build_indexes_sequential(table_name, target_fields, force_rebuild)
            
            # æ±‡æ€»ç»“æœ
            for field_result in field_results:
                results['field_results'].append(field_result)
                if field_result['status'] == 'success':
                    results['success_count'] += 1
                elif field_result['status'] == 'error':
                    results['error_count'] += 1
                else:
                    results['skipped_count'] += 1
            
            results['total_build_time'] = time.time() - start_time
            self.build_stats['tables_processed'] += 1
            
            logger.info(f"âœ… è¡¨ç´¢å¼•æ„å»ºå®Œæˆ: {table_name}, "
                       f"æˆåŠŸ: {results['success_count']}, "
                       f"é”™è¯¯: {results['error_count']}, "
                       f"è·³è¿‡: {results['skipped_count']}, "
                       f"æ€»è€—æ—¶: {results['total_build_time']:.2f}s")
            
            return results
            
        except Exception as e:
            logger.error(f"è¡¨ç´¢å¼•æ„å»ºå¤±è´¥: {table_name} - {str(e)}")
            return {
                'table_name': table_name,
                'status': 'error',
                'error': str(e),
                'total_build_time': time.time() - start_time
            }
    
    def _get_sample_values(self, collection, field_name: str, sample_size: int = 20) -> List[str]:
        """è·å–å­—æ®µæ ·æœ¬å€¼"""
        try:
            pipeline = [
                {'$match': {field_name: {'$exists': True, '$ne': None, '$ne': ''}}},
                {'$sample': {'size': sample_size}},
                {'$project': {field_name: 1}}
            ]
            
            samples = list(collection.aggregate(pipeline))
            return [str(doc.get(field_name, '')) for doc in samples if doc.get(field_name)]
            
        except Exception as e:
            logger.warning(f"è·å–æ ·æœ¬å€¼å¤±è´¥: {field_name} - {str(e)}")
            return []
    
    def _index_exists_and_valid(self, index_table_name: str, source_table: str, field_name: str) -> bool:
        """æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ"""
        try:
            if index_table_name not in self.db.list_collection_names():
                return False
            
            index_collection = self.db[index_table_name]
            
            # æ£€æŸ¥ç´¢å¼•è¡¨æ˜¯å¦æœ‰æ•°æ®
            count = index_collection.count_documents({'field_name': field_name})
            if count == 0:
                return False
            
            # æ£€æŸ¥ç´¢å¼•è¡¨çš„æ•°æ®æ˜¯å¦æ¯”æºè¡¨æ–°
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„ç‰ˆæœ¬æ£€æŸ¥é€»è¾‘
            return True
            
        except Exception as e:
            logger.warning(f"æ£€æŸ¥ç´¢å¼•æœ‰æ•ˆæ€§å¤±è´¥: {index_table_name} - {str(e)}")
            return False
    
    def _clear_existing_index(self, index_table_name: str, source_table: str, field_name: str):
        """æ¸…ç©ºç°æœ‰ç´¢å¼•"""
        try:
            if index_table_name in self.db.list_collection_names():
                # åªæ¸…ç©ºç‰¹å®šå­—æ®µçš„ç´¢å¼•æ•°æ®
                self.db[index_table_name].delete_many({
                    'source_table': source_table,
                    'field_name': field_name
                })
                logger.info(f"å·²æ¸…ç©ºç°æœ‰ç´¢å¼•æ•°æ®: {index_table_name}.{field_name}")
        except Exception as e:
            logger.warning(f"æ¸…ç©ºç´¢å¼•å¤±è´¥: {index_table_name} - {str(e)}")
    
    def _build_index_for_field(self, source_collection, index_table_name: str, 
                              field_name: str, field_type: FieldType, config) -> Dict[str, int]:
        """ä¸ºå•ä¸ªå­—æ®µæ„å»ºç´¢å¼•"""
        index_collection = self.db[index_table_name]
        
        records_processed = 0
        keywords_created = 0
        batch_keywords = []
        
        try:
            # è·å–æ‰€æœ‰æœ‰æ•ˆè®°å½•
            cursor = source_collection.find(
                {field_name: {'$exists': True, '$ne': None, '$ne': ''}},
                {'_id': 1, field_name: 1}
            )
            
            for doc in cursor:
                try:
                    doc_id = doc['_id']
                    field_value = doc.get(field_name)
                    
                    if not field_value:
                        continue
                    
                    # é¢„å¤„ç†å­—æ®µå€¼
                    preprocessed_value = self.text_matcher._apply_preprocessing(field_value, config)
                    if not preprocessed_value:
                        continue
                    
                    # æå–å…³é”®è¯
                    keywords = self.text_matcher._apply_keyword_extraction(preprocessed_value, config)
                    if not keywords:
                        continue
                    
                    # ä¸ºæ¯ä¸ªå…³é”®è¯åˆ›å»ºç´¢å¼•è®°å½•
                    for keyword in keywords:
                        batch_keywords.append({
                            'doc_id': doc_id,
                            'source_table': source_collection.name,
                            'field_name': field_name,
                            'field_type': field_type.value,
                            'keyword': keyword,
                            'original_value': str(field_value),
                            'preprocessed_value': preprocessed_value,
                            'created_at': time.time()
                        })
                        keywords_created += 1
                    
                    records_processed += 1
                    
                    # æ‰¹é‡æ’å…¥
                    if len(batch_keywords) >= self.build_config['batch_size']:
                        index_collection.insert_many(batch_keywords)
                        batch_keywords = []
                        
                        if self.build_config['enable_progress_logging'] and records_processed % 5000 == 0:
                            logger.info(f"ç´¢å¼•æ„å»ºè¿›åº¦: {field_name} - {records_processed} æ¡è®°å½•")
                
                except Exception as e:
                    logger.warning(f"å¤„ç†è®°å½•å¤±è´¥: {doc.get('_id')} - {str(e)}")
                    continue
            
            # æ’å…¥å‰©ä½™çš„å…³é”®è¯
            if batch_keywords:
                index_collection.insert_many(batch_keywords)
            
            # æ›´æ–°å…¨å±€ç»Ÿè®¡
            self.build_stats['total_records_processed'] += records_processed
            self.build_stats['total_keywords_created'] += keywords_created
            
            return {
                'records_processed': records_processed,
                'keywords_created': keywords_created
            }
            
        except Exception as e:
            logger.error(f"æ„å»ºå­—æ®µç´¢å¼•å¤±è´¥: {field_name} - {str(e)}")
            raise
    
    def _create_database_indexes(self, index_table_name: str):
        """ä¸ºç´¢å¼•è¡¨åˆ›å»ºæ•°æ®åº“ç´¢å¼•"""
        try:
            index_collection = self.db[index_table_name]
            
            # åˆ›å»ºå…³é”®è¯ç´¢å¼•ï¼ˆæœ€é‡è¦ï¼‰
            index_collection.create_index([
                ('keyword', ASCENDING),
                ('field_name', ASCENDING)
            ], name='idx_keyword_field', background=True)
            
            # åˆ›å»ºæ–‡æ¡£IDç´¢å¼•
            index_collection.create_index('doc_id', name='idx_doc_id', background=True)
            
            # åˆ›å»ºå¤åˆç´¢å¼•
            if self.build_config['create_compound_indexes']:
                index_collection.create_index([
                    ('source_table', ASCENDING),
                    ('field_name', ASCENDING),
                    ('keyword', ASCENDING)
                ], name='idx_compound', background=True)
            
            logger.debug(f"å·²åˆ›å»ºæ•°æ®åº“ç´¢å¼•: {index_table_name}")
            
        except Exception as e:
            logger.warning(f"åˆ›å»ºæ•°æ®åº“ç´¢å¼•å¤±è´¥: {index_table_name} - {str(e)}")
    
    def _build_indexes_parallel(self, table_name: str, target_fields: Set[str], 
                               force_rebuild: bool) -> List[Dict]:
        """å¹¶è¡Œæ„å»ºç´¢å¼•"""
        results = []
        
        with ThreadPoolExecutor(max_workers=self.build_config['max_workers']) as executor:
            future_to_field = {
                executor.submit(self.build_field_index, table_name, field, None, force_rebuild): field
                for field in target_fields
            }
            
            for future in as_completed(future_to_field):
                field = future_to_field[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger.error(f"å¹¶è¡Œæ„å»ºç´¢å¼•å¤±è´¥: {table_name}.{field} - {str(e)}")
                    results.append({
                        'status': 'error',
                        'table_name': table_name,
                        'field_name': field,
                        'error': str(e)
                    })
        
        return results
    
    def _build_indexes_sequential(self, table_name: str, target_fields: Set[str], 
                                 force_rebuild: bool) -> List[Dict]:
        """é¡ºåºæ„å»ºç´¢å¼•"""
        results = []
        
        for field in target_fields:
            try:
                result = self.build_field_index(table_name, field, None, force_rebuild)
                results.append(result)
            except Exception as e:
                logger.error(f"é¡ºåºæ„å»ºç´¢å¼•å¤±è´¥: {table_name}.{field} - {str(e)}")
                results.append({
                    'status': 'error',
                    'table_name': table_name,
                    'field_name': field,
                    'error': str(e)
                })
        
        return results
    
    def build_mapping_indexes(self, mappings: List[Dict], force_rebuild: bool = False) -> Dict[str, Any]:
        """
        æ ¹æ®å­—æ®µæ˜ å°„æ‰¹é‡æ„å»ºç´¢å¼•
        
        Args:
            mappings: å­—æ®µæ˜ å°„åˆ—è¡¨
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»º
            
        Returns:
            Dict: æ‰¹é‡æ„å»ºç»“æœ
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ—ï¸ å¼€å§‹æ ¹æ®æ˜ å°„æ„å»ºç´¢å¼•, æ˜ å°„æ•°é‡: {len(mappings)}")
            
            # æŒ‰ç›®æ ‡è¡¨åˆ†ç»„
            table_mappings = defaultdict(list)
            for mapping in mappings:
                target_table = mapping.get('target_table')
                if target_table:
                    table_mappings[target_table].append(mapping)
            
            results = {
                'total_tables': len(table_mappings),
                'total_mappings': len(mappings),
                'table_results': [],
                'overall_success_count': 0,
                'overall_error_count': 0,
                'overall_skipped_count': 0,
                'total_build_time': 0.0
            }
            
            # ä¸ºæ¯ä¸ªè¡¨æ„å»ºç´¢å¼•
            for table_name, table_mapping_list in table_mappings.items():
                table_result = self.build_table_indexes(table_name, table_mapping_list, force_rebuild)
                results['table_results'].append(table_result)
                
                results['overall_success_count'] += table_result.get('success_count', 0)
                results['overall_error_count'] += table_result.get('error_count', 0)
                results['overall_skipped_count'] += table_result.get('skipped_count', 0)
            
            results['total_build_time'] = time.time() - start_time
            
            logger.info(f"âœ… æ˜ å°„ç´¢å¼•æ„å»ºå®Œæˆ, "
                       f"è¡¨æ•°é‡: {results['total_tables']}, "
                       f"æˆåŠŸ: {results['overall_success_count']}, "
                       f"é”™è¯¯: {results['overall_error_count']}, "
                       f"è·³è¿‡: {results['overall_skipped_count']}, "
                       f"æ€»è€—æ—¶: {results['total_build_time']:.2f}s")
            
            return results
            
        except Exception as e:
            logger.error(f"æ˜ å°„ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}")
            return {
                'status': 'error',
                'error': str(e),
                'total_build_time': time.time() - start_time
            }
    
    def get_build_stats(self) -> Dict[str, Any]:
        """è·å–æ„å»ºç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_records_processed': self.build_stats['total_records_processed'],
            'total_keywords_created': self.build_stats['total_keywords_created'],
            'total_time_spent': self.build_stats['total_time_spent'],
            'tables_processed': self.build_stats['tables_processed'],
            'fields_processed': self.build_stats['fields_processed'],
            'avg_processing_rate': (
                self.build_stats['total_records_processed'] / 
                max(self.build_stats['total_time_spent'], 0.001)
            )
        }


# å¯¼å…¥defaultdict
from collections import defaultdict
