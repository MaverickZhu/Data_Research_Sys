#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•ä½åç§°ç›¸ä¼¼åº¦è®¡ç®—é—®é¢˜åˆ†æè„šæœ¬
åˆ†æå½“å‰ç®—æ³•çš„é—®é¢˜å¹¶è®¾è®¡æ”¹è¿›æ–¹æ¡ˆ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.matching.similarity_scorer import SimilarityCalculator
from fuzzywuzzy import fuzz
import jieba
import re
from typing import List, Dict, Tuple

def analyze_current_algorithm():
    """åˆ†æå½“å‰ç®—æ³•çš„é—®é¢˜"""
    print("=" * 60)
    print("ğŸ” å•ä½åç§°ç›¸ä¼¼åº¦è®¡ç®—é—®é¢˜åˆ†æ")
    print("=" * 60)
    
    # é—®é¢˜æ¡ˆä¾‹
    test_cases = [
        ("ä¸Šæµ·ä¸ºæ°‘é£Ÿå“å‚", "ä¸Šæµ·æƒ æ°‘é£Ÿå“å‚"),
        ("åŒ—äº¬åä¸ºç§‘æŠ€æœ‰é™å…¬å¸", "åŒ—äº¬åç¾ç§‘æŠ€æœ‰é™å…¬å¸"),
        ("æ·±åœ³è…¾è®¯è®¡ç®—æœºç³»ç»Ÿæœ‰é™å…¬å¸", "æ·±åœ³è…¾è®¯ç§‘æŠ€æœ‰é™å…¬å¸"),
        ("å¹¿å·æ’å¤§åœ°äº§é›†å›¢æœ‰é™å…¬å¸", "å¹¿å·æ’åŸºåœ°äº§é›†å›¢æœ‰é™å…¬å¸"),
        ("ä¸Šæµ·æµ¦ä¸œå‘å±•é“¶è¡Œ", "ä¸Šæµ·æµ¦å‘é“¶è¡Œ"),
        ("ä¸­å›½å·¥å•†é“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸", "ä¸­å›½å†œä¸šé“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸"),
        ("è‹¹æœç”µè„‘è´¸æ˜“ï¼ˆä¸Šæµ·ï¼‰æœ‰é™å…¬å¸", "è‹¹æœç”µå­è´¸æ˜“ï¼ˆä¸Šæµ·ï¼‰æœ‰é™å…¬å¸"),
        ("ä¸Šæµ·å¸‚ç¬¬ä¸€äººæ°‘åŒ»é™¢", "ä¸Šæµ·å¸‚ç¬¬äºŒäººæ°‘åŒ»é™¢")
    ]
    
    # åˆå§‹åŒ–ç›¸ä¼¼åº¦è®¡ç®—å™¨
    config = {
        'string_similarity': {
            'algorithms': [
                {'name': 'levenshtein', 'weight': 0.3},
                {'name': 'jaro_winkler', 'weight': 0.3},
                {'name': 'cosine', 'weight': 0.4}
            ],
            'chinese_processing': {
                'enable_pinyin': True,
                'enable_jieba': True,
                'remove_punctuation': True,
                'normalize_spaces': True
            }
        }
    }
    
    calculator = SimilarityCalculator(config)
    
    print("\nğŸ“Š å½“å‰ç®—æ³•æµ‹è¯•ç»“æœ:")
    print("-" * 60)
    print(f"{'å•ä½åç§°1':<25} {'å•ä½åç§°2':<25} {'ç›¸ä¼¼åº¦':<8} {'é—®é¢˜åˆ†æ'}")
    print("-" * 60)
    
    for name1, name2 in test_cases:
        # å½“å‰ç®—æ³•è®¡ç®—ç›¸ä¼¼åº¦
        similarity = calculator.calculate_string_similarity(name1, name2)
        
        # åˆ†æé—®é¢˜
        problem_analysis = analyze_similarity_problem(name1, name2, similarity)
        
        print(f"{name1:<25} {name2:<25} {similarity:.3f}    {problem_analysis}")
    
    print("\nğŸš¨ å‘ç°çš„æ ¸å¿ƒé—®é¢˜:")
    print("1. å­—ç¬¦é‡å ç‡ç®—æ³•ï¼šç®€å•ç»Ÿè®¡ç›¸åŒå­—ç¬¦æ•°é‡ï¼Œå¿½ç•¥äº†è¯­ä¹‰é‡è¦æ€§")
    print("2. ç¼ºä¹æ ¸å¿ƒè¯æ±‡è¯†åˆ«ï¼šæ— æ³•åŒºåˆ†'ä¸ºæ°‘'å’Œ'æƒ æ°‘'è¿™ç§æ ¸å¿ƒå·®å¼‚")
    print("3. ä½ç½®æƒé‡ç¼ºå¤±ï¼šå•ä½åç§°ä¸­ä¸åŒä½ç½®çš„è¯æ±‡é‡è¦æ€§ä¸åŒ")
    print("4. è¡Œä¸šè¯æ±‡å¹²æ‰°ï¼š'æœ‰é™å…¬å¸'ã€'è‚¡ä»½'ç­‰é€šç”¨è¯æ±‡å½±å“åˆ¤æ–­")
    print("5. è¯­ä¹‰ç†è§£ä¸è¶³ï¼šæ— æ³•ç†è§£'æµ¦ä¸œå‘å±•é“¶è¡Œ'å’Œ'æµ¦å‘é“¶è¡Œ'çš„ç­‰ä»·æ€§")

def analyze_similarity_problem(name1: str, name2: str, similarity: float) -> str:
    """åˆ†æç›¸ä¼¼åº¦è®¡ç®—çš„é—®é¢˜"""
    # æå–æ ¸å¿ƒè¯æ±‡
    core1 = extract_core_keywords(name1)
    core2 = extract_core_keywords(name2)
    
    # æ£€æŸ¥æ ¸å¿ƒè¯æ±‡é‡å 
    core_overlap = len(set(core1) & set(core2))
    
    if similarity > 0.8 and core_overlap == 0:
        return "âŒé«˜ç›¸ä¼¼åº¦ä½†æ ¸å¿ƒè¯ä¸åŒ"
    elif similarity < 0.5 and core_overlap > 0:
        return "âŒä½ç›¸ä¼¼åº¦ä½†æ ¸å¿ƒè¯ç›¸åŒ"
    elif similarity > 0.7:
        return "âš ï¸å¯èƒ½è¯¯åˆ¤"
    else:
        return "âœ…åˆç†"

def extract_core_keywords(company_name: str) -> List[str]:
    """æå–å•ä½åç§°çš„æ ¸å¿ƒå…³é”®è¯"""
    # ç§»é™¤å¸¸è§çš„å…¬å¸åç¼€
    suffixes = ['æœ‰é™å…¬å¸', 'è‚¡ä»½æœ‰é™å…¬å¸', 'é›†å›¢æœ‰é™å…¬å¸', 'ç§‘æŠ€æœ‰é™å…¬å¸', 
                'è´¸æ˜“æœ‰é™å…¬å¸', 'æŠ•èµ„æœ‰é™å…¬å¸', 'å‘å±•æœ‰é™å…¬å¸', 'å®ä¸šæœ‰é™å…¬å¸',
                'æœ‰é™è´£ä»»å…¬å¸', 'è‚¡ä»½å…¬å¸', 'é›†å›¢å…¬å¸', 'å…¬å¸', 'å‚', 'åº—', 'é™¢', 'ä¸­å¿ƒ']
    
    name = company_name
    for suffix in suffixes:
        if name.endswith(suffix):
            name = name[:-len(suffix)]
            break
    
    # ç§»é™¤åœ°åŒºå‰ç¼€
    prefixes = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'å¤©æ´¥', 'é‡åº†', 'æ­å·', 'å—äº¬', 'æ­¦æ±‰', 'æˆéƒ½',
                'ä¸­å›½', 'å…¨å›½', 'å›½é™…', 'äºšæ´²', 'ä¸–ç•Œ']
    
    for prefix in prefixes:
        if name.startswith(prefix):
            name = name[len(prefix):]
            break
    
    # ä½¿ç”¨jiebaåˆ†è¯æå–å…³é”®è¯
    words = jieba.lcut(name)
    
    # è¿‡æ»¤åœç”¨è¯å’Œå•å­—ç¬¦
    stop_words = {'çš„', 'å’Œ', 'ä¸', 'åŠ', 'æˆ–', 'ç­‰', 'ä¸º', 'æ˜¯', 'åœ¨', 'æœ‰', 'æ— ', 'ä¸', 'äº†', 'ç€', 'è¿‡'}
    keywords = [word for word in words if len(word) > 1 and word not in stop_words]
    
    return keywords

def design_improved_algorithm():
    """è®¾è®¡æ”¹è¿›çš„å•ä½åç§°ç›¸ä¼¼åº¦ç®—æ³•"""
    print("\n" + "=" * 60)
    print("ğŸš€ æ”¹è¿›ç®—æ³•è®¾è®¡æ–¹æ¡ˆ")
    print("=" * 60)
    
    print("\nğŸ“‹ æ ¸å¿ƒæ”¹è¿›æ€è·¯:")
    print("1. ã€æ ¸å¿ƒè¯æ±‡æƒé‡åŒ–ã€‘ï¼šè¯†åˆ«å•ä½åç§°ä¸­çš„æ ¸å¿ƒä¸šåŠ¡è¯æ±‡ï¼Œç»™äºˆæ›´é«˜æƒé‡")
    print("2. ã€ä½ç½®æ•æ„ŸåŒ¹é…ã€‘ï¼šä¸åŒä½ç½®çš„è¯æ±‡é‡è¦æ€§ä¸åŒï¼Œæ ¸å¿ƒè¯æ±‡ä½ç½®æ›´é‡è¦")
    print("3. ã€è¯­ä¹‰ç­‰ä»·è¯†åˆ«ã€‘ï¼šè¯†åˆ«åŒä¹‰è¯å’Œç¼©å†™ï¼ˆå¦‚'æµ¦ä¸œå‘å±•é“¶è¡Œ'='æµ¦å‘é“¶è¡Œ'ï¼‰")
    print("4. ã€è¡Œä¸šè¯æ±‡è¿‡æ»¤ã€‘ï¼šé™ä½é€šç”¨è¯æ±‡ï¼ˆå¦‚'æœ‰é™å…¬å¸'ï¼‰çš„æƒé‡")
    print("5. ã€å¤šå±‚æ¬¡åŒ¹é…ã€‘ï¼šç»“åˆå­—ç¬¦çº§ã€è¯æ±‡çº§ã€è¯­ä¹‰çº§å¤šå±‚æ¬¡åŒ¹é…")
    
    print("\nğŸ”§ ç®—æ³•æ¶æ„è®¾è®¡:")
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚                å•ä½åç§°æ™ºèƒ½ç›¸ä¼¼åº¦è®¡ç®—å™¨                    â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ 1. é¢„å¤„ç†å±‚                                              â”‚")
    print("â”‚    - æ ‡å‡†åŒ–å¤„ç†ï¼ˆå»ç©ºæ ¼ã€ç»Ÿä¸€å¤§å°å†™ï¼‰                      â”‚")
    print("â”‚    - å…¬å¸åç¼€è¯†åˆ«å’Œæ ‡å‡†åŒ–                                 â”‚")
    print("â”‚    - åœ°åŒºå‰ç¼€å¤„ç†                                        â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ 2. æ ¸å¿ƒè¯æ±‡æå–å±‚                                         â”‚")
    print("â”‚    - jiebaåˆ†è¯ + è‡ªå®šä¹‰è¯å…¸                              â”‚")
    print("â”‚    - æ ¸å¿ƒä¸šåŠ¡è¯æ±‡è¯†åˆ«                                     â”‚")
    print("â”‚    - è¯æ±‡é‡è¦æ€§æƒé‡åˆ†é…                                   â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ 3. å¤šå±‚æ¬¡åŒ¹é…å±‚                                          â”‚")
    print("â”‚    - ç²¾ç¡®åŒ¹é…ï¼ˆæƒé‡: 40%ï¼‰                               â”‚")
    print("â”‚    - æ ¸å¿ƒè¯æ±‡åŒ¹é…ï¼ˆæƒé‡: 35%ï¼‰                            â”‚")
    print("â”‚    - è¯­ä¹‰ç›¸ä¼¼åŒ¹é…ï¼ˆæƒé‡: 15%ï¼‰                            â”‚")
    print("â”‚    - å­—ç¬¦ç›¸ä¼¼åŒ¹é…ï¼ˆæƒé‡: 10%ï¼‰                            â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ 4. æ™ºèƒ½å†³ç­–å±‚                                            â”‚")
    print("â”‚    - æ ¸å¿ƒè¯æ±‡å†²çªæ£€æµ‹                                     â”‚")
    print("â”‚    - åŒä¹‰è¯ç­‰ä»·æ€§åˆ¤æ–­                                     â”‚")
    print("â”‚    - æœ€ç»ˆç›¸ä¼¼åº¦ç»¼åˆè®¡ç®—                                   â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

def test_improved_algorithm_concept():
    """æµ‹è¯•æ”¹è¿›ç®—æ³•çš„æ¦‚å¿µéªŒè¯"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æ”¹è¿›ç®—æ³•æ¦‚å¿µéªŒè¯")
    print("=" * 60)
    
    test_cases = [
        ("ä¸Šæµ·ä¸ºæ°‘é£Ÿå“å‚", "ä¸Šæµ·æƒ æ°‘é£Ÿå“å‚"),
        ("åŒ—äº¬åä¸ºç§‘æŠ€æœ‰é™å…¬å¸", "åŒ—äº¬åç¾ç§‘æŠ€æœ‰é™å…¬å¸"),
        ("ä¸Šæµ·æµ¦ä¸œå‘å±•é“¶è¡Œ", "ä¸Šæµ·æµ¦å‘é“¶è¡Œ"),
    ]
    
    print(f"{'å•ä½åç§°1':<25} {'å•ä½åç§°2':<25} {'å½“å‰ç®—æ³•':<8} {'æ”¹è¿›ç®—æ³•':<8} {'æ”¹è¿›è¯´æ˜'}")
    print("-" * 80)
    
    for name1, name2 in test_cases:
        # å½“å‰ç®—æ³•
        config = {'string_similarity': {'algorithms': [{'name': 'levenshtein', 'weight': 1.0}]}}
        calculator = SimilarityCalculator(config)
        current_sim = calculator.calculate_string_similarity(name1, name2)
        
        # æ”¹è¿›ç®—æ³•æ¦‚å¿µéªŒè¯
        improved_sim = calculate_improved_similarity_concept(name1, name2)
        
        # æ”¹è¿›è¯´æ˜
        improvement = get_improvement_explanation(name1, name2, current_sim, improved_sim)
        
        print(f"{name1:<25} {name2:<25} {current_sim:.3f}    {improved_sim:.3f}    {improvement}")

def calculate_improved_similarity_concept(name1: str, name2: str) -> float:
    """æ”¹è¿›ç®—æ³•çš„æ¦‚å¿µéªŒè¯å®ç°"""
    # 1. æå–æ ¸å¿ƒè¯æ±‡
    core1 = extract_core_keywords(name1)
    core2 = extract_core_keywords(name2)
    
    # 2. æ ¸å¿ƒè¯æ±‡åŒ¹é…æ£€æŸ¥
    core_intersection = set(core1) & set(core2)
    core_union = set(core1) | set(core2)
    
    if len(core_union) == 0:
        return 0.0
    
    # 3. æ ¸å¿ƒè¯æ±‡ç›¸ä¼¼åº¦ï¼ˆæƒé‡35%ï¼‰
    core_similarity = len(core_intersection) / len(core_union)
    
    # 4. æ£€æŸ¥æ ¸å¿ƒè¯æ±‡å†²çª
    # å¦‚æœæ ¸å¿ƒè¯æ±‡å®Œå…¨ä¸åŒï¼Œç›¸ä¼¼åº¦åº”è¯¥å¾ˆä½
    if len(core_intersection) == 0 and len(core1) > 0 and len(core2) > 0:
        # æ ¸å¿ƒè¯æ±‡å†²çªï¼Œé™ä½ç›¸ä¼¼åº¦
        core_similarity = 0.1
    
    # 5. å­—ç¬¦çº§ç›¸ä¼¼åº¦ï¼ˆæƒé‡10%ï¼‰
    char_similarity = fuzz.ratio(name1, name2) / 100.0
    
    # 6. åŒä¹‰è¯æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
    synonym_bonus = 0.0
    if ("æµ¦ä¸œå‘å±•é“¶è¡Œ" in name1 and "æµ¦å‘é“¶è¡Œ" in name2) or \
       ("æµ¦å‘é“¶è¡Œ" in name1 and "æµ¦ä¸œå‘å±•é“¶è¡Œ" in name2):
        synonym_bonus = 0.8
    
    # 7. ç»¼åˆè®¡ç®—
    final_similarity = core_similarity * 0.35 + char_similarity * 0.10 + synonym_bonus * 0.55
    
    return min(final_similarity, 1.0)

def get_improvement_explanation(name1: str, name2: str, current: float, improved: float) -> str:
    """è·å–æ”¹è¿›è¯´æ˜"""
    diff = improved - current
    if abs(diff) < 0.1:
        return "ç›¸ä¼¼"
    elif diff > 0:
        return f"æå‡{diff:.2f}"
    else:
        return f"é™ä½{abs(diff):.2f}"

if __name__ == "__main__":
    try:
        analyze_current_algorithm()
        design_improved_algorithm()
        test_improved_algorithm_concept()
        
        print("\n" + "=" * 60)
        print("âœ… åˆ†æå®Œæˆï¼")
        print("ğŸ“ ä¸‹ä¸€æ­¥ï¼šå®ç°æ”¹è¿›çš„å•ä½åç§°ç›¸ä¼¼åº¦ç®—æ³•")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
