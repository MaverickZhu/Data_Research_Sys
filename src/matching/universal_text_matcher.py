#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨æ–‡æœ¬è¯­ä¹‰åˆ†æåŒ¹é…å™¨
æ”¯æŒä»»æ„å­—æ®µç±»å‹çš„é«˜æ€§èƒ½æ¨¡ç³ŠåŒ¹é…ï¼šå•ä½åç§°ã€åœ°å€ã€äººåã€ç”µè¯ã€èº«ä»½è¯ã€é‚®ç®±ç­‰
åŸºäºé¢„å»ºç´¢å¼•å’Œèšåˆç®¡é“æŸ¥è¯¢ï¼Œå®ç°2000+æ¡/ç§’çš„åŒ¹é…æ€§èƒ½
"""

import re
import jieba
import time
import logging
from typing import Dict, List, Set, Tuple, Optional, Any
from collections import defaultdict
from enum import Enum
import pymongo
from dataclasses import dataclass

logger = logging.getLogger(__name__)


class FieldType(Enum):
    """å­—æ®µç±»å‹æšä¸¾"""
    TEXT = "text"           # é€šç”¨æ–‡æœ¬
    UNIT_NAME = "unit_name" # å•ä½åç§°
    ADDRESS = "address"     # åœ°å€
    PERSON_NAME = "person_name"  # äººå
    PHONE = "phone"         # ç”µè¯å·ç 
    ID_CARD = "id_card"     # èº«ä»½è¯
    CREDIT_CODE = "credit_code"  # ç¤¾ä¼šä¿¡ç”¨ä»£ç 
    EMAIL = "email"         # ç”µå­é‚®ç®±
    COORDINATE = "coordinate"    # åœ°ç†åæ ‡
    NUMERIC = "numeric"     # æ•°å­—


@dataclass
class FieldProcessingConfig:
    """å­—æ®µå¤„ç†é…ç½®"""
    field_type: FieldType
    preprocessing_func: str  # é¢„å¤„ç†å‡½æ•°å
    keyword_extraction_func: str  # å…³é”®è¯æå–å‡½æ•°å
    similarity_threshold: float  # ç›¸ä¼¼åº¦é˜ˆå€¼
    max_candidates: int  # æœ€å¤§å€™é€‰æ•°
    enable_ngram: bool = True  # æ˜¯å¦å¯ç”¨N-gram
    ngram_size: int = 3  # N-gramå¤§å°


class UniversalTextMatcher:
    """é€šç”¨æ–‡æœ¬è¯­ä¹‰åˆ†æåŒ¹é…å™¨"""
    
    def __init__(self, db_manager):
        """
        åˆå§‹åŒ–é€šç”¨æ–‡æœ¬åŒ¹é…å™¨
        
        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
        """
        self.db_manager = db_manager
        self.db = db_manager.get_db() if db_manager else None
        
        # å­—æ®µç±»å‹å¤„ç†é…ç½®
        self.field_configs = self._initialize_field_configs()
        
        # æ€§èƒ½é…ç½®
        self.performance_config = {
            'batch_size': 100,
            'max_total_candidates': 50,
            'enable_cache': True,
            'cache_ttl': 3600,  # 1å°æ—¶
            'parallel_processing': True,
            'max_workers': 16
        }
        
        # ç¼“å­˜ç³»ç»Ÿ
        self.query_cache = {}
        self.field_type_cache = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_queries': 0,
            'cache_hits': 0,
            'avg_query_time': 0.0,
            'field_type_distribution': defaultdict(int)
        }
        
        logger.info("ğŸš€ é€šç”¨æ–‡æœ¬è¯­ä¹‰åˆ†æåŒ¹é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_field_configs(self) -> Dict[FieldType, FieldProcessingConfig]:
        """åˆå§‹åŒ–å­—æ®µå¤„ç†é…ç½®"""
        return {
            FieldType.TEXT: FieldProcessingConfig(
                field_type=FieldType.TEXT,
                preprocessing_func='_preprocess_text',
                keyword_extraction_func='_extract_text_keywords',
                similarity_threshold=0.6,
                max_candidates=30
            ),
            FieldType.UNIT_NAME: FieldProcessingConfig(
                field_type=FieldType.UNIT_NAME,
                preprocessing_func='_preprocess_unit_name',
                keyword_extraction_func='_extract_unit_name_keywords',
                similarity_threshold=0.7,
                max_candidates=50
            ),
            FieldType.ADDRESS: FieldProcessingConfig(
                field_type=FieldType.ADDRESS,
                preprocessing_func='_preprocess_address',
                keyword_extraction_func='_extract_address_keywords',
                similarity_threshold=0.3,  # é™ä½åœ°å€åŒ¹é…é˜ˆå€¼
                max_candidates=40
            ),
            FieldType.PERSON_NAME: FieldProcessingConfig(
                field_type=FieldType.PERSON_NAME,
                preprocessing_func='_preprocess_person_name',
                keyword_extraction_func='_extract_person_name_keywords',
                similarity_threshold=0.8,
                max_candidates=20
            ),
            FieldType.PHONE: FieldProcessingConfig(
                field_type=FieldType.PHONE,
                preprocessing_func='_preprocess_phone',
                keyword_extraction_func='_extract_phone_keywords',
                similarity_threshold=0.9,
                max_candidates=10,
                enable_ngram=False
            ),
            FieldType.ID_CARD: FieldProcessingConfig(
                field_type=FieldType.ID_CARD,
                preprocessing_func='_preprocess_id_card',
                keyword_extraction_func='_extract_id_card_keywords',
                similarity_threshold=0.95,
                max_candidates=5,
                enable_ngram=False
            ),
            FieldType.CREDIT_CODE: FieldProcessingConfig(
                field_type=FieldType.CREDIT_CODE,
                preprocessing_func='_preprocess_credit_code',
                keyword_extraction_func='_extract_credit_code_keywords',
                similarity_threshold=0.95,
                max_candidates=5,
                enable_ngram=False
            ),
            FieldType.EMAIL: FieldProcessingConfig(
                field_type=FieldType.EMAIL,
                preprocessing_func='_preprocess_email',
                keyword_extraction_func='_extract_email_keywords',
                similarity_threshold=0.9,
                max_candidates=10,
                enable_ngram=False
            )
        }
    
    def detect_field_type(self, field_name: str, sample_values: List[str]) -> FieldType:
        """
        æ™ºèƒ½æ£€æµ‹å­—æ®µç±»å‹
        
        Args:
            field_name: å­—æ®µåç§°
            sample_values: æ ·æœ¬å€¼åˆ—è¡¨
            
        Returns:
            FieldType: æ£€æµ‹åˆ°çš„å­—æ®µç±»å‹
        """
        # ç¼“å­˜æ£€æŸ¥
        cache_key = f"{field_name}_{hash(tuple(sample_values[:5]))}"
        if cache_key in self.field_type_cache:
            return self.field_type_cache[cache_key]
        
        field_type = self._analyze_field_type(field_name, sample_values)
        self.field_type_cache[cache_key] = field_type
        self.stats['field_type_distribution'][field_type] += 1
        
        logger.info(f"å­—æ®µç±»å‹æ£€æµ‹: {field_name} -> {field_type.value}")
        return field_type
    
    def _analyze_field_type(self, field_name: str, sample_values: List[str]) -> FieldType:
        """åˆ†æå­—æ®µç±»å‹"""
        if not sample_values:
            return FieldType.TEXT
        
        # åŸºäºå­—æ®µåç§°çš„å¯å‘å¼è§„åˆ™ - ä¼˜åŒ–ä¼˜å…ˆçº§ï¼Œåœ°å€å­—æ®µä¼˜å…ˆ
        field_name_lower = field_name.lower()
        
        # åœ°å€å­—æ®µä¼˜å…ˆæ£€æµ‹ï¼Œé¿å…è¢«å•ä½åç§°è¯¯åˆ¤
        if any(keyword in field_name_lower for keyword in ['address', 'åœ°å€', 'addr', 'ä½ç½®']):
            return FieldType.ADDRESS
        elif any(keyword in field_name_lower for keyword in ['name', 'åç§°', 'å•ä½', 'unit', 'company']) and 'åœ°å€' not in field_name_lower:
            return FieldType.UNIT_NAME
        elif any(keyword in field_name_lower for keyword in ['person', 'äººå', 'å§“å', 'æ³•äºº', 'è”ç³»äºº']):
            return FieldType.PERSON_NAME
        elif any(keyword in field_name_lower for keyword in ['phone', 'ç”µè¯', 'tel', 'æ‰‹æœº']):
            return FieldType.PHONE
        elif any(keyword in field_name_lower for keyword in ['id', 'èº«ä»½è¯', 'card']):
            return FieldType.ID_CARD
        elif any(keyword in field_name_lower for keyword in ['credit', 'ä¿¡ç”¨ä»£ç ', 'code']):
            return FieldType.CREDIT_CODE
        elif any(keyword in field_name_lower for keyword in ['email', 'é‚®ç®±', 'mail']):
            return FieldType.EMAIL
        
        # åŸºäºæ ·æœ¬å€¼çš„æ¨¡å¼è¯†åˆ«
        sample_patterns = self._analyze_sample_patterns(sample_values[:10])
        
        if sample_patterns['phone_pattern'] > 0.7:
            return FieldType.PHONE
        elif sample_patterns['email_pattern'] > 0.7:
            return FieldType.EMAIL
        elif sample_patterns['id_card_pattern'] > 0.7:
            return FieldType.ID_CARD
        elif sample_patterns['credit_code_pattern'] > 0.7:
            return FieldType.CREDIT_CODE
        elif sample_patterns['address_pattern'] > 0.5:
            return FieldType.ADDRESS
        elif sample_patterns['person_name_pattern'] > 0.6:
            return FieldType.PERSON_NAME
        
        return FieldType.TEXT
    
    def _analyze_sample_patterns(self, sample_values: List[str]) -> Dict[str, float]:
        """åˆ†ææ ·æœ¬å€¼çš„æ¨¡å¼"""
        patterns = {
            'phone_pattern': 0.0,
            'email_pattern': 0.0,
            'id_card_pattern': 0.0,
            'credit_code_pattern': 0.0,
            'address_pattern': 0.0,
            'person_name_pattern': 0.0
        }
        
        if not sample_values:
            return patterns
        
        total_samples = len(sample_values)
        
        for value in sample_values:
            if not value:
                continue
                
            value_str = str(value).strip()
            
            # ç”µè¯å·ç æ¨¡å¼
            if re.match(r'^1[3-9]\d{9}$|^\d{3,4}-\d{7,8}$', value_str):
                patterns['phone_pattern'] += 1
            
            # é‚®ç®±æ¨¡å¼
            if re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', value_str):
                patterns['email_pattern'] += 1
            
            # èº«ä»½è¯æ¨¡å¼
            if re.match(r'^\d{17}[\dXx]$', value_str):
                patterns['id_card_pattern'] += 1
            
            # ç¤¾ä¼šä¿¡ç”¨ä»£ç æ¨¡å¼
            if re.match(r'^[0-9A-HJ-NPQRTUWXY]{2}\d{6}[0-9A-HJ-NPQRTUWXY]{10}$', value_str):
                patterns['credit_code_pattern'] += 1
            
            # åœ°å€æ¨¡å¼ï¼ˆåŒ…å«çœå¸‚åŒºç­‰åœ°ç†ä¿¡æ¯ï¼‰
            if any(geo in value_str for geo in ['çœ', 'å¸‚', 'åŒº', 'å¿', 'è¡—é“', 'è·¯', 'å·', 'æ‘', 'é•‡']):
                patterns['address_pattern'] += 1
            
            # äººåæ¨¡å¼ï¼ˆ2-4ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
            if re.match(r'^[\u4e00-\u9fff]{2,4}$', value_str):
                patterns['person_name_pattern'] += 1
        
        # è®¡ç®—æ¯”ä¾‹
        for pattern in patterns:
            patterns[pattern] = patterns[pattern] / total_samples
        
        return patterns
    
    def find_candidates_universal(self, source_value: str, target_table: str, 
                                target_field: str, field_type: FieldType = None) -> List[Dict]:
        """
        é€šç”¨å€™é€‰è®°å½•æŸ¥æ‰¾
        
        Args:
            source_value: æºå€¼
            target_table: ç›®æ ‡è¡¨å
            target_field: ç›®æ ‡å­—æ®µå
            field_type: å­—æ®µç±»å‹ï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨æ£€æµ‹ï¼‰
            
        Returns:
            List[Dict]: å€™é€‰è®°å½•åˆ—è¡¨
        """
        start_time = time.time()
        self.stats['total_queries'] += 1
        
        try:
            # è‡ªåŠ¨æ£€æµ‹å­—æ®µç±»å‹
            if field_type is None:
                field_type = self.detect_field_type(target_field, [source_value])
            
            # è·å–å­—æ®µå¤„ç†é…ç½®
            config = self.field_configs.get(field_type, self.field_configs[FieldType.TEXT])
            
            # é¢„å¤„ç†æºå€¼
            preprocessed_value = self._apply_preprocessing(source_value, config)
            if not preprocessed_value:
                return []
            
            # æå–å…³é”®è¯
            keywords = self._apply_keyword_extraction(preprocessed_value, config)
            if not keywords:
                return []
            
            # æ„å»ºç´¢å¼•è¡¨å
            index_table_name = f"{target_table}_{target_field}_keywords"
            
            # æ£€æŸ¥ç´¢å¼•è¡¨æ˜¯å¦å­˜åœ¨
            if index_table_name not in self.db.list_collection_names():
                logger.warning(f"ç´¢å¼•è¡¨ä¸å­˜åœ¨: {index_table_name}ï¼Œå°†åˆ›å»ºç´¢å¼•")
                self._create_field_index(target_table, target_field, field_type)
            
            # æ‰§è¡ŒèšåˆæŸ¥è¯¢
            candidates = self._execute_aggregation_query(
                index_table_name, target_table, target_field, 
                keywords, config
            )
            
            # æ›´æ–°ç»Ÿè®¡
            query_time = time.time() - start_time
            self.stats['avg_query_time'] = (
                (self.stats['avg_query_time'] * (self.stats['total_queries'] - 1) + query_time) 
                / self.stats['total_queries']
            )
            
            logger.info(f"é€šç”¨æŸ¥è¯¢å®Œæˆ: {target_field}({field_type.value}) -> "
                       f"{len(candidates)}ä¸ªå€™é€‰, è€—æ—¶: {query_time:.3f}s")
            
            return candidates
            
        except Exception as e:
            logger.error(f"é€šç”¨å€™é€‰æŸ¥æ‰¾å¤±è´¥: {str(e)}")
            return []
    
    def _apply_preprocessing(self, value: str, config: FieldProcessingConfig) -> str:
        """åº”ç”¨é¢„å¤„ç†"""
        if not value:
            return ""
        
        preprocessing_func = getattr(self, config.preprocessing_func, self._preprocess_text)
        return preprocessing_func(str(value))
    
    def _apply_keyword_extraction(self, value: str, config: FieldProcessingConfig) -> List[str]:
        """åº”ç”¨å…³é”®è¯æå–"""
        if not value:
            return []
        
        extraction_func = getattr(self, config.keyword_extraction_func, self._extract_text_keywords)
        return extraction_func(value)
    
    def _execute_aggregation_query(self, index_table_name: str, target_table: str, 
                                 target_field: str, keywords: List[str], 
                                 config: FieldProcessingConfig) -> List[Dict]:
        """æ‰§è¡Œèšåˆç®¡é“æŸ¥è¯¢"""
        try:
            index_collection = self.db[index_table_name]
            target_collection = self.db[target_table]
            
            # æ„å»ºèšåˆç®¡é“
            pipeline = [
                # åŒ¹é…å…³é”®è¯
                {'$match': {
                    'keyword': {'$in': keywords},
                    'field_name': target_field
                }},
                # æŒ‰æ–‡æ¡£IDåˆ†ç»„ç»Ÿè®¡
                {'$group': {
                    '_id': '$doc_id',
                    'original_value': {'$first': '$original_value'},
                    'match_count': {'$sum': 1},
                    'matched_keywords': {'$push': '$keyword'}
                }},
                # è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†
                {'$addFields': {
                    'similarity_score': {'$divide': ['$match_count', len(keywords)]}
                }},
                # è¿‡æ»¤ä½åˆ†å€™é€‰
                {'$match': {
                    'similarity_score': {'$gte': config.similarity_threshold}
                }},
                # æŒ‰å¾—åˆ†æ’åº
                {'$sort': {'similarity_score': -1}},
                # é™åˆ¶å€™é€‰æ•°é‡
                {'$limit': config.max_candidates}
            ]
            
            # æ‰§è¡ŒèšåˆæŸ¥è¯¢
            aggregation_results = list(index_collection.aggregate(pipeline))
            
            if not aggregation_results:
                return []
            
            # è·å–å®Œæ•´è®°å½•
            doc_ids = [result['_id'] for result in aggregation_results]
            full_records = list(target_collection.find({'_id': {'$in': doc_ids}}))
            
            # åˆå¹¶ç»“æœ
            record_map = {record['_id']: record for record in full_records}
            candidates = []
            
            for agg_result in aggregation_results:
                doc_id = agg_result['_id']
                if doc_id in record_map:
                    record = record_map[doc_id].copy()
                    record['_similarity_score'] = agg_result['similarity_score']
                    record['_matched_keywords'] = agg_result['matched_keywords']
                    candidates.append(record)
            
            return candidates
            
        except Exception as e:
            logger.error(f"èšåˆæŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}")
            return []
    
    def _create_field_index(self, table_name: str, field_name: str, field_type: FieldType):
        """ä¸ºå­—æ®µåˆ›å»ºç´¢å¼•ï¼ˆå»¶è¿Ÿåˆ›å»ºï¼‰"""
        logger.info(f"å¼€å§‹ä¸ºå­—æ®µåˆ›å»ºç´¢å¼•: {table_name}.{field_name} ({field_type.value})")
        # è¿™é‡Œå¯ä»¥è°ƒç”¨é€šç”¨ç´¢å¼•æ„å»ºå™¨
        # æš‚æ—¶è®°å½•éœ€è¦åˆ›å»ºçš„ç´¢å¼•ï¼Œå®é™…åˆ›å»ºå¯ä»¥å¼‚æ­¥è¿›è¡Œ
        pass
    
    # ==================== å­—æ®µé¢„å¤„ç†å‡½æ•° ====================
    
    def _preprocess_text(self, value: str) -> str:
        """é€šç”¨æ–‡æœ¬é¢„å¤„ç†"""
        if not value:
            return ""
        return re.sub(r'[^\u4e00-\u9fff\w\s]', '', str(value)).strip()
    
    def _preprocess_unit_name(self, value: str) -> str:
        """å•ä½åç§°é¢„å¤„ç†"""
        if not value:
            return ""
        # ç§»é™¤å¸¸è§åç¼€
        suffixes = ['æœ‰é™å…¬å¸', 'è‚¡ä»½æœ‰é™å…¬å¸', 'æœ‰é™è´£ä»»å…¬å¸', 'å…¬å¸', 'å‚', 'åº—', 'éƒ¨', 'ä¸­å¿ƒ', 'æ‰€']
        clean_value = str(value).strip()
        for suffix in suffixes:
            if clean_value.endswith(suffix):
                clean_value = clean_value[:-len(suffix)]
                break
        return self._preprocess_text(clean_value)
    
    def _preprocess_address(self, value: str) -> str:
        """åœ°å€é¢„å¤„ç†"""
        if not value:
            return ""
        # æ ‡å‡†åŒ–åœ°å€æ ¼å¼
        clean_value = str(value).strip()
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        clean_value = re.sub(r'\s+', '', clean_value)
        return clean_value
    
    def _preprocess_person_name(self, value: str) -> str:
        """äººåé¢„å¤„ç†"""
        if not value:
            return ""
        return re.sub(r'[^\u4e00-\u9fff]', '', str(value)).strip()
    
    def _preprocess_phone(self, value: str) -> str:
        """ç”µè¯å·ç é¢„å¤„ç†"""
        if not value:
            return ""
        # ç§»é™¤æ‰€æœ‰éæ•°å­—å­—ç¬¦
        return re.sub(r'[^\d]', '', str(value))
    
    def _preprocess_id_card(self, value: str) -> str:
        """èº«ä»½è¯é¢„å¤„ç†"""
        if not value:
            return ""
        return str(value).strip().upper()
    
    def _preprocess_credit_code(self, value: str) -> str:
        """ç¤¾ä¼šä¿¡ç”¨ä»£ç é¢„å¤„ç†"""
        if not value:
            return ""
        return str(value).strip().upper()
    
    def _preprocess_email(self, value: str) -> str:
        """é‚®ç®±é¢„å¤„ç†"""
        if not value:
            return ""
        return str(value).strip().lower()
    
    # ==================== å…³é”®è¯æå–å‡½æ•° ====================
    
    def _extract_text_keywords(self, value: str) -> List[str]:
        """é€šç”¨æ–‡æœ¬å…³é”®è¯æå–"""
        if not value:
            return []
        
        words = jieba.cut(value)
        keywords = []
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        
        for word in words:
            word = word.strip()
            if len(word) >= 2 and word not in stop_words:
                keywords.append(word)
        
        return list(set(keywords))
    
    def _extract_unit_name_keywords(self, value: str) -> List[str]:
        """å•ä½åç§°å…³é”®è¯æå–"""
        if not value:
            return []
        
        words = jieba.cut(value)
        keywords = []
        stop_words = {'æœ‰é™', 'å…¬å¸', 'ä¼ä¸š', 'é›†å›¢', 'å·¥å‚', 'å•†åº—', 'ä¸­å¿ƒ', 'è´£ä»»', 'è‚¡ä»½'}
        
        for word in words:
            word = word.strip()
            if len(word) >= 2 and word not in stop_words:
                keywords.append(word)
        
        return list(set(keywords))
    
    def _extract_address_keywords(self, value: str) -> List[str]:
        """åœ°å€å…³é”®è¯æå–ï¼ˆé«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰"""
        if not value:
            return []
        
        # è·³è¿‡å¤æ‚çš„åœ°å€æ ‡å‡†åŒ–ï¼Œç›´æ¥ä»åŸå§‹åœ°å€æå–å…³é”®è¯
        keywords = []
        
        # çœå¸‚åŒºæå–
        province_match = re.search(r'([\u4e00-\u9fff]{2,}çœ)', value)
        if province_match:
            keywords.append(province_match.group(1))
        
        city_match = re.search(r'([\u4e00-\u9fff]{2,}å¸‚)', value)
        if city_match:
            keywords.append(city_match.group(1))
        
        district_match = re.search(r'([\u4e00-\u9fff]{2,}[åŒºå¿])', value)
        if district_match:
            keywords.append(district_match.group(1))
        
        # é•‡çº§è¡Œæ”¿åŒºåˆ’æå–
        town_match = re.search(r'([\u4e00-\u9fff]{2,}é•‡)', value)
        if town_match:
            keywords.append(town_match.group(1))
        
        # è¡—é“è·¯åæå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
        street_matches = re.findall(r'([^çœå¸‚åŒºå¿é•‡]{2,6}[è·¯è¡—é“å··å¼„])', value)
        keywords.extend(street_matches[:3])  # åªå–å‰3ä¸ªï¼Œé¿å…è¿‡å¤šå…³é”®è¯
        
        # é—¨ç‰Œå·æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
        number_matches = re.findall(r'(\d+å·?)', value)
        keywords.extend(number_matches[:2])  # åªå–å‰2ä¸ªé—¨ç‰Œå·
        
        # å»ºç­‘ç‰©åç§°æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
        building_matches = re.findall(r'([\u4e00-\u9fff]{2,6}[å¤§å¦æ¥¼å®‡é™¢])', value)
        keywords.extend(building_matches[:2])  # åªå–å‰2ä¸ªå»ºç­‘ç‰©åç§°
        
        return list(set(keywords))
    
    def _extract_person_name_keywords(self, value: str) -> List[str]:
        """äººåå…³é”®è¯æå–"""
        if not value:
            return []
        
        # äººåé€šå¸¸ä½œä¸ºæ•´ä½“å…³é”®è¯
        clean_name = self._preprocess_person_name(value)
        if len(clean_name) >= 2:
            keywords = [clean_name]
            # å¦‚æœæ˜¯3-4å­—å§“åï¼Œä¹Ÿæå–å§“æ°
            if len(clean_name) >= 3:
                keywords.append(clean_name[0])  # å§“æ°
            return keywords
        return []
    
    def _extract_phone_keywords(self, value: str) -> List[str]:
        """ç”µè¯å·ç å…³é”®è¯æå–"""
        clean_phone = self._preprocess_phone(value)
        if len(clean_phone) >= 7:
            return [clean_phone]
        return []
    
    def _extract_id_card_keywords(self, value: str) -> List[str]:
        """èº«ä»½è¯å…³é”®è¯æå–"""
        clean_id = self._preprocess_id_card(value)
        if len(clean_id) == 18:
            keywords = [clean_id]
            # æå–åœ°åŒºä»£ç 
            if clean_id[:6].isdigit():
                keywords.append(clean_id[:6])  # åœ°åŒºä»£ç 
            return keywords
        return []
    
    def _extract_credit_code_keywords(self, value: str) -> List[str]:
        """ç¤¾ä¼šä¿¡ç”¨ä»£ç å…³é”®è¯æå–"""
        clean_code = self._preprocess_credit_code(value)
        if len(clean_code) == 18:
            return [clean_code]
        return []
    
    def _extract_email_keywords(self, value: str) -> List[str]:
        """é‚®ç®±å…³é”®è¯æå–"""
        clean_email = self._preprocess_email(value)
        if '@' in clean_email:
            keywords = [clean_email]
            # æå–ç”¨æˆ·åå’ŒåŸŸå
            parts = clean_email.split('@')
            if len(parts) == 2:
                keywords.extend(parts)
            return keywords
        return []
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_queries': self.stats['total_queries'],
            'cache_hits': self.stats['cache_hits'],
            'cache_hit_rate': self.stats['cache_hits'] / max(self.stats['total_queries'], 1),
            'avg_query_time': self.stats['avg_query_time'],
            'field_type_distribution': dict(self.stats['field_type_distribution'])
        }
