#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åŒ¹é…åˆ†æè„šæœ¬
åˆ†æå½“å‰åŒ¹é…æƒ…å†µï¼Œè¯†åˆ«ä¼˜åŒ–æœºä¼š
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pymongo
import pandas as pd
import re
from collections import Counter, defaultdict
from datetime import datetime
import jieba
from difflib import SequenceMatcher

class MatchDataAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.client = pymongo.MongoClient("mongodb://localhost:27017/")
        self.db = self.client["Unit_Info"]
        
        # é›†åˆå¼•ç”¨
        self.supervision_collection = self.db["xxj_shdwjbxx"]  # ç›‘ç£ç®¡ç†ç³»ç»Ÿ
        self.inspection_collection = self.db["xfaqpc_jzdwxx"]  # å®‰å…¨æ’æŸ¥ç³»ç»Ÿ
        self.match_collection = self.db["unit_match_results"]  # åŒ¹é…ç»“æœ
        
        print("ğŸ” æ•°æ®åŒ¹é…åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_basic_stats(self):
        """è·å–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ“Š åŸºç¡€æ•°æ®ç»Ÿè®¡")
        print("="*60)
        
        # æ•°æ®æºç»Ÿè®¡
        supervision_count = self.supervision_collection.count_documents({})
        inspection_count = self.inspection_collection.count_documents({})
        match_count = self.match_collection.count_documents({})
        
        print(f"ç›‘ç£ç®¡ç†ç³»ç»Ÿ: {supervision_count:,} æ¡")
        print(f"å®‰å…¨æ’æŸ¥ç³»ç»Ÿ: {inspection_count:,} æ¡")
        print(f"åŒ¹é…ç»“æœ: {match_count:,} æ¡")
        
        # åŒ¹é…ç‡è®¡ç®—
        total_records = supervision_count + inspection_count
        match_rate = (match_count / total_records) * 100 if total_records > 0 else 0
        print(f"æ€»è®°å½•æ•°: {total_records:,} æ¡")
        print(f"åŒ¹é…ç‡: {match_rate:.4f}%")
        
        return {
            'supervision_count': supervision_count,
            'inspection_count': inspection_count,
            'match_count': match_count,
            'total_records': total_records,
            'match_rate': match_rate
        }
    
    def analyze_match_types(self):
        """åˆ†æåŒ¹é…ç±»å‹åˆ†å¸ƒ"""
        print("\n" + "="*60)
        print("ğŸ¯ åŒ¹é…ç±»å‹åˆ†æ")
        print("="*60)
        
        # åŒ¹é…ç±»å‹ç»Ÿè®¡
        pipeline = [
            {"$group": {
                "_id": "$match_type",
                "count": {"$sum": 1},
                "avg_score": {"$avg": "$similarity_score"}
            }},
            {"$sort": {"count": -1}}
        ]
        
        match_types = list(self.match_collection.aggregate(pipeline))
        
        for match_type in match_types:
            match_type_name = match_type["_id"] or "æœªçŸ¥"
            count = match_type["count"]
            avg_score = match_type["avg_score"] or 0
            print(f"{match_type_name}: {count} æ¡ (å¹³å‡ç›¸ä¼¼åº¦: {avg_score:.3f})")
        
        return match_types
    
    def analyze_similarity_distribution(self):
        """åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ç›¸ä¼¼åº¦åˆ†å¸ƒåˆ†æ")
        print("="*60)
        
        # ç›¸ä¼¼åº¦åŒºé—´ç»Ÿè®¡
        pipeline = [
            {"$bucket": {
                "groupBy": "$similarity_score",
                "boundaries": [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
                "default": "å…¶ä»–",
                "output": {
                    "count": {"$sum": 1},
                    "examples": {"$push": {
                        "primary_unit_name": "$primary_unit_name",
                        "matched_unit_name": "$matched_unit_name",
                        "score": "$similarity_score"
                    }}
                }
            }}
        ]
        
        similarity_dist = list(self.match_collection.aggregate(pipeline))
        
        for bucket in similarity_dist:
            range_start = bucket["_id"]
            count = bucket["count"]
            if isinstance(range_start, (int, float)):
                range_end = range_start + 0.1
                print(f"ç›¸ä¼¼åº¦ {range_start:.1f}-{range_end:.1f}: {count} æ¡")
            else:
                print(f"{range_start}: {count} æ¡")
        
        return similarity_dist
    
    def analyze_unit_name_patterns(self):
        """åˆ†æå•ä½åç§°æ¨¡å¼"""
        print("\n" + "="*60)
        print("ğŸ¢ å•ä½åç§°æ¨¡å¼åˆ†æ")
        print("="*60)
        
        # åˆ†æç›‘ç£ç®¡ç†ç³»ç»Ÿçš„å•ä½åç§°
        print("ç›‘ç£ç®¡ç†ç³»ç»Ÿå•ä½åç§°ç‰¹å¾:")
        supervision_sample = list(self.supervision_collection.find({}, {"dwmc": 1}).limit(1000))
        self._analyze_name_patterns(supervision_sample, "dwmc")
        
        print("\nå®‰å…¨æ’æŸ¥ç³»ç»Ÿå•ä½åç§°ç‰¹å¾:")
        inspection_sample = list(self.inspection_collection.find({}, {"UNIT_NAME": 1}).limit(1000))
        self._analyze_name_patterns(inspection_sample, "UNIT_NAME")
    
    def _analyze_name_patterns(self, data, field_name):
        """åˆ†æåç§°æ¨¡å¼çš„è¾…åŠ©æ–¹æ³•"""
        names = [item.get(field_name, "") for item in data if item.get(field_name)]
        
        # é•¿åº¦åˆ†å¸ƒ
        lengths = [len(name) for name in names]
        avg_length = sum(lengths) / len(lengths) if lengths else 0
        print(f"  å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
        print(f"  é•¿åº¦èŒƒå›´: {min(lengths) if lengths else 0} - {max(lengths) if lengths else 0}")
        
        # å¸¸è§åç¼€
        suffixes = []
        for name in names:
            if len(name) > 2:
                suffixes.append(name[-2:])
                if len(name) > 3:
                    suffixes.append(name[-3:])
        
        suffix_counter = Counter(suffixes)
        print("  å¸¸è§åç¼€:")
        for suffix, count in suffix_counter.most_common(10):
            print(f"    {suffix}: {count} æ¬¡")
        
        # å¸¸è§å…³é”®è¯
        keywords = []
        for name in names:
            # ä½¿ç”¨jiebaåˆ†è¯
            words = jieba.lcut(name)
            keywords.extend([word for word in words if len(word) > 1])
        
        keyword_counter = Counter(keywords)
        print("  å¸¸è§å…³é”®è¯:")
        for keyword, count in keyword_counter.most_common(10):
            print(f"    {keyword}: {count} æ¬¡")
    
    def analyze_unmatched_data(self):
        """åˆ†ææœªåŒ¹é…æ•°æ®"""
        print("\n" + "="*60)
        print("âŒ æœªåŒ¹é…æ•°æ®åˆ†æ")
        print("="*60)
        
        # è·å–å·²åŒ¹é…çš„å•ä½åç§°
        matched_supervision = set()
        matched_inspection = set()
        
        for match in self.match_collection.find({}):
            if match.get("primary_unit_name"):
                matched_supervision.add(match["primary_unit_name"])
            if match.get("matched_unit_name"):
                matched_inspection.add(match["matched_unit_name"])
        
        print(f"å·²åŒ¹é…ç›‘ç£ç®¡ç†ç³»ç»Ÿå•ä½: {len(matched_supervision)} ä¸ª")
        print(f"å·²åŒ¹é…å®‰å…¨æ’æŸ¥ç³»ç»Ÿå•ä½: {len(matched_inspection)} ä¸ª")
        
        # åˆ†ææœªåŒ¹é…çš„æ•°æ®æ ·æœ¬
        print("\næœªåŒ¹é…ç›‘ç£ç®¡ç†ç³»ç»Ÿå•ä½æ ·æœ¬:")
        unmatched_supervision = list(self.supervision_collection.find({
            "dwmc": {"$nin": list(matched_supervision)}
        }, {"dwmc": 1}).limit(10))
        
        for item in unmatched_supervision:
            print(f"  {item.get('dwmc', 'N/A')}")
        
        print("\næœªåŒ¹é…å®‰å…¨æ’æŸ¥ç³»ç»Ÿå•ä½æ ·æœ¬:")
        unmatched_inspection = list(self.inspection_collection.find({
            "UNIT_NAME": {"$nin": list(matched_inspection)}
        }, {"UNIT_NAME": 1}).limit(10))
        
        for item in unmatched_inspection:
            print(f"  {item.get('UNIT_NAME', 'N/A')}")
    
    def find_potential_matches(self):
        """å¯»æ‰¾æ½œåœ¨åŒ¹é…"""
        print("\n" + "="*60)
        print("ğŸ” æ½œåœ¨åŒ¹é…åˆ†æ")
        print("="*60)
        
        # è·å–æ ·æœ¬æ•°æ®è¿›è¡Œåˆ†æ
        supervision_sample = list(self.supervision_collection.find({}, {"dwmc": 1}).limit(100))
        inspection_sample = list(self.inspection_collection.find({}, {"UNIT_NAME": 1}).limit(100))
        
        potential_matches = []
        
        for sup_item in supervision_sample:
            sup_name = sup_item.get("dwmc", "")
            if not sup_name:
                continue
                
            for ins_item in inspection_sample:
                ins_name = ins_item.get("UNIT_NAME", "")
                if not ins_name:
                    continue
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self._calculate_similarity(sup_name, ins_name)
                
                if similarity > 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    potential_matches.append({
                        'supervision_name': sup_name,
                        'inspection_name': ins_name,
                        'similarity': similarity
                    })
        
        # æ’åºå¹¶æ˜¾ç¤ºæ½œåœ¨åŒ¹é…
        potential_matches.sort(key=lambda x: x['similarity'], reverse=True)
        
        print(f"å‘ç° {len(potential_matches)} ä¸ªæ½œåœ¨åŒ¹é… (ç›¸ä¼¼åº¦ > 0.6):")
        for i, match in enumerate(potential_matches[:10]):
            print(f"  {i+1}. ç›¸ä¼¼åº¦: {match['similarity']:.3f}")
            print(f"     ç›‘ç£: {match['supervision_name']}")
            print(f"     æ’æŸ¥: {match['inspection_name']}")
            print()
        
        return potential_matches
    
    def _calculate_similarity(self, name1, name2):
        """è®¡ç®—ä¸¤ä¸ªåç§°çš„ç›¸ä¼¼åº¦"""
        if not name1 or not name2:
            return 0.0
        
        # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        return SequenceMatcher(None, name1, name2).ratio()
    
    def analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡"""
        print("\n" + "="*60)
        print("ğŸ” æ•°æ®è´¨é‡åˆ†æ")
        print("="*60)
        
        # ç›‘ç£ç®¡ç†ç³»ç»Ÿæ•°æ®è´¨é‡
        print("ç›‘ç£ç®¡ç†ç³»ç»Ÿæ•°æ®è´¨é‡:")
        supervision_stats = self._analyze_collection_quality(self.supervision_collection)
        
        print("\nå®‰å…¨æ’æŸ¥ç³»ç»Ÿæ•°æ®è´¨é‡:")
        inspection_stats = self._analyze_collection_quality(self.inspection_collection)
        
        return {
            'supervision': supervision_stats,
            'inspection': inspection_stats
        }
    
    def _analyze_collection_quality(self, collection):
        """åˆ†æé›†åˆæ•°æ®è´¨é‡çš„è¾…åŠ©æ–¹æ³•"""
        total_count = collection.count_documents({})
        
        # ç©ºå€¼ç»Ÿè®¡ (æ ¹æ®é›†åˆç±»å‹ä½¿ç”¨ä¸åŒå­—æ®µ)
        if collection.name == "xxj_shdwjbxx":
            empty_unit_name = collection.count_documents({"dwmc": {"$in": ["", None]}})
            empty_address = collection.count_documents({"dwdz": {"$in": ["", None]}})
        else:
            empty_unit_name = collection.count_documents({"UNIT_NAME": {"$in": ["", None]}})
            empty_address = collection.count_documents({"ADDRESS": {"$in": ["", None]}})
        
        print(f"  æ€»è®°å½•æ•°: {total_count:,}")
        print(f"  ç©ºå•ä½åç§°: {empty_unit_name:,} ({empty_unit_name/total_count*100:.2f}%)")
        print(f"  ç©ºåœ°å€: {empty_address:,} ({empty_address/total_count*100:.2f}%)")
        
        # é‡å¤æ•°æ®ç»Ÿè®¡
        unit_field = "dwmc" if collection.name == "xxj_shdwjbxx" else "UNIT_NAME"
        pipeline = [
            {"$group": {
                "_id": f"${unit_field}",
                "count": {"$sum": 1}
            }},
            {"$match": {"count": {"$gt": 1}}},
            {"$count": "duplicate_names"}
        ]
        
        duplicate_result = list(collection.aggregate(pipeline))
        duplicate_count = duplicate_result[0]["duplicate_names"] if duplicate_result else 0
        print(f"  é‡å¤å•ä½åç§°: {duplicate_count:,}")
        
        return {
            'total_count': total_count,
            'empty_unit_name': empty_unit_name,
            'empty_address': empty_address,
            'duplicate_count': duplicate_count
        }
    
    def generate_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print("\n" + "="*60)
        print("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
        print("="*60)
        
        recommendations = [
            "1. æ•°æ®é¢„å¤„ç†ä¼˜åŒ–:",
            "   - æ¸…ç†ç©ºå€¼å’Œæ— æ•ˆæ•°æ®",
            "   - æ ‡å‡†åŒ–å•ä½åç§°æ ¼å¼",
            "   - å¤„ç†é‡å¤æ•°æ®",
            "",
            "2. åŒ¹é…ç®—æ³•æ”¹è¿›:",
            "   - å®ç°æ¨¡ç³ŠåŒ¹é…ç®—æ³•",
            "   - æ·»åŠ åœ°å€ä¿¡æ¯åŒ¹é…",
            "   - ä½¿ç”¨å¤šå­—æ®µç»„åˆåŒ¹é…",
            "",
            "3. ç›¸ä¼¼åº¦è®¡ç®—ä¼˜åŒ–:",
            "   - é›†æˆå¤šç§ç›¸ä¼¼åº¦ç®—æ³•",
            "   - æ ¹æ®ä¸šåŠ¡ç‰¹ç‚¹è°ƒæ•´æƒé‡",
            "   - æ·»åŠ è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—",
            "",
            "4. æ€§èƒ½ä¼˜åŒ–:",
            "   - åˆ›å»ºé€‚å½“çš„æ•°æ®åº“ç´¢å¼•",
            "   - å®ç°åˆ†æ‰¹å¤„ç†æœºåˆ¶",
            "   - æ·»åŠ ç¼“å­˜ç­–ç•¥",
            "",
            "5. ç›‘æ§å’Œè¯„ä¼°:",
            "   - å»ºç«‹åŒ¹é…è´¨é‡è¯„ä¼°ä½“ç³»",
            "   - å®ç°å®æ—¶ç›‘æ§",
            "   - å®šæœŸè¯„ä¼°å’Œè°ƒä¼˜"
        ]
        
        for rec in recommendations:
            print(rec)
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ”¥ æ¶ˆé˜²å•ä½å»ºç­‘æ•°æ®å…³è”ç³»ç»Ÿ - æ•°æ®åŒ¹é…åˆ†æ")
        print("="*60)
        print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # æ‰§è¡Œå„é¡¹åˆ†æ
            basic_stats = self.get_basic_stats()
            match_types = self.analyze_match_types()
            similarity_dist = self.analyze_similarity_distribution()
            self.analyze_unit_name_patterns()
            self.analyze_unmatched_data()
            potential_matches = self.find_potential_matches()
            quality_stats = self.analyze_data_quality()
            self.generate_recommendations()
            
            print("\n" + "="*60)
            print("âœ… åˆ†æå®Œæˆ")
            print("="*60)
            print(f"åŸºç¡€åŒ¹é…ç‡: {basic_stats['match_rate']:.4f}%")
            print(f"å‘ç°æ½œåœ¨åŒ¹é…: {len(potential_matches)} ä¸ª")
            print("è¯¦ç»†å»ºè®®è¯·å‚è€ƒä¸Šè¿°åˆ†æç»“æœ")
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        finally:
            self.client.close()

def main():
    """ä¸»å‡½æ•°"""
    analyzer = MatchDataAnalyzer()
    analyzer.run_full_analysis()

if __name__ == "__main__":
    main() 