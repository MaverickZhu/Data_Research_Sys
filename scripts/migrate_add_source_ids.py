import sys
import os
from datetime import datetime

# Add project root to Python path for module imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.database.connection import DatabaseManager
from src.utils.config import ConfigManager
from src.utils.logger import setup_logger
from pymongo import UpdateOne
from tqdm import tqdm
from bson import ObjectId

# Setup logger
logger = setup_logger(__name__)

def migrate_add_source_ids():
    """
    One-time migration script to backfill `xfaqpc_jzdwxx_id` and `xxj_shdwjbxx_id`
    into the `unit_match_results` collection based on `primary_record_id` and `matched_record_id`.
    """
    logger.info("="*50)
    logger.info(f"å¯åŠ¨å†å²æ•°æ®è¿ç§»è„šæœ¬... ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
    logger.info("="*50)

    try:
        # Initialize database connection
        config_manager = ConfigManager()
        db_manager = DatabaseManager(config=config_manager.get_database_config())
        
        # Get collections
        results_collection = db_manager.get_collection('unit_match_results')
        xfaqpc_collection = db_manager.get_collection('xfaqpc_jzdwxx')
        xxj_collection = db_manager.get_collection('xxj_shdwjbxx')

        # Find documents that need migration
        query = {
            "$or": [
                {"xfaqpc_jzdwxx_id": {"$exists": False}},
                {"xxj_shdwjbxx_id": {"$exists": False}}
            ]
        }
        total_docs_to_migrate = results_collection.count_documents(query)
        
        if total_docs_to_migrate == 0:
            logger.info("ğŸ‰ æ‰€æœ‰è®°å½•éƒ½å·²åŒ…å«æºIDï¼Œæ— éœ€è¿ç§»ã€‚")
            return

        logger.info(f"å‘ç° {total_docs_to_migrate} æ¡è®°å½•éœ€è¦è¿ç§»ã€‚")

        # Use a cursor to iterate through documents
        cursor = results_collection.find(query, no_cursor_timeout=True)

        operations = []
        batch_size = 500
        updated_count = 0
        processed_count = 0
        error_count = 0

        with tqdm(total=total_docs_to_migrate, desc="è¿ç§»è¿›åº¦") as pbar:
            for doc in cursor:
                processed_count += 1
                update_payload = {}

                # 1. Get xfaqpc_jzdwxx_id
                primary_record_obj_id = doc.get('primary_record_id')
                if primary_record_obj_id and 'xfaqpc_jzdwxx_id' not in doc:
                    try:
                        # Ensure it's a valid ObjectId
                        if not isinstance(primary_record_obj_id, ObjectId):
                           primary_record_obj_id = ObjectId(primary_record_obj_id)
                        
                        source_doc = xfaqpc_collection.find_one(
                            {"_id": primary_record_obj_id},
                            {"ID": 1} # Projection
                        )
                        if source_doc and 'ID' in source_doc:
                            update_payload['xfaqpc_jzdwxx_id'] = source_doc['ID']
                    except Exception as e:
                        logger.warning(f"å¤„ç† primary_record_id {primary_record_obj_id} å¤±è´¥: {e}")
                        error_count += 1
                
                # 2. Get xxj_shdwjbxx_id
                matched_record_obj_id = doc.get('matched_record_id')
                if matched_record_obj_id and 'xxj_shdwjbxx_id' not in doc:
                    try:
                        # Ensure it's a valid ObjectId
                        if not isinstance(matched_record_obj_id, ObjectId):
                           matched_record_obj_id = ObjectId(matched_record_obj_id)

                        target_doc = xxj_collection.find_one(
                            {"_id": matched_record_obj_id},
                            {"ID": 1} # Projection
                        )
                        if target_doc and 'ID' in target_doc:
                            update_payload['xxj_shdwjbxx_id'] = target_doc['ID']
                    except Exception as e:
                        logger.warning(f"å¤„ç† matched_record_id {matched_record_obj_id} å¤±è´¥: {e}")
                        error_count += 1

                # If we found any ID, prepare the update operation
                if update_payload:
                    operations.append(
                        UpdateOne(
                            {"_id": doc["_id"]},
                            {"$set": update_payload}
                        )
                    )
                
                # Execute bulk write when batch is full
                if len(operations) >= batch_size:
                    try:
                        result = results_collection.bulk_write(operations)
                        updated_count += result.modified_count
                        operations = []
                    except Exception as e:
                        logger.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
                        error_count += len(operations)
                        operations = [] # Clear operations after error

                pbar.update(1)

            # Process the final batch
            if operations:
                try:
                    result = results_collection.bulk_write(operations)
                    updated_count += result.modified_count
                except Exception as e:
                    logger.error(f"æœ€åæ‰¹æ¬¡å†™å…¥å¤±è´¥: {e}")
                    error_count += len(operations)

        logger.info("\n" + "="*50)
        logger.info("è¿ç§»å®Œæˆï¼")
        logger.info(f"æ€»å¤„ç†è®°å½•: {processed_count}")
        logger.info(f"æˆåŠŸæ›´æ–°è®°å½•: {updated_count}")
        logger.info(f"å¤„ç†å¤±è´¥æˆ–è·³è¿‡è®°å½•: {error_count}")
        logger.info("="*50)

    except Exception as e:
        logger.error(f"è¿ç§»è„šæœ¬å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
    finally:
        if 'cursor' in locals() and cursor:
            cursor.close()
            logger.info("æ•°æ®åº“æ¸¸æ ‡å·²å…³é—­ã€‚")


if __name__ == "__main__":
    migrate_add_source_ids() 