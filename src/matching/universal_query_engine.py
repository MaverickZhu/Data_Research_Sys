#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨èšåˆæŸ¥è¯¢å¼•æ“
ä½¿ç”¨ç»Ÿä¸€çš„èšåˆç®¡é“æ¨¡æ¿è¿›è¡Œé«˜é€ŸæŸ¥è¯¢ï¼Œæ›¿æ¢å¤æ‚çš„$oræŸ¥è¯¢
æ”¯æŒæ‰¹é‡æŸ¥è¯¢ã€å¹¶è¡Œå¤„ç†å’Œæ™ºèƒ½ç¼“å­˜
"""

import logging
import time
from typing import Dict, List, Set, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
import pymongo
from .universal_text_matcher import UniversalTextMatcher, FieldType
from .universal_index_builder import UniversalIndexBuilder
from .similarity_scorer import SimilarityCalculator

logger = logging.getLogger(__name__)


class QueryResult:
    """æŸ¥è¯¢ç»“æœå°è£…"""
    
    def __init__(self, candidates: List[Dict], query_time: float, 
                 source_record: Dict, field_info: Dict):
        self.candidates = candidates
        self.query_time = query_time
        self.source_record = source_record
        self.field_info = field_info
        self.candidate_count = len(candidates)


class UniversalQueryEngine:
    """é€šç”¨èšåˆæŸ¥è¯¢å¼•æ“"""
    
    def __init__(self, db_manager):
        """
        åˆå§‹åŒ–é€šç”¨æŸ¥è¯¢å¼•æ“
        
        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
        """
        self.db_manager = db_manager
        self.db = db_manager.get_db() if db_manager else None
        self.text_matcher = UniversalTextMatcher(db_manager)
        self.index_builder = UniversalIndexBuilder(db_manager)
        
        # åˆå§‹åŒ–åœ°å€ç›¸ä¼¼åº¦è®¡ç®—å™¨
        try:
            # åˆ›å»ºé»˜è®¤é…ç½®
            config = {
                'address_similarity': {
                    'enable_segmentation': True,
                    'province_weight': 0.05,
                    'city_weight': 0.10,
                    'district_weight': 0.15,
                    'detail_weight': 0.02
                },
                'string_similarity': {
                    'algorithms': [
                        {'name': 'levenshtein', 'weight': 0.3},
                        {'name': 'jaro_winkler', 'weight': 0.3},
                        {'name': 'cosine', 'weight': 0.4}
                    ]
                }
            }
            self.similarity_calculator = SimilarityCalculator(config)
            logger.info("åœ°å€ç›¸ä¼¼åº¦è®¡ç®—å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"åˆå§‹åŒ–åœ°å€ç›¸ä¼¼åº¦è®¡ç®—å™¨å¤±è´¥: {e}")
            self.similarity_calculator = None
        
        # æŸ¥è¯¢é…ç½®ï¼ˆé«˜æ€§èƒ½ä¼˜åŒ–ï¼‰
        self.query_config = {
            'enable_batch_query': True,
            'batch_size': 500,  # å¢åŠ æ‰¹é‡å¤§å°ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢æ¬¡æ•°
            'max_workers': 32,  # å¢åŠ å¹¶å‘æ•°
            'enable_parallel': True,
            'enable_cache': True,
            'cache_ttl': 3600,
            'default_similarity_threshold': 0.6,
            'max_candidates_per_field': 30,  # å‡å°‘å€™é€‰æ•°é‡ï¼Œæå‡é€Ÿåº¦
            'query_timeout': 60.0,  # å¢åŠ è¶…æ—¶æ—¶é—´
            'enable_auto_index_creation': True,
            'enable_fast_mode': True  # å¯ç”¨å¿«é€Ÿæ¨¡å¼
        }
        
        # ç¼“å­˜ç³»ç»Ÿ
        self.query_cache = {}
        self.pipeline_cache = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.query_stats = {
            'total_queries': 0,
            'batch_queries': 0,
            'cache_hits': 0,
            'avg_query_time': 0.0,
            'avg_candidates_per_query': 0.0,
            'total_candidates_found': 0,
            'auto_indexes_created': 0
        }
        
        logger.info("ğŸš€ é€šç”¨èšåˆæŸ¥è¯¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def query_single_field(self, source_value: str, target_table: str, 
                          target_field: str, field_type: FieldType = None,
                          similarity_threshold: float = None) -> QueryResult:
        """
        å•å­—æ®µæŸ¥è¯¢
        
        Args:
            source_value: æºå€¼
            target_table: ç›®æ ‡è¡¨å
            target_field: ç›®æ ‡å­—æ®µå
            field_type: å­—æ®µç±»å‹
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            QueryResult: æŸ¥è¯¢ç»“æœ
        """
        start_time = time.time()
        self.query_stats['total_queries'] += 1
        
        try:
            # è‡ªåŠ¨æ£€æµ‹å­—æ®µç±»å‹
            if field_type is None:
                field_type = self.text_matcher.detect_field_type(target_field, [source_value])
            
            # è·å–é…ç½®
            config = self.text_matcher.field_configs.get(field_type, 
                                                       self.text_matcher.field_configs[FieldType.TEXT])
            
            # ä½¿ç”¨é…ç½®çš„é˜ˆå€¼æˆ–é»˜è®¤é˜ˆå€¼
            if similarity_threshold is None:
                similarity_threshold = config.similarity_threshold
            
            # é¢„å¤„ç†å’Œå…³é”®è¯æå–
            preprocessed_value = self.text_matcher._apply_preprocessing(source_value, config)
            if not preprocessed_value:
                return QueryResult([], time.time() - start_time, 
                                 {'original_value': source_value}, 
                                 {'field_type': field_type.value, 'reason': 'empty_after_preprocessing'})
            
            keywords = self.text_matcher._apply_keyword_extraction(preprocessed_value, config)
            if not keywords:
                return QueryResult([], time.time() - start_time,
                                 {'original_value': source_value, 'preprocessed_value': preprocessed_value},
                                 {'field_type': field_type.value, 'reason': 'no_keywords_extracted'})
            
            # æ‰§è¡ŒæŸ¥è¯¢
            candidates = self._execute_single_field_query(
                target_table, target_field, keywords, config, similarity_threshold
            )
            
            # æ›´æ–°ç»Ÿè®¡
            query_time = time.time() - start_time
            self._update_query_stats(query_time, len(candidates))
            
            return QueryResult(
                candidates, query_time,
                {'original_value': source_value, 'preprocessed_value': preprocessed_value, 'keywords': keywords},
                {'field_type': field_type.value, 'similarity_threshold': similarity_threshold}
            )
            
        except Exception as e:
            logger.error(f"å•å­—æ®µæŸ¥è¯¢å¤±è´¥: {target_table}.{target_field} - {str(e)}")
            return QueryResult([], time.time() - start_time,
                             {'original_value': source_value},
                             {'field_type': field_type.value if field_type else 'unknown', 'error': str(e)})
    
    def query_batch_records(self, batch_records: List[Dict], mappings: List[Dict]) -> Dict[str, QueryResult]:
        """
        æ‰¹é‡è®°å½•æŸ¥è¯¢ - è¿™æ˜¯æ›¿æ¢58ç§’ç“¶é¢ˆçš„æ ¸å¿ƒæ–¹æ³•
        
        Args:
            batch_records: æ‰¹é‡æºè®°å½•
            mappings: å­—æ®µæ˜ å°„é…ç½®
            
        Returns:
            Dict[str, QueryResult]: è®°å½•IDåˆ°æŸ¥è¯¢ç»“æœçš„æ˜ å°„
        """
        start_time = time.time()
        self.query_stats['batch_queries'] += 1
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æŸ¥è¯¢: {len(batch_records)} æ¡è®°å½•, {len(mappings)} ä¸ªæ˜ å°„")
            
            # æŒ‰ç›®æ ‡è¡¨å’Œå­—æ®µåˆ†ç»„æ˜ å°„
            grouped_mappings = self._group_mappings_by_target(mappings)
            
            # æ„å»ºæ‰¹é‡æŸ¥è¯¢è®¡åˆ’
            query_plan = self._build_batch_query_plan(batch_records, grouped_mappings)
            
            # æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢
            if self.query_config['enable_parallel'] and len(query_plan) > 1:
                batch_results = self._execute_batch_queries_parallel(query_plan)
            else:
                batch_results = self._execute_batch_queries_sequential(query_plan)
            
            # åˆå¹¶ç»“æœ
            final_results = self._merge_batch_results(batch_records, batch_results, mappings)
            
            # æ›´æ–°ç»Ÿè®¡
            batch_time = time.time() - start_time
            total_candidates = sum(len(result.candidates) for result in final_results.values())
            
            logger.info(f"âœ… æ‰¹é‡æŸ¥è¯¢å®Œæˆ: {len(final_results)} ä¸ªç»“æœ, "
                       f"æ€»å€™é€‰æ•°: {total_candidates}, è€—æ—¶: {batch_time:.3f}s, "
                       f"å¹³å‡é€Ÿåº¦: {len(batch_records) / max(batch_time, 0.001):.1f} æ¡/ç§’")
            
            return final_results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return {}
    
    def _execute_single_field_query(self, target_table: str, target_field: str, 
                                   keywords: List[str], config, similarity_threshold: float) -> List[Dict]:
        """æ‰§è¡Œå•å­—æ®µèšåˆæŸ¥è¯¢"""
        try:
            # æ„å»ºç´¢å¼•è¡¨åï¼ˆæ ¹æ®å­—æ®µååŠ¨æ€ç”Ÿæˆï¼‰
            index_table_name = f"{target_table}_{target_field}_keywords"
            
            # æ£€æŸ¥ç´¢å¼•è¡¨æ˜¯å¦å­˜åœ¨
            if index_table_name not in self.db.list_collection_names():
                if self.query_config['enable_auto_index_creation']:
                    logger.info(f"ç´¢å¼•è¡¨ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»º: {index_table_name}")
                    self.index_builder.build_field_index(target_table, target_field)
                    self.query_stats['auto_indexes_created'] += 1
                else:
                    logger.warning(f"ç´¢å¼•è¡¨ä¸å­˜åœ¨: {index_table_name}")
                    return []
            
            # è·å–èšåˆç®¡é“
            pipeline = self._build_aggregation_pipeline(
                target_field, keywords, similarity_threshold, config.max_candidates
            )
            
            # æ‰§è¡ŒèšåˆæŸ¥è¯¢
            index_collection = self.db[index_table_name]
            aggregation_results = list(index_collection.aggregate(pipeline))
            
            if not aggregation_results:
                return []
            
            # è·å–å®Œæ•´è®°å½•
            return self._fetch_full_records(target_table, aggregation_results)
            
        except Exception as e:
            logger.error(f"å•å­—æ®µæŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {target_table}.{target_field} - {str(e)}")
            return []
    
    def _build_aggregation_pipeline(self, target_field: str, keywords: List[str], 
                                   similarity_threshold: float, max_candidates: int) -> List[Dict]:
        """æ„å»ºèšåˆç®¡é“"""
        # ç¼“å­˜æ£€æŸ¥
        cache_key = f"{target_field}_{len(keywords)}_{similarity_threshold}_{max_candidates}"
        if cache_key in self.pipeline_cache:
            pipeline_template = self.pipeline_cache[cache_key]
        else:
            pipeline_template = [
                # ç¬¬1é˜¶æ®µï¼šåŒ¹é…å…³é”®è¯
                {'$match': {
                    'keyword': {'$in': keywords},
                    'field_name': target_field
                }},
                # ç¬¬2é˜¶æ®µï¼šæŒ‰æ–‡æ¡£IDåˆ†ç»„ç»Ÿè®¡
                {'$group': {
                    '_id': '$doc_id',
                    'original_value': {'$first': '$original_value'},
                    'match_count': {'$sum': 1},
                    'matched_keywords': {'$push': '$keyword'}
                }},
                # ç¬¬3é˜¶æ®µï¼šè®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†
                {'$addFields': {
                    'similarity_score': {'$divide': ['$match_count', len(keywords)]}
                }},
                # ç¬¬4é˜¶æ®µï¼šè¿‡æ»¤ä½åˆ†å€™é€‰
                {'$match': {
                    'similarity_score': {'$gte': similarity_threshold}
                }},
                # ç¬¬5é˜¶æ®µï¼šæŒ‰å¾—åˆ†æ’åº
                {'$sort': {'similarity_score': -1}},
                # ç¬¬6é˜¶æ®µï¼šé™åˆ¶å€™é€‰æ•°é‡
                {'$limit': max_candidates}
            ]
            self.pipeline_cache[cache_key] = pipeline_template
        
        # åŠ¨æ€æ›¿æ¢å…³é”®è¯ï¼ˆé¿å…ç¼“å­˜æ±¡æŸ“ï¼‰
        pipeline = []
        for stage in pipeline_template:
            if '$match' in stage and 'keyword' in stage['$match']:
                stage_copy = stage.copy()
                stage_copy['$match'] = stage['$match'].copy()
                stage_copy['$match']['keyword'] = {'$in': keywords}
                pipeline.append(stage_copy)
            elif '$addFields' in stage:
                stage_copy = stage.copy()
                stage_copy['$addFields'] = stage['$addFields'].copy()
                stage_copy['$addFields']['similarity_score'] = {'$divide': ['$match_count', len(keywords)]}
                pipeline.append(stage_copy)
            else:
                pipeline.append(stage)
        
        return pipeline
    
    def _fetch_full_records(self, target_table: str, aggregation_results: List[Dict]) -> List[Dict]:
        """è·å–å®Œæ•´è®°å½•"""
        try:
            if not aggregation_results:
                return []
            
            # æå–æ–‡æ¡£IDå¹¶è½¬æ¢ä¸ºObjectId
            from bson import ObjectId
            doc_ids = []
            for result in aggregation_results:
                doc_id = result['_id']
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºObjectId
                if isinstance(doc_id, str):
                    try:
                        doc_ids.append(ObjectId(doc_id))
                    except Exception as e:
                        logger.warning(f"æ— æ•ˆçš„ObjectIdå­—ç¬¦ä¸²: {doc_id} - {e}")
                        continue
                else:
                    doc_ids.append(doc_id)
            
            # æ‰¹é‡æŸ¥è¯¢å®Œæ•´è®°å½•
            target_collection = self.db[target_table]
            full_records = list(target_collection.find({'_id': {'$in': doc_ids}}))
            
            # åˆå¹¶èšåˆç»“æœå’Œå®Œæ•´è®°å½•
            record_map = {record['_id']: record for record in full_records}
            candidates = []
            
            for agg_result in aggregation_results:
                doc_id = agg_result['_id']
                
                # ç»Ÿä¸€IDæ ¼å¼è¿›è¡ŒåŒ¹é…
                if isinstance(doc_id, str):
                    try:
                        lookup_id = ObjectId(doc_id)
                    except:
                        continue
                else:
                    lookup_id = doc_id
                
                if lookup_id in record_map:
                    record = record_map[lookup_id].copy()
                    record['similarity_score'] = agg_result['similarity_score']
                    record['_matched_keywords'] = agg_result['matched_keywords']
                    record['_original_value'] = agg_result['original_value']
                    candidates.append(record)
            
            return candidates
            
        except Exception as e:
            logger.error(f"è·å–å®Œæ•´è®°å½•å¤±è´¥: {target_table} - {str(e)}")
            return []
    
    def _fetch_full_records_batch(self, target_table: str, aggregation_results: List[Dict]) -> List[Dict]:
        """è·å–å®Œæ•´è®°å½•ï¼ˆæ‰¹é‡æŸ¥è¯¢ç‰ˆæœ¬ï¼‰"""
        try:
            if not aggregation_results:
                return []
            
            # æå–æ–‡æ¡£IDå¹¶è½¬æ¢ä¸ºObjectId
            from bson import ObjectId
            doc_ids = []
            for result in aggregation_results:
                doc_id = result['_id']
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºObjectId
                if isinstance(doc_id, str):
                    try:
                        doc_ids.append(ObjectId(doc_id))
                    except Exception as e:
                        logger.warning(f"æ— æ•ˆçš„ObjectIdå­—ç¬¦ä¸²: {doc_id} - {e}")
                        continue
                else:
                    doc_ids.append(doc_id)
            
            # æ‰¹é‡æŸ¥è¯¢å®Œæ•´è®°å½•
            target_collection = self.db[target_table]
            full_records = list(target_collection.find({'_id': {'$in': doc_ids}}))
            
            # åˆå¹¶èšåˆç»“æœå’Œå®Œæ•´è®°å½•
            record_map = {record['_id']: record for record in full_records}
            candidates = []
            
            for agg_result in aggregation_results:
                doc_id = agg_result['_id']
                
                # ç»Ÿä¸€IDæ ¼å¼è¿›è¡ŒåŒ¹é…
                if isinstance(doc_id, str):
                    try:
                        lookup_id = ObjectId(doc_id)
                    except:
                        continue
                else:
                    lookup_id = doc_id
                
                if lookup_id in record_map:
                    record = record_map[lookup_id].copy()
                    # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨èšåˆç»“æœä¸­çš„å®é™…å­—æ®µ
                    record['_keyword_count'] = agg_result.get('match_count', 0)
                    record['_matched_keywords'] = agg_result.get('matched_keywords', [])
                    record['_original_value'] = agg_result.get('original_value', '')
                    # ã€å…³é”®ä¿®å¤ã€‘ä¸åœ¨æ­¤å¤„è®¡ç®—ç›¸ä¼¼åº¦ï¼Œç•™ç»™åç»­çš„_match_candidates_to_recordsæ–¹æ³•å¤„ç†
                    # ç›¸ä¼¼åº¦è®¡ç®—éœ€è¦æºè®°å½•çš„å…³é”®è¯ä¿¡æ¯ï¼Œè¿™é‡Œæ— æ³•è·å–
                    # record['similarity_score'] å°†åœ¨_match_candidates_to_recordsä¸­æ­£ç¡®è®¡ç®—
                    candidates.append(record)
            
            return candidates
            
        except Exception as e:
            logger.error(f"æ‰¹é‡è·å–å®Œæ•´è®°å½•å¤±è´¥: {target_table} - {str(e)}")
            return []
    
    def _group_mappings_by_target(self, mappings: List[Dict]) -> Dict[Tuple[str, str], List[Dict]]:
        """æŒ‰ç›®æ ‡è¡¨å’Œå­—æ®µåˆ†ç»„æ˜ å°„"""
        grouped = defaultdict(list)
        
        for mapping in mappings:
            target_table = mapping.get('target_table')
            target_field = mapping.get('target_field')
            if target_table and target_field:
                key = (target_table, target_field)
                grouped[key].append(mapping)
        
        return dict(grouped)
    
    def _build_batch_query_plan(self, batch_records: List[Dict], 
                               grouped_mappings: Dict[Tuple[str, str], List[Dict]]) -> List[Dict]:
        """æ„å»ºæ‰¹é‡æŸ¥è¯¢è®¡åˆ’"""
        query_plan = []
        
        for (target_table, target_field), mapping_list in grouped_mappings.items():
            # æ”¶é›†æ‰€æœ‰æºå€¼
            source_values = []
            record_mapping = {}
            
            for record in batch_records:
                record_id = str(record.get('_id', ''))
                for mapping in mapping_list:
                    source_field = mapping.get('source_field')
                    source_value = record.get(source_field)
                    
                    if source_value:
                        source_values.append({
                            'record_id': record_id,
                            'source_value': str(source_value),
                            'source_field': source_field,
                            'mapping': mapping
                        })
                        record_mapping[record_id] = record
            
            if source_values:
                query_plan.append({
                    'target_table': target_table,
                    'target_field': target_field,
                    'source_values': source_values,
                    'record_mapping': record_mapping
                })
        
        return query_plan
    
    def _execute_batch_queries_parallel(self, query_plan: List[Dict]) -> List[Dict]:
        """å¹¶è¡Œæ‰§è¡Œæ‰¹é‡æŸ¥è¯¢"""
        results = []
        
        try:
            with ThreadPoolExecutor(max_workers=self.query_config['max_workers']) as executor:
                # æ£€æŸ¥è§£é‡Šå™¨çŠ¶æ€
                import sys
                if hasattr(sys, '_getframe') and sys.is_finalizing():
                    logger.warning("è§£é‡Šå™¨æ­£åœ¨å…³é—­ï¼Œé™çº§åˆ°é¡ºåºæ‰§è¡Œ")
                    return self._execute_batch_queries_sequential(query_plan)
                
                future_to_plan = {}
                
                # å®‰å…¨åœ°æäº¤ä»»åŠ¡
                for plan in query_plan:
                    try:
                        future = executor.submit(self._execute_single_batch_query, plan)
                        future_to_plan[future] = plan
                    except RuntimeError as e:
                        if "cannot schedule new futures after interpreter shutdown" in str(e):
                            logger.warning("æ£€æµ‹åˆ°è§£é‡Šå™¨å…³é—­ï¼Œåœæ­¢æäº¤æ–°ä»»åŠ¡")
                            break
                        else:
                            raise
                
                # å¤„ç†å·²æäº¤çš„ä»»åŠ¡
                for future in as_completed(future_to_plan):
                    plan = future_to_plan[future]
                    try:
                        result = future.result(timeout=self.query_config['query_timeout'])
                        results.append(result)
                    except Exception as e:
                        error_msg = str(e)
                        if "cannot schedule new futures after interpreter shutdown" in error_msg:
                            logger.warning(f"ä»»åŠ¡æ‰§è¡Œä¸­æ–­ï¼ˆè§£é‡Šå™¨å…³é—­ï¼‰: {plan['target_table']}.{plan['target_field']}")
                        else:
                            logger.error(f"å¹¶è¡Œæ‰¹é‡æŸ¥è¯¢å¤±è´¥: {plan['target_table']}.{plan['target_field']} - {error_msg}")
                        
                        results.append({
                            'target_table': plan['target_table'],
                            'target_field': plan['target_field'],
                            'status': 'error',
                            'error': error_msg,
                            'record_results': {}
                        })
        
        except RuntimeError as e:
            if "cannot schedule new futures after interpreter shutdown" in str(e):
                logger.warning("ThreadPoolExecutoråˆ›å»ºå¤±è´¥ï¼ˆè§£é‡Šå™¨å…³é—­ï¼‰ï¼Œé™çº§åˆ°é¡ºåºæ‰§è¡Œ")
                return self._execute_batch_queries_sequential(query_plan)
            else:
                raise
        except Exception as e:
            logger.error(f"å¹¶è¡Œæ‰§è¡Œæ¡†æ¶å¼‚å¸¸: {str(e)}")
            # é™çº§åˆ°é¡ºåºæ‰§è¡Œ
            return self._execute_batch_queries_sequential(query_plan)
        
        return results
    
    def _execute_batch_queries_sequential(self, query_plan: List[Dict]) -> List[Dict]:
        """é¡ºåºæ‰§è¡Œæ‰¹é‡æŸ¥è¯¢"""
        results = []
        
        for plan in query_plan:
            try:
                result = self._execute_single_batch_query(plan)
                results.append(result)
            except Exception as e:
                logger.error(f"é¡ºåºæ‰¹é‡æŸ¥è¯¢å¤±è´¥: {plan['target_table']}.{plan['target_field']} - {str(e)}")
                results.append({
                    'target_table': plan['target_table'],
                    'target_field': plan['target_field'],
                    'status': 'error',
                    'error': str(e),
                    'record_results': {}
                })
        
        return results
    
    def _execute_single_batch_query(self, plan: Dict) -> Dict:
        """æ‰§è¡Œå•ä¸ªæ‰¹é‡æŸ¥è¯¢"""
        try:
            target_table = plan['target_table']
            target_field = plan['target_field']
            source_values = plan['source_values']
            
            # è‡ªåŠ¨æ£€æµ‹å­—æ®µç±»å‹
            sample_values = [sv['source_value'] for sv in source_values[:10]]
            field_type = self.text_matcher.detect_field_type(target_field, sample_values)
            config = self.text_matcher.field_configs.get(field_type, 
                                                       self.text_matcher.field_configs[FieldType.TEXT])
            
            # æ”¶é›†æ‰€æœ‰å…³é”®è¯
            all_keywords = set()
            record_keywords = {}
            
            for sv in source_values:
                record_id = sv['record_id']
                source_value = sv['source_value']
                
                # é¢„å¤„ç†å’Œå…³é”®è¯æå–
                preprocessed_value = self.text_matcher._apply_preprocessing(source_value, config)
                if preprocessed_value:
                    keywords = self.text_matcher._apply_keyword_extraction(preprocessed_value, config)
                    if keywords:
                        all_keywords.update(keywords)
                        record_keywords[record_id] = {
                            'keywords': keywords,
                            'preprocessed_value': preprocessed_value,
                            'original_value': source_value
                        }
            
            if not all_keywords:
                return {
                    'target_table': target_table,
                    'target_field': target_field,
                    'status': 'no_keywords',
                    'record_results': {}
                }
            
            # æ‰§è¡Œæ‰¹é‡èšåˆæŸ¥è¯¢
            candidates = self._execute_batch_aggregation_query(
                target_table, target_field, list(all_keywords), config, record_keywords
            )
            
            # ä¸ºæ¯ä¸ªè®°å½•åŒ¹é…å€™é€‰
            record_results = self._match_candidates_to_records(
                candidates, record_keywords, config.similarity_threshold
            )
            
            return {
                'target_table': target_table,
                'target_field': target_field,
                'field_type': field_type.value,
                'status': 'success',
                'total_candidates': len(candidates),
                'record_results': record_results
            }
            
        except Exception as e:
            logger.error(f"å•ä¸ªæ‰¹é‡æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise
    
    def _execute_batch_aggregation_query(self, target_table: str, target_field: str, 
                                        all_keywords: List[str], config, record_keywords: Dict) -> List[Dict]:
        """æ‰§è¡Œæ‰¹é‡èšåˆæŸ¥è¯¢"""
        try:
            # æ„å»ºç´¢å¼•è¡¨åï¼ˆæ ¹æ®å­—æ®µååŠ¨æ€ç”Ÿæˆï¼‰
            index_table_name = f"{target_table}_{target_field}_keywords"
            
            # æ£€æŸ¥ç´¢å¼•è¡¨
            if index_table_name not in self.db.list_collection_names():
                if self.query_config['enable_auto_index_creation']:
                    logger.info(f"æ‰¹é‡æŸ¥è¯¢æ—¶è‡ªåŠ¨åˆ›å»ºç´¢å¼•: {index_table_name}")
                    self.index_builder.build_field_index(target_table, target_field)
                    self.query_stats['auto_indexes_created'] += 1
                else:
                    return []
            
            # ã€å…³é”®ä¿®å¤ã€‘æ„å»ºæ‰¹é‡èšåˆç®¡é“ - ç§»é™¤é”™è¯¯çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼Œç”±åç»­æ–¹æ³•æ­£ç¡®å¤„ç†
            pipeline = [
                # ç¬¬1é˜¶æ®µï¼šåŒ¹é…å…³é”®è¯
                {'$match': {
                    'keyword': {'$in': all_keywords},
                    'field_name': target_field
                }},
                # ç¬¬2é˜¶æ®µï¼šæŒ‰æ–‡æ¡£IDåˆ†ç»„ç»Ÿè®¡
                {'$group': {
                    '_id': '$doc_id',
                    'original_value': {'$first': '$original_value'},
                    'match_count': {'$sum': 1},
                    'matched_keywords': {'$push': '$keyword'}
                }},
                # ç¬¬3é˜¶æ®µï¼šæŒ‰åŒ¹é…å…³é”®è¯æ•°é‡æ’åºï¼ˆæ›´å¤šåŒ¹é…çš„ä¼˜å…ˆï¼‰
                {'$sort': {'match_count': -1}},
                # ç¬¬4é˜¶æ®µï¼šé™åˆ¶å€™é€‰æ•°é‡ï¼ˆåœ¨ç›¸ä¼¼åº¦è®¡ç®—å‰å…ˆé™åˆ¶ï¼Œæé«˜æ€§èƒ½ï¼‰
                {'$limit': self.query_config['max_candidates_per_field'] * 2}  # å¤šå–ä¸€äº›ï¼Œåç»­å†ç²¾ç¡®è¿‡æ»¤
            ]
            
            # æ‰§è¡ŒæŸ¥è¯¢
            index_collection = self.db[index_table_name]
            
            aggregation_results = list(index_collection.aggregate(pipeline))
            
            # è·å–å®Œæ•´è®°å½•ï¼ˆæ‰¹é‡æŸ¥è¯¢ç‰ˆæœ¬ï¼‰
            return self._fetch_full_records_batch(target_table, aggregation_results)
            
        except Exception as e:
            logger.error(f"æ‰¹é‡èšåˆæŸ¥è¯¢å¤±è´¥: {target_table}.{target_field} - {str(e)}")
            return []
    
    def _match_candidates_to_records(self, candidates: List[Dict], record_keywords: Dict[str, Dict], 
                                   similarity_threshold: float) -> Dict[str, List[Dict]]:
        """å°†å€™é€‰è®°å½•åŒ¹é…åˆ°æºè®°å½•"""
        record_results = {}
        
        for record_id, keyword_info in record_keywords.items():
            record_keywords_set = set(keyword_info['keywords'])
            record_candidates = []
            
            for candidate in candidates:
                # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå _matched_keywordsï¼ˆæ¥è‡ª_fetch_full_records_batchï¼‰
                candidate_keywords = set(candidate.get('_matched_keywords', []))
                
                # è®¡ç®—å…³é”®è¯äº¤é›†ç›¸ä¼¼åº¦ï¼ˆä¸å•æŸ¥è¯¢ä¿æŒä¸€è‡´ï¼‰
                intersection = len(record_keywords_set.intersection(candidate_keywords))
                similarity = intersection / max(len(record_keywords_set), 1)
                
                if similarity >= similarity_threshold:
                    candidate_copy = candidate.copy()
                    candidate_copy['similarity_score'] = similarity
                    record_candidates.append(candidate_copy)
            
            if record_candidates:
                # æŒ‰ç›¸ä¼¼åº¦æ’åº
                record_candidates.sort(key=lambda x: x['similarity_score'], reverse=True)
                record_results[record_id] = record_candidates
        
        return record_results
    
    def _calculate_address_similarity(self, addr1: str, addr2: str) -> float:
        """
        è®¡ç®—åœ°å€ç›¸ä¼¼åº¦ï¼ˆé«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            addr1: åœ°å€1
            addr2: åœ°å€2
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # å¿«é€Ÿé¢„æ£€æŸ¥
        if not addr1 or not addr2:
            return 0.0
        if addr1 == addr2:
            return 1.0
            
        # ä½¿ç”¨ç®€å•é«˜æ•ˆçš„å­—ç¬¦é‡å æ¯”ä¾‹ï¼Œé¿å…å¤æ‚çš„åœ°å€æ ‡å‡†åŒ–
        addr1_chars = set(addr1)
        addr2_chars = set(addr2)
        intersection = len(addr1_chars.intersection(addr2_chars))
        union = len(addr1_chars.union(addr2_chars))
        
        # åŸºç¡€ç›¸ä¼¼åº¦
        base_similarity = intersection / max(union, 1)
        
        # å¿«é€Ÿå…³é”®è¯åŒ¹é…åŠ æƒ
        if any(keyword in addr1 and keyword in addr2 for keyword in ['åŒº', 'å¸‚', 'è·¯', 'è¡—', 'å·']):
            base_similarity *= 1.2  # æå‡æœ‰å…±åŒåœ°å€å…³é”®è¯çš„ç›¸ä¼¼åº¦
            
        return min(base_similarity, 1.0)
    
    def _merge_batch_results(self, batch_records: List[Dict], batch_results: List[Dict], 
                           mappings: List[Dict]) -> Dict[str, QueryResult]:
        """åˆå¹¶æ‰¹é‡æŸ¥è¯¢ç»“æœ"""
        final_results = {}
        
        # ä¸ºæ¯ä¸ªæºè®°å½•åˆ›å»ºç»“æœ
        for record in batch_records:
            record_id = str(record.get('_id', ''))
            all_candidates = []
            field_info = {'mappings': []}
            
            # æ”¶é›†æ‰€æœ‰å­—æ®µçš„å€™é€‰
            for batch_result in batch_results:
                if batch_result.get('status') == 'success':
                    record_candidates = batch_result.get('record_results', {}).get(record_id, [])
                    all_candidates.extend(record_candidates)
                    
                    if record_candidates:
                        field_info['mappings'].append({
                            'target_table': batch_result['target_table'],
                            'target_field': batch_result['target_field'],
                            'field_type': batch_result.get('field_type', 'unknown'),
                            'candidate_count': len(record_candidates)
                        })
            
            # å»é‡å’Œæ’åº
            unique_candidates = self._deduplicate_candidates(all_candidates)
            
            final_results[record_id] = QueryResult(
                unique_candidates, 0.0,  # æ‰¹é‡æŸ¥è¯¢æ—¶é—´åœ¨å¤–å±‚ç»Ÿè®¡
                record, field_info
            )
        
        return final_results
    
    def _deduplicate_candidates(self, candidates: List[Dict]) -> List[Dict]:
        """å»é‡å€™é€‰è®°å½•"""
        seen_ids = set()
        unique_candidates = []
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        candidates.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        
        for candidate in candidates:
            candidate_id = candidate.get('_id')
            if candidate_id not in seen_ids:
                seen_ids.add(candidate_id)
                unique_candidates.append(candidate)
        
        return unique_candidates
    
    def _update_query_stats(self, query_time: float, candidate_count: int):
        """æ›´æ–°æŸ¥è¯¢ç»Ÿè®¡"""
        self.query_stats['avg_query_time'] = (
            (self.query_stats['avg_query_time'] * (self.query_stats['total_queries'] - 1) + query_time) /
            self.query_stats['total_queries']
        )
        
        self.query_stats['total_candidates_found'] += candidate_count
        self.query_stats['avg_candidates_per_query'] = (
            self.query_stats['total_candidates_found'] / self.query_stats['total_queries']
        )
    
    def get_query_stats(self) -> Dict[str, Any]:
        """è·å–æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_queries': self.query_stats['total_queries'],
            'batch_queries': self.query_stats['batch_queries'],
            'cache_hits': self.query_stats['cache_hits'],
            'cache_hit_rate': self.query_stats['cache_hits'] / max(self.query_stats['total_queries'], 1),
            'avg_query_time': self.query_stats['avg_query_time'],
            'avg_candidates_per_query': self.query_stats['avg_candidates_per_query'],
            'total_candidates_found': self.query_stats['total_candidates_found'],
            'auto_indexes_created': self.query_stats['auto_indexes_created']
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.query_cache.clear()
        self.pipeline_cache.clear()
        logger.info("æŸ¥è¯¢ç¼“å­˜å·²æ¸…ç©º")
