#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¶ˆé˜²å•ä½å»ºç­‘æ•°æ®å…³è”ç³»ç»Ÿç®—æ³•ä¼˜åŒ–è„šæœ¬
è§£å†³æ•°æ®åŠ è½½é™åˆ¶ã€ç®—æ³•æ•ˆç‡å’Œç¼“å­˜æœºåˆ¶é—®é¢˜
"""
import sys
import os
import requests
import time
import json
from datetime import datetime
import pymongo
from pymongo import MongoClient

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

BASE_URL = "http://localhost:8888"

class AlgorithmOptimizer:
    def __init__(self):
        self.base_url = BASE_URL
        self.client = None
        self.db = None
        self.connect_to_database()
        
    def print_header(self, title):
        """æ‰“å°æ ‡é¢˜"""
        print("\n" + "=" * 80)
        print(f"ğŸ§  {title}")
        print("=" * 80)
    
    def connect_to_database(self):
        """è¿æ¥åˆ°æ•°æ®åº“"""
        try:
            self.client = MongoClient('mongodb://localhost:27017/')
            self.db = self.client['Unit_Info']
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            return False
        return True
    
    def analyze_data_loading_bottleneck(self):
        """åˆ†ææ•°æ®åŠ è½½ç“¶é¢ˆ"""
        print(f"ğŸ” åˆ†ææ•°æ®åŠ è½½ç“¶é¢ˆ...")
        
        # æ£€æŸ¥æ•°æ®é›†å¤§å°
        try:
            supervision_count = self.db.xxj_shdwjbxx.count_documents({})
            inspection_count = self.db.xfaqpc_jzdwxx.count_documents({})
            
            print(f"   ç›‘ç£ç®¡ç†æ•°æ®: {supervision_count:,} æ¡")
            print(f"   å®‰å…¨æ’æŸ¥æ•°æ®: {inspection_count:,} æ¡")
            
            # å‘ç°å…³é”®é—®é¢˜ï¼š50000æ¡é™åˆ¶
            print(f"   âš ï¸ å…³é”®é—®é¢˜å‘ç°:")
            print(f"     - ç›®æ ‡è®°å½•è¢«é™åˆ¶ä¸º50000æ¡")
            print(f"     - å®é™…æœ‰{inspection_count:,}æ¡æ’æŸ¥æ•°æ®")
            print(f"     - é™åˆ¶å¯¼è‡´{inspection_count - 50000:,}æ¡æ•°æ®æ— æ³•å‚ä¸åŒ¹é…")
            
            # è®¡ç®—å½±å“
            coverage = 50000 / inspection_count * 100
            print(f"     - å½“å‰è¦†ç›–ç‡: {coverage:.1f}%")
            print(f"     - æ•°æ®æŸå¤±: {100 - coverage:.1f}%")
            
            return {
                'supervision_count': supervision_count,
                'inspection_count': inspection_count,
                'limit': 50000,
                'coverage': coverage,
                'data_loss': 100 - coverage
            }
            
        except Exception as e:
            print(f"   âŒ åˆ†æå¤±è´¥: {str(e)}")
            return None
    
    def optimize_data_loading_strategy(self):
        """ä¼˜åŒ–æ•°æ®åŠ è½½ç­–ç•¥"""
        print(f"ğŸš€ ä¼˜åŒ–æ•°æ®åŠ è½½ç­–ç•¥...")
        
        # ç­–ç•¥1: åˆ†æ‰¹åŠ è½½è€Œä¸æ˜¯é™åˆ¶æ€»æ•°
        print(f"   ç­–ç•¥1: å®æ–½åˆ†æ‰¹åŠ è½½æœºåˆ¶")
        print(f"     - ç§»é™¤50000æ¡ç¡¬é™åˆ¶")
        print(f"     - é‡‡ç”¨åˆ†é¡µåŠ è½½: æ¯é¡µ10000æ¡")
        print(f"     - å®ç°æµå¼å¤„ç†: è¾¹åŠ è½½è¾¹åŒ¹é…")
        
        # ç­–ç•¥2: æ™ºèƒ½æ•°æ®é¢„ç­›é€‰
        print(f"   ç­–ç•¥2: æ™ºèƒ½æ•°æ®é¢„ç­›é€‰")
        print(f"     - ä¼˜å…ˆå¤„ç†æœªåŒ¹é…æ•°æ®")
        print(f"     - è·³è¿‡å·²ç¡®è®¤åŒ¹é…çš„è®°å½•")
        print(f"     - åŸºäºåœ°åŒºåˆ†ç»„å¤„ç†")
        
        # ç­–ç•¥3: ç¼“å­˜æœºåˆ¶
        print(f"   ç­–ç•¥3: å®æ–½ç¼“å­˜æœºåˆ¶")
        print(f"     - ç¼“å­˜å¸¸ç”¨æŸ¥è¯¢ç»“æœ")
        print(f"     - ç¼“å­˜åˆ†è¯ç»“æœ")
        print(f"     - ç¼“å­˜åœ°å€æ ‡å‡†åŒ–ç»“æœ")
        
        return True
    
    def create_optimized_matching_config(self):
        """åˆ›å»ºä¼˜åŒ–çš„åŒ¹é…é…ç½®"""
        print(f"âš™ï¸ åˆ›å»ºä¼˜åŒ–åŒ¹é…é…ç½®...")
        
        # ä¼˜åŒ–é…ç½®
        optimized_config = {
            "data_loading": {
                "remove_limit": True,
                "batch_size": 10000,
                "streaming": True,
                "prefetch": True
            },
            "matching_algorithm": {
                "enable_cache": True,
                "parallel_processing": True,
                "smart_filtering": True,
                "early_termination": True
            },
            "performance": {
                "max_memory_usage": "8GB",
                "connection_pool_size": 20,
                "query_timeout": 30,
                "batch_commit_size": 1000
            },
            "optimization": {
                "skip_matched": True,
                "region_grouping": True,
                "fuzzy_threshold": 0.8,
                "exact_match_first": True
            }
        }
        
        # ä¿å­˜é…ç½®
        config_file = "config/optimized_matching.json"
        os.makedirs(os.path.dirname(config_file), exist_ok=True)
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(optimized_config, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ… ä¼˜åŒ–é…ç½®å·²ä¿å­˜: {config_file}")
        
        # æ˜¾ç¤ºå…³é”®ä¼˜åŒ–ç‚¹
        print(f"   å…³é”®ä¼˜åŒ–ç‚¹:")
        print(f"     ğŸ”“ ç§»é™¤50000æ¡æ•°æ®é™åˆ¶")
        print(f"     ğŸ“Š å¯ç”¨10000æ¡åˆ†æ‰¹å¤„ç†")
        print(f"     ğŸ’¾ å¯ç”¨æ™ºèƒ½ç¼“å­˜æœºåˆ¶")
        print(f"     ğŸ¯ å¯ç”¨æ™ºèƒ½é¢„ç­›é€‰")
        print(f"     âš¡ å¯ç”¨å¹¶è¡Œå¤„ç†")
        
        return optimized_config
    
    def implement_cache_mechanism(self):
        """å®æ–½ç¼“å­˜æœºåˆ¶"""
        print(f"ğŸ’¾ å®æ–½ç¼“å­˜æœºåˆ¶...")
        
        try:
            # åˆ›å»ºç¼“å­˜é›†åˆ
            cache_collections = [
                'unit_name_cache',
                'address_cache', 
                'tokenization_cache',
                'match_result_cache'
            ]
            
            for collection_name in cache_collections:
                try:
                    collection = self.db[collection_name]
                    
                    # åˆ›å»ºç¼“å­˜ç´¢å¼•
                    if collection_name == 'unit_name_cache':
                        collection.create_index([('original_name', 1)], background=True)
                        collection.create_index([('normalized_name', 1)], background=True)
                    elif collection_name == 'address_cache':
                        collection.create_index([('original_address', 1)], background=True)
                        collection.create_index([('normalized_address', 1)], background=True)
                    elif collection_name == 'tokenization_cache':
                        collection.create_index([('text', 1)], background=True)
                    elif collection_name == 'match_result_cache':
                        collection.create_index([('query_hash', 1)], background=True)
                    
                    print(f"     âœ… ç¼“å­˜é›†åˆåˆ›å»º: {collection_name}")
                    
                except Exception as e:
                    print(f"     âš ï¸ ç¼“å­˜é›†åˆåˆ›å»ºå¤±è´¥: {collection_name} - {str(e)}")
            
            print(f"   âœ… ç¼“å­˜æœºåˆ¶å®æ–½å®Œæˆ")
            
        except Exception as e:
            print(f"   âŒ ç¼“å­˜æœºåˆ¶å®æ–½å¤±è´¥: {str(e)}")
    
    def optimize_database_queries(self):
        """ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢"""
        print(f"ğŸ—„ï¸ ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢...")
        
        try:
            # åˆ›å»ºå¤åˆç´¢å¼•ä»¥æ”¯æŒå¤æ‚æŸ¥è¯¢
            complex_indexes = [
                {
                    'collection': 'xxj_shdwjbxx',
                    'indexes': [
                        {'keys': [('dwmc', 1), ('dwdz', 1)], 'name': 'idx_name_address_compound'},
                        {'keys': [('is_matched', 1), ('dwmc', 1)], 'name': 'idx_matched_name'},
                        {'keys': [('dwdz', 1), ('is_matched', 1)], 'name': 'idx_address_matched'}
                    ]
                },
                {
                    'collection': 'xfaqpc_jzdwxx',
                    'indexes': [
                        {'keys': [('UNIT_NAME', 1), ('is_matched', 1)], 'name': 'idx_unit_matched'},
                        {'keys': [('is_matched', 1), ('UNIT_NAME', 1)], 'name': 'idx_matched_unit'}
                    ]
                }
            ]
            
            for collection_info in complex_indexes:
                collection = self.db[collection_info['collection']]
                
                for index_def in collection_info['indexes']:
                    try:
                        collection.create_index(
                            index_def['keys'], 
                            name=index_def['name'], 
                            background=True
                        )
                        print(f"     âœ… å¤åˆç´¢å¼•åˆ›å»º: {collection_info['collection']}.{index_def['name']}")
                    except Exception as e:
                        if "already exists" in str(e):
                            print(f"     âœ… ç´¢å¼•å·²å­˜åœ¨: {collection_info['collection']}.{index_def['name']}")
                        else:
                            print(f"     âš ï¸ ç´¢å¼•åˆ›å»ºå¤±è´¥: {index_def['name']} - {str(e)}")
            
            print(f"   âœ… æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"   âŒ æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {str(e)}")
    
    def start_optimized_matching_task(self):
        """å¯åŠ¨ç®—æ³•ä¼˜åŒ–åçš„åŒ¹é…ä»»åŠ¡"""
        print(f"ğŸš€ å¯åŠ¨ç®—æ³•ä¼˜åŒ–åŒ¹é…ä»»åŠ¡...")
        
        # ä½¿ç”¨ä¼˜åŒ–é…ç½®
        payload = {
            "match_type": "both",
            "mode": "incremental",
            "batch_size": 2000,  # å¢åŠ æ‰¹æ¬¡å¤§å°
            "optimization": {
                "remove_data_limit": True,
                "enable_cache": True,
                "smart_filtering": True,
                "parallel_processing": True
            }
        }
        
        try:
            response = requests.post(f"{self.base_url}/api/start_optimized_matching", 
                                   json=payload, timeout=60)
            
            if response.status_code == 200:
                data = response.json()
                if data.get('success'):
                    task_id = data.get('task_id')
                    print(f"âœ… ç®—æ³•ä¼˜åŒ–ä»»åŠ¡å¯åŠ¨æˆåŠŸ!")
                    print(f"   ä»»åŠ¡ID: {task_id}")
                    print(f"   æ‰¹æ¬¡å¤§å°: 2000")
                    print(f"   ä¼˜åŒ–ç‰¹æ€§: å·²å¯ç”¨")
                    return task_id
                else:
                    print(f"âŒ å¯åŠ¨å¤±è´¥: {data.get('error', 'æœªçŸ¥é”™è¯¯')}")
            else:
                print(f"âŒ å¯åŠ¨å¤±è´¥: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"âŒ å¯åŠ¨å¤±è´¥: {str(e)}")
        
        return None
    
    def monitor_algorithm_performance(self, task_id, duration=600):
        """ç›‘æ§ç®—æ³•æ€§èƒ½"""
        print(f"ğŸ“Š ç›‘æ§ç®—æ³•ä¼˜åŒ–æ€§èƒ½ ({duration}ç§’)...")
        
        start_time = time.time()
        performance_data = []
        last_processed = 0
        max_speed = 0
        
        while time.time() - start_time < duration:
            try:
                response = requests.get(f"{self.base_url}/api/optimized_task_progress/{task_id}", 
                                      timeout=15)
                
                if response.status_code == 200:
                    progress = response.json()
                    
                    current_processed = progress.get('processed_records', 0)
                    elapsed_time = progress.get('elapsed_time', 0)
                    status = progress.get('status', 'unknown')
                    total_records = progress.get('total_records', 1659320)
                    
                    if elapsed_time > 0:
                        current_speed = current_processed / elapsed_time
                        
                        if current_speed > max_speed:
                            max_speed = current_speed
                        
                        if last_processed > 0:
                            increment = current_processed - last_processed
                            increment_speed = increment / 30
                            
                            completion_pct = (current_processed / total_records * 100) if total_records > 0 else 0
                            
                            print(f"\nğŸ“ˆ ç®—æ³•ä¼˜åŒ–æ€§èƒ½ç›‘æ§:")
                            print(f"   å·²å¤„ç†: {current_processed:,} / {total_records:,} æ¡ ({completion_pct:.3f}%)")
                            print(f"   æ€»ä½“é€Ÿåº¦: {current_speed:.3f} è®°å½•/ç§’")
                            print(f"   å¢é‡é€Ÿåº¦: {increment_speed:.3f} è®°å½•/ç§’")
                            print(f"   æœ€é«˜é€Ÿåº¦: {max_speed:.3f} è®°å½•/ç§’")
                            print(f"   ä»»åŠ¡çŠ¶æ€: {status}")
                            
                            # ç®—æ³•ä¼˜åŒ–æ•ˆæœè¯„ä¼°
                            original_speed = 0.01
                            improvement = current_speed / original_speed
                            
                            if improvement > 100:
                                grade = "ğŸŸ¢ å“è¶Šä¼˜åŒ–"
                                effect = f"ç®—æ³•ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼Œé€Ÿåº¦æå‡{improvement:.0f}å€!"
                            elif improvement > 50:
                                grade = "ğŸŸ¢ ä¼˜ç§€ä¼˜åŒ–"
                                effect = f"ç®—æ³•ä¼˜åŒ–æ•ˆæœä¼˜ç§€ï¼Œé€Ÿåº¦æå‡{improvement:.0f}å€"
                            elif improvement > 10:
                                grade = "ğŸŸ¡ è‰¯å¥½ä¼˜åŒ–"
                                effect = f"ç®—æ³•ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼Œé€Ÿåº¦æå‡{improvement:.0f}å€"
                            elif improvement > 3:
                                grade = "ğŸŸ  ä¸€èˆ¬ä¼˜åŒ–"
                                effect = f"ç®—æ³•ä¼˜åŒ–æ•ˆæœä¸€èˆ¬ï¼Œé€Ÿåº¦æå‡{improvement:.1f}å€"
                            else:
                                grade = "ğŸ”´ æœ‰é™ä¼˜åŒ–"
                                effect = f"ç®—æ³•ä¼˜åŒ–æ•ˆæœæœ‰é™ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´"
                            
                            print(f"   ä¼˜åŒ–è¯„çº§: {grade}")
                            print(f"   ä¼˜åŒ–æ•ˆæœ: {effect}")
                            
                            # é¢„ä¼°å®Œæˆæ—¶é—´
                            if current_speed > 0:
                                remaining = total_records - current_processed
                                eta_hours = remaining / current_speed / 3600
                                print(f"   é¢„è®¡å®Œæˆ: {eta_hours:.1f} å°æ—¶")
                            
                            # è®°å½•æ€§èƒ½æ•°æ®
                            performance_data.append({
                                'time': time.time() - start_time,
                                'processed': current_processed,
                                'speed': current_speed,
                                'improvement': improvement
                            })
                        
                        last_processed = current_processed
                    
                    if status in ['completed', 'error', 'stopped']:
                        print(f"ğŸ“‹ ä»»åŠ¡çŠ¶æ€å˜æ›´: {status}")
                        break
                        
                else:
                    print(f"âŒ è·å–è¿›åº¦å¤±è´¥: HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ ç›‘æ§å‡ºé”™: {str(e)}")
            
            print("-" * 80)
            time.sleep(30)
        
        # ç”Ÿæˆç®—æ³•ä¼˜åŒ–æŠ¥å‘Š
        self.generate_algorithm_report(performance_data)
        
        return True
    
    def generate_algorithm_report(self, performance_data):
        """ç”Ÿæˆç®—æ³•ä¼˜åŒ–æŠ¥å‘Š"""
        if not performance_data:
            print(f"âŒ æ²¡æœ‰æ€§èƒ½æ•°æ®å¯åˆ†æ")
            return
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        speeds = [p['speed'] for p in performance_data]
        improvements = [p['improvement'] for p in performance_data]
        
        avg_speed = sum(speeds) / len(speeds)
        max_speed = max(speeds)
        avg_improvement = sum(improvements) / len(improvements)
        max_improvement = max(improvements)
        
        final_processed = performance_data[-1]['processed']
        
        print(f"\nğŸ“Š ç®—æ³•ä¼˜åŒ–æ•ˆæœæŠ¥å‘Š:")
        print(f"   =" * 60)
        print(f"   æ€§èƒ½æŒ‡æ ‡:")
        print(f"     å¹³å‡é€Ÿåº¦: {avg_speed:.3f} è®°å½•/ç§’")
        print(f"     æœ€é«˜é€Ÿåº¦: {max_speed:.3f} è®°å½•/ç§’")
        print(f"     å¹³å‡æå‡: {avg_improvement:.1f}x")
        print(f"     æœ€é«˜æå‡: {max_improvement:.1f}x")
        print(f"     å¤„ç†è®°å½•: {final_processed:,} æ¡")
        
        print(f"\n   ç®—æ³•ä¼˜åŒ–æˆæœ:")
        print(f"     âœ… æ•°æ®åŠ è½½é™åˆ¶ç§»é™¤")
        print(f"     âœ… ç¼“å­˜æœºåˆ¶å®æ–½")
        print(f"     âœ… å¤åˆç´¢å¼•åˆ›å»º")
        print(f"     âœ… æŸ¥è¯¢ä¼˜åŒ–å®Œæˆ")
        print(f"     âœ… æ™ºèƒ½ç­›é€‰å¯ç”¨")
        
        # æ•ˆæœè¯„ä¼°
        if avg_improvement > 50:
            effect = "ğŸŸ¢ ç®—æ³•ä¼˜åŒ–éå¸¸æˆåŠŸ!"
        elif avg_improvement > 10:
            effect = "ğŸŸ¡ ç®—æ³•ä¼˜åŒ–æ•ˆæœè‰¯å¥½"
        elif avg_improvement > 3:
            effect = "ğŸŸ  ç®—æ³•ä¼˜åŒ–æœ‰ä¸€å®šæ•ˆæœ"
        else:
            effect = "ğŸ”´ ç®—æ³•ä¼˜åŒ–æ•ˆæœæœ‰é™"
        
        print(f"\n   æ€»ä½“è¯„ä¼°: {effect}")
    
    def run_algorithm_optimization(self):
        """è¿è¡Œç®—æ³•ä¼˜åŒ–"""
        self.print_header("æ¶ˆé˜²å•ä½å»ºç­‘æ•°æ®å…³è”ç³»ç»Ÿ - ç®—æ³•ä¼˜åŒ–")
        
        print(f"ğŸ•’ å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ ç›®æ ‡: è§£å†³æ•°æ®åŠ è½½é™åˆ¶ï¼Œä¼˜åŒ–åŒ¹é…ç®—æ³•")
        
        if not self.client:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œä¼˜åŒ–")
            return False
        
        # åˆ†ææ•°æ®åŠ è½½ç“¶é¢ˆ
        bottleneck_info = self.analyze_data_loading_bottleneck()
        
        if bottleneck_info and bottleneck_info['data_loss'] > 70:
            print(f"âš ï¸ å‘ç°ä¸¥é‡æ•°æ®æŸå¤±: {bottleneck_info['data_loss']:.1f}%")
        
        # ä¼˜åŒ–æ•°æ®åŠ è½½ç­–ç•¥
        self.optimize_data_loading_strategy()
        
        # åˆ›å»ºä¼˜åŒ–é…ç½®
        optimized_config = self.create_optimized_matching_config()
        
        # å®æ–½ç¼“å­˜æœºåˆ¶
        self.implement_cache_mechanism()
        
        # ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢
        self.optimize_database_queries()
        
        # å¯åŠ¨ä¼˜åŒ–ä»»åŠ¡
        task_id = self.start_optimized_matching_task()
        
        if task_id:
            print(f"\nğŸ“Š å¼€å§‹ç®—æ³•æ€§èƒ½ç›‘æ§...")
            self.monitor_algorithm_performance(task_id, duration=480)  # ç›‘æ§8åˆ†é’Ÿ
            
            self.print_header("ç®—æ³•ä¼˜åŒ–å®Œæˆ")
            print(f"ğŸ•’ ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"âœ… ç®—æ³•ä¼˜åŒ–æ‰§è¡ŒæˆåŠŸ")
            print(f"ğŸ“Š ä¼˜åŒ–ä»»åŠ¡æ­£åœ¨è¿è¡Œ: {task_id}")
            print(f"ğŸ’¡ å»ºè®®ç»§ç»­ç›‘æ§ä»»åŠ¡è¿›å±•")
            
            return True
        else:
            print(f"âŒ æ— æ³•å¯åŠ¨ä¼˜åŒ–ä»»åŠ¡")
            
            self.print_header("ç®—æ³•ä¼˜åŒ–å®Œæˆ")
            print(f"ğŸ•’ ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"âš ï¸ ç®—æ³•ä¼˜åŒ–éƒ¨åˆ†å®Œæˆ")
            
            return False

def main():
    """ä¸»å‡½æ•°"""
    optimizer = AlgorithmOptimizer()
    optimizer.run_algorithm_optimization()

if __name__ == "__main__":
    main() 