"""
ç”¨æˆ·æ•°æ®æ™ºèƒ½åŒ¹é…å™¨
ä¸“é—¨å¤„ç†ç”¨æˆ·ä¸Šä¼ æ•°æ®çš„æ™ºèƒ½åŒ¹é…ï¼Œå¤ç”¨åŸé¡¹ç›®æˆç†Ÿç®—æ³•
"""

import logging
import re
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
from datetime import datetime
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from src.utils.memory_manager import get_memory_manager, check_memory_before_task

# å¯¼å…¥åŸé¡¹ç›®æˆç†ŸåŒ¹é…ç®—æ³•
from .exact_matcher import ExactMatcher
from .fuzzy_matcher import FuzzyMatcher
from .optimized_match_processor import OptimizedMatchProcessor
from .enhanced_fuzzy_matcher import EnhancedFuzzyMatcher
from .similarity_scorer import SimilarityCalculator
from .match_result import MatchResult
from .smart_index_manager import SmartIndexManager
from .optimized_prefilter import OptimizedPrefilter, CandidateRanker
from .graph_matcher import GraphMatcher
from .slice_enhanced_matcher import SliceEnhancedMatcher
from .universal_query_engine import UniversalQueryEngine
from .hierarchical_matcher import HierarchicalMatcher

logger = logging.getLogger(__name__)


class UserDataMatcher:
    """ç”¨æˆ·æ•°æ®æ™ºèƒ½åŒ¹é…å™¨"""
    
    def __init__(self, db_manager=None, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ç”¨æˆ·æ•°æ®åŒ¹é…å™¨
        
        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            config: é…ç½®å‚æ•°
        """
        self.db_manager = db_manager
        self.config = config or self._get_default_config()
        
        # åˆå§‹åŒ–å„ç§åŒ¹é…ç®—æ³•
        self._init_matchers()
        
        # åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–ç»„ä»¶
        self.index_manager = SmartIndexManager(db_manager) if db_manager else None
        self.prefilter = None  # å°†åœ¨å¯åŠ¨åŒ¹é…ä»»åŠ¡æ—¶åˆå§‹åŒ–
        self.candidate_ranker = None  # å°†åœ¨å¯åŠ¨åŒ¹é…ä»»åŠ¡æ—¶åˆå§‹åŒ–
        
        # åˆå§‹åŒ–é«˜æ€§èƒ½åŒ¹é…ç»„ä»¶ï¼ˆåŸé¡¹ç›®ç®—æ³•ï¼‰
        self.graph_matcher = None  # å›¾åŒ¹é…å™¨
        self.slice_matcher = None  # åˆ‡ç‰‡å¢å¼ºåŒ¹é…å™¨
        self.hierarchical_matcher = None  # åˆ†å±‚åŒ¹é…å™¨
        self.use_high_performance = True  # å¯ç”¨é«˜æ€§èƒ½æ¨¡å¼
        self.use_hierarchical_matching = True  # å¯ç”¨åˆ†å±‚åŒ¹é…
        
        # åˆå§‹åŒ–é€šç”¨æŸ¥è¯¢å¼•æ“ï¼ˆæ›¿æ¢58ç§’ç“¶é¢ˆï¼‰
        self.universal_query_engine = UniversalQueryEngine(db_manager)
        
        # åŒ¹é…ä»»åŠ¡ç¼“å­˜
        self.running_tasks = {}
        self.stop_flags = {}  # ä»»åŠ¡åœæ­¢æ ‡å¿—
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_processed': 0,
            'total_matched': 0,
            'avg_processing_time': 0,
            'index_creation_time': 0,
            'prefilter_performance': {}
        }
        
        logger.info("ç”¨æˆ·æ•°æ®æ™ºèƒ½åŒ¹é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'exact_match': {
                'fields': {
                    'unit_name': {
                        'source_field': 'UNIT_NAME',
                        'target_field': 'dwmc',
                        'match_type': 'string',
                        'weight': 1.0,
                        'required': True
                    },
                    'credit_code': {
                        'source_field': 'TYSHXYDM',
                        'target_field': 'tyshxydm',
                        'match_type': 'string',
                        'weight': 1.0,
                        'required': False
                    }
                },
                'enable_preprocessing': True,
                'case_sensitive': False
            },
            'fuzzy_match': {
                'fields': {
                    'unit_name': {
                        'source_field': 'UNIT_NAME',
                        'target_field': 'dwmc',
                        'match_type': 'string',
                        'weight': 1.0,
                        'required': True
                    },
                    'address': {
                        'source_field': 'ADDRESS',
                        'target_field': 'dz',
                        'match_type': 'address',
                        'weight': 0.8,
                        'required': False
                    }
                },
                'similarity_threshold': 0.7,
                'enable_chinese_processing': True,
                'enable_vector_similarity': True
            },
            'similarity': {
                'string_similarity': {
                    'chinese_processing': {
                        'enable': True,
                        'use_pinyin': True,
                        'use_jieba': True
                    }
                },
                'numeric_similarity': {
                    'tolerance': 0.1
                },
                'address_similarity': {
                    'enable_fuzzy_match': True,
                    'similarity_threshold': 0.6
                }
            },
            'performance': {
                'batch_size': 50000,  # åŸé¡¹ç›®çº§åˆ«ä¼˜åŒ–
                'max_workers': 4,
                'enable_cache': True
            }
        }
    
    def _init_matchers(self):
        """åˆå§‹åŒ–å„ç§åŒ¹é…ç®—æ³•"""
        try:
            # ç²¾ç¡®åŒ¹é…å™¨
            self.exact_matcher = ExactMatcher(self.config)
            
            # æ¨¡ç³ŠåŒ¹é…å™¨
            self.fuzzy_matcher = FuzzyMatcher(self.config)
            
            # å¢å¼ºæ¨¡ç³ŠåŒ¹é…å™¨
            self.enhanced_fuzzy_matcher = EnhancedFuzzyMatcher(self.config)
            
            # ç›¸ä¼¼åº¦è¯„åˆ†å™¨
            self.similarity_calculator = SimilarityCalculator(self.config.get('similarity', {}))
            
            # ä¼˜åŒ–åŒ¹é…å¤„ç†å™¨ï¼ˆå¦‚æœéœ€è¦å¤æ‚åŒ¹é…ï¼‰
            # æ³¨æ„ï¼šOptimizedMatchProcessoréœ€è¦ConfigManagerï¼Œè¿™é‡Œå…ˆä¸åˆå§‹åŒ–
            # å¦‚æœéœ€è¦ä½¿ç”¨ï¼Œå¯ä»¥åœ¨å…·ä½“åŒ¹é…æ–¹æ³•ä¸­ä¸´æ—¶åˆ›å»º
            self.optimized_processor = None
            
            logger.info("åŒ¹é…ç®—æ³•åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åŒ¹é…ç®—æ³•åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def start_matching_task(self, task_config: Dict[str, Any]) -> str:
        """
        å¯åŠ¨åŒ¹é…ä»»åŠ¡ï¼ˆåŒ…å«æ€§èƒ½ä¼˜åŒ–ï¼‰
        
        Args:
            task_config: ä»»åŠ¡é…ç½®
            
        Returns:
            str: ä»»åŠ¡ID
        """
        task_id = task_config['task_id']
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºä¼˜åŒ–ç´¢å¼•
            if self.index_manager and task_config.get('mappings'):
                logger.info("å¼€å§‹åˆ›å»ºä¼˜åŒ–ç´¢å¼•...")
                index_start_time = time.time()
                
                index_result = self.index_manager.create_mapping_optimized_indexes(
                    mappings=task_config['mappings'],
                    source_table=task_config.get('source_table', ''),
                    target_tables=task_config.get('target_tables', [])
                )
                
                index_creation_time = time.time() - index_start_time
                self.performance_stats['index_creation_time'] = index_creation_time
                
                logger.info(f"ç´¢å¼•åˆ›å»ºå®Œæˆï¼Œè€—æ—¶: {index_creation_time:.2f}ç§’, "
                           f"åˆ›å»º: {index_result.get('created_count', 0)}, "
                           f"è·³è¿‡: {index_result.get('skipped_count', 0)}")
            
            # ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–é«˜æ€§èƒ½åŒ¹é…ç»„ä»¶
            if task_config.get('mappings') and self.use_high_performance:
                logger.info("åˆå§‹åŒ–é«˜æ€§èƒ½åŒ¹é…ç»„ä»¶...")
                
                # åˆå§‹åŒ–å›¾åŒ¹é…å™¨
                if self.db_manager and hasattr(self.db_manager, 'mongo_client') and self.db_manager.mongo_client is not None:
                    try:
                        # è·å–æ•°æ®åº“å®ä¾‹
                        db = self.db_manager.get_db()
                        logger.debug(f"æˆåŠŸè·å–æ•°æ®åº“å®ä¾‹: {db}")
                            
                        if db is not None:
                            self.graph_matcher = GraphMatcher(db, self.config)
                            logger.info("å›¾åŒ¹é…å™¨åˆå§‹åŒ–å®Œæˆ")
                            
                            # é¢„å…ˆæ„å»ºå›¾ç»“æ„ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
                            logger.info("å¼€å§‹é¢„å»ºå›¾ç»“æ„...")
                            graph_build_start = time.time()
                            self.graph_matcher.build_graph(limit=50000)  # é¢„å»ºæ›´å¤§çš„å›¾
                            logger.info(f"å›¾ç»“æ„é¢„å»ºå®Œæˆï¼Œè€—æ—¶: {time.time() - graph_build_start:.2f}ç§’")
                            
                            # åˆå§‹åŒ–åˆ‡ç‰‡å¢å¼ºåŒ¹é…å™¨
                            self.slice_matcher = SliceEnhancedMatcher(self.db_manager)
                            logger.info("åˆ‡ç‰‡å¢å¼ºåŒ¹é…å™¨åˆå§‹åŒ–å®Œæˆ")
                        else:
                            logger.warning("æ•°æ®åº“è¿æ¥æ— æ•ˆï¼Œè·³è¿‡å›¾åŒ¹é…å™¨åˆå§‹åŒ–")
                    except Exception as e:
                        logger.error(f"å›¾åŒ¹é…å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                        self.graph_matcher = None
                        self.slice_matcher = None
                
                # åˆå§‹åŒ–é¢„è¿‡æ»¤ç³»ç»Ÿï¼ˆä½œä¸ºè¡¥å……ï¼‰
                self.prefilter = OptimizedPrefilter(self.db_manager, task_config['mappings'])
                self.candidate_ranker = CandidateRanker(task_config['mappings'])
                
                # åˆå§‹åŒ–åˆ†å±‚åŒ¹é…å™¨ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
                if self.use_hierarchical_matching and task_config.get('mappings'):
                    try:
                        self.hierarchical_matcher = HierarchicalMatcher(
                            mapping_config=task_config['mappings'],
                            similarity_calculator=self.similarity_calculator
                        )
                        
                        # è®°å½•åˆ†å±‚åŒ¹é…ç»Ÿè®¡
                        stats = self.hierarchical_matcher.get_performance_stats()
                        logger.info(f"åˆ†å±‚åŒ¹é…å™¨åˆå§‹åŒ–å®Œæˆ - ä¸»è¦å­—æ®µ: {stats['primary_fields']}, "
                                  f"è¾…åŠ©å­—æ®µ: {stats['secondary_fields']}, "
                                  f"é˜ˆå€¼é…ç½®: {stats['threshold_config']}")
                    except Exception as e:
                        logger.warning(f"åˆ†å±‚åŒ¹é…å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                        self.hierarchical_matcher = None
                
                logger.info("é«˜æ€§èƒ½åŒ¹é…ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
            # ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºåå°çº¿ç¨‹æ‰§è¡ŒåŒ¹é…
            thread = threading.Thread(
                target=self._execute_optimized_matching_task,
                args=(task_id, task_config)
            )
            thread.daemon = True
            thread.start()
            
            self.running_tasks[task_id] = {
                'thread': thread,
                'config': task_config,
                'start_time': datetime.now(),
                'has_optimization': True
            }
            
            logger.info(f"å¯åŠ¨ç”¨æˆ·æ•°æ®åŒ¹é…ä»»åŠ¡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰: {task_id}")
            return task_id
            
        except Exception as e:
            logger.error(f"å¯åŠ¨ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
            # é™çº§åˆ°æ™®é€šåŒ¹é…
            return self._start_fallback_matching_task(task_config)
    
    def _start_fallback_matching_task(self, task_config: Dict[str, Any]) -> str:
        """å¯åŠ¨é™çº§åŒ¹é…ä»»åŠ¡ï¼ˆä¸ä½¿ç”¨ä¼˜åŒ–ï¼‰"""
        task_id = task_config['task_id']
        
        logger.info(f"å¯åŠ¨é™çº§åŒ¹é…ä»»åŠ¡: {task_id}")
        
        thread = threading.Thread(
            target=self._execute_matching_task,
            args=(task_id, task_config)
        )
        thread.daemon = True
        thread.start()
        
        self.running_tasks[task_id] = {
            'thread': thread,
            'config': task_config,
            'start_time': datetime.now(),
            'has_optimization': False
        }
        
        return task_id
    
    def _execute_optimized_matching_task(self, task_id: str, config: Dict[str, Any]):
        """
        æ‰§è¡Œä¼˜åŒ–åŒ¹é…ä»»åŠ¡ï¼ˆåå°çº¿ç¨‹ï¼‰
        """
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œä¼˜åŒ–åŒ¹é…ä»»åŠ¡: {task_id}")
            
            # ä»»åŠ¡æ‰§è¡Œå‰å†…å­˜æ£€æŸ¥
            if not check_memory_before_task(min_available_mb=2000):
                error_msg = "ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œæ— æ³•å¯åŠ¨åŒ¹é…ä»»åŠ¡"
                logger.error(error_msg)
                self._update_task_status(task_id, 'failed', error=error_msg)
                return
            
            # è·å–å†…å­˜ç®¡ç†å™¨
            memory_manager = get_memory_manager()
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self._update_task_status(task_id, 'running', 0, 'åˆå§‹åŒ–åŒ¹é…ç¯å¢ƒ')
            
            # è·å–æºè¡¨å’Œæ˜ å°„é…ç½®
            source_table = config.get('source_table', '')
            mappings = config.get('mappings', [])
            
            if not source_table or not mappings:
                raise ValueError("ç¼ºå°‘æºè¡¨æˆ–å­—æ®µæ˜ å°„é…ç½®")
            
            # è·å–æºæ•°æ® - æ·»åŠ æ•°æ®åº“è¿æ¥æ£€æŸ¥å’Œé‡è¿æœºåˆ¶
            try:
                # ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨çš„get_collectionæ–¹æ³•ï¼Œå®ƒåŒ…å«äº†è¿æ¥æ£€æŸ¥å’Œé‡è¿é€»è¾‘
                source_collection = self.db_manager.get_collection(source_table)
                db = source_collection.database
                logger.info(f"æ•°æ®åº“è¿æ¥æ­£å¸¸ï¼Œè·å–æºè¡¨: {source_table}")
            except Exception as db_error:
                logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œå°è¯•é‡æ–°è¿æ¥: {str(db_error)}")
                # å¼ºåˆ¶é‡æ–°è¿æ¥æ•°æ®åº“
                try:
                    self.db_manager._reconnect_mongodb()
                    source_collection = self.db_manager.get_collection(source_table)
                    db = source_collection.database
                    logger.info(f"æ•°æ®åº“é‡è¿æˆåŠŸï¼Œè·å–æºè¡¨: {source_table}")
                except Exception as reconnect_error:
                    error_msg = f"æ•°æ®åº“é‡è¿å¤±è´¥: {str(reconnect_error)}"
                    logger.error(error_msg)
                    self._update_task_status(task_id, 'failed', error=error_msg)
                    return
            
            # ç»Ÿè®¡æ€»è®°å½•æ•°
            total_records = source_collection.count_documents({})
            logger.info(f"æºè¡¨ {source_table} æ€»è®°å½•æ•°: {total_records}")
            
            if total_records == 0:
                self._update_task_status(task_id, 'completed', 100, 'æºè¡¨æ— æ•°æ®', 0, 0, 0)
                return
            
            # åˆå§‹åŒ–è¿›åº¦
            processed_count = 0
            matched_count = 0
            batch_size = config.get('batch_size', 5000)  # ä¼˜åŒ–æ‰¹æ¬¡å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
            
            # åˆ›å»ºç»“æœé›†åˆ
            result_collection_name = f'user_match_results_{task_id}'
            result_collection = db[result_collection_name]
            
            # åˆ†æ‰¹å¤„ç†è®°å½•
            batch_start_time = time.time()
            
            for batch_records in self._get_source_records_batch(source_collection, batch_size):
                if self.stop_flags.get(task_id, False):
                    logger.info(f"ä»»åŠ¡ {task_id} è¢«åœæ­¢")
                    self._update_task_status(task_id, 'stopped', 100, 'ä»»åŠ¡å·²åœæ­¢', 
                                           processed_count, total_records, matched_count)
                    return
                
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_results = self._process_optimized_batch(
                    batch_records, mappings, source_table, task_id
                )
                
                # ä¿å­˜åŒ¹é…ç»“æœ - æ·»åŠ æ•°æ®åº“è¿æ¥æ£€æŸ¥
                if batch_results:
                    try:
                        result_collection.insert_many(batch_results)
                        matched_count += len(batch_results)
                    except Exception as e:
                        logger.error(f"ä¿å­˜åŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“è¿æ¥é—®é¢˜
                        if "connection" in str(e).lower() or "network" in str(e).lower():
                            logger.warning("æ£€æµ‹åˆ°æ•°æ®åº“è¿æ¥é—®é¢˜ï¼Œå°è¯•é‡æ–°è¿æ¥...")
                            try:
                                # é‡æ–°è¿æ¥æ•°æ®åº“
                                self.db_manager._reconnect_mongodb()
                                # é‡æ–°è·å–ç»“æœé›†åˆ
                                db = self.db_manager.get_collection(source_table).database
                                result_collection = db[result_collection_name]
                                # é‡è¯•ä¿å­˜
                                result_collection.insert_many(batch_results)
                                matched_count += len(batch_results)
                                logger.info("æ•°æ®åº“é‡è¿æˆåŠŸï¼ŒåŒ¹é…ç»“æœä¿å­˜å®Œæˆ")
                            except Exception as retry_error:
                                logger.error(f"æ•°æ®åº“é‡è¿åä»ç„¶ä¿å­˜å¤±è´¥: {str(retry_error)}")
                                # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡ï¼Œä¸ä¸­æ–­æ•´ä¸ªä»»åŠ¡
                
                processed_count += len(batch_records)
                
                # æ›´æ–°è¿›åº¦
                progress = (processed_count / total_records) * 100
                elapsed_time = time.time() - batch_start_time
                
                self._update_task_status(task_id, 'running', round(progress, 2), 
                                       f'å¤„ç†ä¸­ {processed_count}/{total_records}',
                                       processed_count, total_records, matched_count)
                
                logger.info(f"ä»»åŠ¡ {task_id} è¿›åº¦: {progress:.1f}% "
                           f"({processed_count}/{total_records}), åŒ¹é…: {matched_count}")
            
            # ä»»åŠ¡å®Œæˆ
            total_time = time.time() - batch_start_time
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self.performance_stats['total_processed'] += processed_count
            self.performance_stats['total_matched'] += matched_count
            if self.prefilter:
                self.performance_stats['prefilter_performance'] = self.prefilter.get_performance_stats()
            
            self._update_task_status(task_id, 'completed', 100.0, 
                                   f'åŒ¹é…å®Œæˆï¼Œå…±æ‰¾åˆ° {matched_count} ä¸ªåŒ¹é…ç»“æœ',
                                   processed_count, total_records, matched_count)
            
            logger.info(f"ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å®Œæˆ: {task_id}, "
                       f"å¤„ç†: {processed_count}, åŒ¹é…: {matched_count}, "
                       f"è€—æ—¶: {total_time:.2f}ç§’")
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œä¼˜åŒ–åŒ¹é…ä»»åŠ¡å¤±è´¥: {task_id} - {str(e)}")
            self._update_task_status(task_id, 'failed', 0, f'åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}',
                                   0, 0, 0)
        finally:
            # æ¸…ç†ä»»åŠ¡
            if task_id in self.running_tasks:
                del self.running_tasks[task_id]
    
    def _process_optimized_batch(self, batch_records: List[Dict], mappings: List[Dict], 
                               source_table: str, task_id: str) -> List[Dict]:
        """å¤„ç†ä¼˜åŒ–æ‰¹æ¬¡ï¼ˆä½¿ç”¨æ‰¹é‡é¢„è¿‡æ»¤ä¼˜åŒ–ï¼‰"""
        batch_results = []
        batch_start_time = time.time()
        batch_size = len(batch_records)
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†: {batch_size} æ¡è®°å½•ï¼ˆè¶…é«˜æ€§èƒ½ä¼˜åŒ–ï¼‰")
        
        try:
            # ã€å…³é”®ä¼˜åŒ–ã€‘æ‰¹é‡é¢„è¿‡æ»¤ - ä¸€æ¬¡æ€§è·å–æ‰€æœ‰å€™é€‰è®°å½•
            logger.info("ğŸ“Š æ‰§è¡Œæ‰¹é‡é¢„è¿‡æ»¤...")
            prefilter_start = time.time()
            
            # æ‰¹é‡è·å–æ‰€æœ‰å€™é€‰è®°å½•æ˜ å°„ {source_id: [candidates]}
            batch_candidates_map = self._batch_prefilter_candidates(batch_records, mappings, source_table)
            
            prefilter_time = time.time() - prefilter_start
            logger.info(f"âœ… æ‰¹é‡é¢„è¿‡æ»¤å®Œæˆ: {len(batch_candidates_map)} æ¡è®°å½•æœ‰å€™é€‰, è€—æ—¶: {prefilter_time:.2f}ç§’")
            
            # ã€é«˜æ€§èƒ½å¹¶è¡Œå¤„ç†ã€‘ä½¿ç”¨æ‰¹é‡å€™é€‰è¿›è¡ŒåŒ¹é…
            max_workers = min(32, batch_size)
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_record = {}
                
                for source_record in batch_records:
                    source_id = str(source_record.get('_id', ''))
                    candidates = batch_candidates_map.get(source_id, [])
                    
                    future = executor.submit(
                        self._process_single_record_with_candidates,
                        source_record, candidates, mappings, source_table, task_id
                    )
                    future_to_record[future] = source_record
                
                # æ”¶é›†ç»“æœ
                processed_count = 0
                for future in as_completed(future_to_record):
                    source_record = future_to_record[future]
                    try:
                        result = future.result(timeout=10)  # å‡å°‘è¶…æ—¶æ—¶é—´
                        if result:
                            batch_results.append(result)
                        processed_count += 1
                        
                        # æ¯å¤„ç†500æ¡è®°å½•æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
                        if processed_count % 500 == 0:
                            logger.info(f"ğŸ“Š æ‰¹æ¬¡è¿›åº¦: {processed_count}/{batch_size}")
                            
                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†è®°å½•å¤±è´¥: {source_record.get('_id', 'Unknown')}, é”™è¯¯: {e}")
                        processed_count += 1
        
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
            return []
        
        # è®¡ç®—æ‰¹æ¬¡æ€§èƒ½ç»Ÿè®¡
        batch_duration = time.time() - batch_start_time
        records_per_second = batch_size / batch_duration if batch_duration > 0 else 0
        
        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†å®Œæˆ: {batch_size} æ¡è®°å½•, "
                   f"è€—æ—¶: {batch_duration:.2f}ç§’, "
                   f"é€Ÿåº¦: {records_per_second:.1f} æ¡/ç§’, "
                   f"åŒ¹é…ç»“æœ: {len(batch_results)}")
        
        # ã€æ€§èƒ½ç›®æ ‡æ£€æŸ¥ã€‘ä¸åŸé¡¹ç›®å¯¹æ¯”
        target_speed = 1040  # åŸé¡¹ç›®ç›®æ ‡é€Ÿåº¦
        if records_per_second >= target_speed:
            logger.info(f"ğŸ¯ æ€§èƒ½ä¼˜ç§€: å½“å‰ {records_per_second:.1f} æ¡/ç§’ >= ç›®æ ‡ {target_speed} æ¡/ç§’")
        elif records_per_second >= target_speed * 0.5:
            logger.info(f"ğŸŸ¡ æ€§èƒ½è‰¯å¥½: å½“å‰ {records_per_second:.1f} æ¡/ç§’ï¼Œæ¥è¿‘ç›®æ ‡ {target_speed} æ¡/ç§’")
        else:
            logger.warning(f"ğŸ”´ æ€§èƒ½å¾…ä¼˜åŒ–: å½“å‰ {records_per_second:.1f} æ¡/ç§’ < ç›®æ ‡ {target_speed} æ¡/ç§’")
        
        return batch_results
    
    def _batch_prefilter_candidates(self, batch_records: List[Dict], mappings: List[Dict], 
                                   source_table: str) -> Dict[str, List[Dict]]:
        """æ‰¹é‡é¢„è¿‡æ»¤å€™é€‰è®°å½•ï¼ˆé€šç”¨æŸ¥è¯¢å¼•æ“ä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            logger.info(f"ğŸš€ ä½¿ç”¨é€šç”¨æŸ¥è¯¢å¼•æ“è¿›è¡Œæ‰¹é‡é¢„è¿‡æ»¤: {len(batch_records)} æ¡è®°å½•")
            
            # ä½¿ç”¨é€šç”¨æŸ¥è¯¢å¼•æ“æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢
            query_results = self.universal_query_engine.query_batch_records(batch_records, mappings)
            
            # è½¬æ¢æŸ¥è¯¢ç»“æœæ ¼å¼
            batch_candidates_map = {}
            
            for record_id, query_result in query_results.items():
                if query_result.candidates:
                    batch_candidates_map[record_id] = query_result.candidates
                    logger.debug(f"è®°å½• {record_id} è·å¾— {len(query_result.candidates)} ä¸ªå€™é€‰")
            
            logger.info(f"âœ… é€šç”¨æŸ¥è¯¢å¼•æ“æ‰¹é‡é¢„è¿‡æ»¤å®Œæˆ: {len(batch_candidates_map)} æ¡è®°å½•æœ‰å€™é€‰")
            
            return batch_candidates_map
            
        except Exception as e:
            logger.error(f"é€šç”¨æŸ¥è¯¢å¼•æ“æ‰¹é‡é¢„è¿‡æ»¤å¤±è´¥: {str(e)}")
            # é™çº§åˆ°åŸæœ‰æ–¹æ³•ï¼ˆä¿æŒç³»ç»Ÿç¨³å®šæ€§ï¼‰
            logger.warning("é™çº§åˆ°åŸæœ‰é¢„è¿‡æ»¤æ–¹æ³•")
            return self._batch_prefilter_candidates_fallback(batch_records, mappings, source_table)
    
    def _batch_prefilter_candidates_fallback(self, batch_records: List[Dict], mappings: List[Dict], 
                                           source_table: str) -> Dict[str, List[Dict]]:
        """æ‰¹é‡é¢„è¿‡æ»¤å€™é€‰è®°å½•ï¼ˆé™çº§æ–¹æ³•ï¼‰"""
        batch_candidates_map = {}
        
        try:
            logger.info("ä½¿ç”¨é™çº§é¢„è¿‡æ»¤æ–¹æ³•")
            
            # ç®€åŒ–çš„é¢„è¿‡æ»¤é€»è¾‘
            for source_record in batch_records:
                source_id = str(source_record.get('_id', ''))
                candidates = []
                
                for mapping in mappings:
                    source_field = mapping.get('source_field')
                    target_field = mapping.get('target_field')
                    target_table = mapping.get('target_table')
                    
                    if not all([source_field, target_field, target_table]):
                        continue
                    
                    source_value = source_record.get(source_field)
                    if not source_value:
                        continue
                    
                    try:
                        # ç®€å•çš„ç²¾ç¡®åŒ¹é…æŸ¥è¯¢
                        collection = self.db_manager.get_db()[target_table]
                        query_candidates = list(collection.find(
                            {target_field: str(source_value)}, 
                            limit=20
                        ))
                        candidates.extend(query_candidates)
                        
                    except Exception as e:
                        logger.warning(f"é™çº§æŸ¥è¯¢å¤±è´¥: {source_id}.{source_field} - {str(e)}")
                        continue
                
                if candidates:
                    # å»é‡
                    seen_ids = set()
                    unique_candidates = []
                    for candidate in candidates:
                        candidate_id = str(candidate.get('_id', ''))
                        if candidate_id not in seen_ids:
                            seen_ids.add(candidate_id)
                            unique_candidates.append(candidate)
                    
                    batch_candidates_map[source_id] = unique_candidates[:30]
            
            logger.info(f"é™çº§é¢„è¿‡æ»¤å®Œæˆ: {len(batch_candidates_map)} æ¡è®°å½•æœ‰å€™é€‰")
            return batch_candidates_map
            
        except Exception as e:
            logger.error(f"é™çº§é¢„è¿‡æ»¤å¤±è´¥: {str(e)}")
            return {}
    
    def _is_candidate_match(self, source_record: Dict, target_record: Dict, field_info: Dict) -> bool:
        """æ£€æŸ¥å€™é€‰è®°å½•æ˜¯å¦åŒ¹é…ï¼ˆæ”¯æŒåœ°å€è¯­ä¹‰åŒ¹é…ï¼‰"""
        source_field = field_info['source_field']
        target_field = field_info['target_field']
        
        source_value = str(source_record.get(source_field, '')).strip()
        target_value = str(target_record.get(target_field, '')).strip()
        
        if not source_value or not target_value:
            return False
        
        # å¿«é€Ÿç²¾ç¡®åŒ¹é…æ£€æŸ¥
        if source_value.lower() == target_value.lower():
            return True
        
        if source_value.lower() in target_value.lower() or target_value.lower() in source_value.lower():
            return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåœ°å€å­—æ®µï¼Œä½¿ç”¨åœ°å€è¯­ä¹‰åŒ¹é…
        if self._is_address_field_simple(source_field, target_field):
            try:
                # ä½¿ç”¨åœ°å€è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
                similarity = self.similarity_calculator.calculate_address_similarity(source_value, target_value)
                logger.debug(f"åœ°å€è¯­ä¹‰ç›¸ä¼¼åº¦: {source_value} <-> {target_value} = {similarity:.3f}")
                return similarity >= 0.3  # åœ°å€åŒ¹é…ä½¿ç”¨æ›´ä½é˜ˆå€¼
            except Exception as e:
                logger.debug(f"åœ°å€è¯­ä¹‰åŒ¹é…å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æ–¹æ³•: {e}")
        
        # ç®€å•çš„å­—ç¬¦ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
        if len(source_value) >= 3 and len(target_value) >= 3:
            common_chars = set(source_value.lower()) & set(target_value.lower())
            similarity = len(common_chars) / max(len(set(source_value.lower())), len(set(target_value.lower())))
            return similarity >= 0.3
        
        return False
    
    def _is_address_field_simple(self, source_field: str, target_field: str) -> bool:
        """ç®€å•æ£€æŸ¥æ˜¯å¦ä¸ºåœ°å€å­—æ®µï¼ˆç”¨äºé¢„è¿‡æ»¤é˜¶æ®µï¼‰"""
        address_keywords = ['åœ°å€', 'åœ°ç‚¹', 'address', 'addr', 'dz', 'zcdz', 'èµ·ç«åœ°ç‚¹', 'æ³¨å†Œåœ°å€']
        
        source_field_lower = source_field.lower()
        target_field_lower = target_field.lower()
        
        # æ£€æŸ¥å­—æ®µåæ˜¯å¦åŒ…å«åœ°å€å…³é”®è¯
        for keyword in address_keywords:
            if keyword in source_field_lower or keyword in target_field_lower:
                return True
        
        return False
    
    def _extract_address_keywords_simple(self, address: str) -> List[str]:
        """
        æå–åœ°å€å…³é”®è¯ï¼ˆç²¾ç®€é«˜æ•ˆç‰ˆï¼‰
        åªæå–æœ€é‡è¦çš„åœ°å€ç»„ä»¶ç”¨äºé¢„è¿‡æ»¤æŸ¥è¯¢
        """
        if not address:
            return []
        
        keywords = []
        
        # 1. æå–é—¨ç‰Œå·ï¼ˆæœ€é‡è¦ï¼‰
        number_pattern = r'(\d+å·|\d+æ ‹|\d+å¹¢|\d+åº§|\d+æ¥¼)'
        numbers = re.findall(number_pattern, address)
        keywords.extend(numbers)
        
        # 2. æå–è¡—é“åï¼ˆé‡è¦ï¼‰
        street_pattern = r'([^çœå¸‚åŒºå¿]{2,8}(?:è·¯|è¡—|é“|å··|å¼„|é‡Œ|å¤§è¡—|å¤§é“))'
        streets = re.findall(street_pattern, address)
        keywords.extend(streets)
        
        # 3. æå–åŒºå¿åï¼ˆé‡è¦ï¼‰
        district_pattern = r'([^çœå¸‚åŒºå¿]{2,6}(?:åŒº|å¿))'
        districts = re.findall(district_pattern, address)
        keywords.extend(districts)
        
        # 4. æå–å»ºç­‘ç‰©åï¼ˆè¾…åŠ©ï¼‰
        building_pattern = r'([^è·¯è¡—é“å··å¼„é‡Œå·æ ‹å¹¢åº§æ¥¼]{3,15}(?:å¤§å¦|å¤§æ¥¼|å¹¿åœº|ä¸­å¿ƒ|é™¢|å›­|æ‘|å°åŒº|å…¬å¸|å‚|åº—|é¦†|æ‰€|ç«™|åœº|å…»è€é™¢))'
        buildings = re.findall(building_pattern, address)
        keywords.extend(buildings)
        
        # å»é‡å¹¶è¿‡æ»¤çŸ­è¯
        unique_keywords = []
        seen = set()
        for keyword in keywords:
            if keyword and len(keyword) >= 3 and keyword not in seen:
                unique_keywords.append(keyword)
                seen.add(keyword)
        
        # é™åˆ¶å…³é”®è¯æ•°é‡ï¼ˆé¿å…æŸ¥è¯¢è¿‡äºå¤æ‚ï¼‰
        return unique_keywords[:5]
    
    def _get_address_candidates_from_index(self, source_address: str, target_table: str, target_field: str) -> List[Dict]:
        """
        ä»åœ°å€å…³é”®è¯ç´¢å¼•ä¸­è·å–å€™é€‰è®°å½•
        è¿™æ˜¯é«˜æ•ˆåœ°å€åŒ¹é…çš„æ ¸å¿ƒæ–¹æ³•ï¼Œç±»ä¼¼äºå•ä½åç§°çš„åˆ‡ç‰‡ç´¢å¼•æŸ¥è¯¢
        """
        try:
            # æå–æºåœ°å€çš„å…³é”®è¯
            source_keywords = self._extract_address_keywords_simple(source_address)
            if not source_keywords:
                return []
            
            # æ„å»ºç´¢å¼•è¡¨åï¼ˆåŸºäºç›®æ ‡è¡¨åï¼‰
            keyword_collection_name = f"{target_table}_address_keywords"
            
            # æ£€æŸ¥ç´¢å¼•è¡¨æ˜¯å¦å­˜åœ¨
            db = self.db_manager.get_db()
            if keyword_collection_name not in db.list_collection_names():
                return []  # ç´¢å¼•è¡¨ä¸å­˜åœ¨ï¼Œå›é€€åˆ°ä¼ ç»ŸæŸ¥è¯¢
            
            keyword_collection = db[keyword_collection_name]
            
            # ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–æ€§èƒ½ï¼Œé¿å…å¤šæ¬¡å•ç‹¬æŸ¥è¯¢
            candidate_docs = set()
            keyword_scores = {}
            
            # æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰å…³é”®è¯ï¼Œå‡å°‘æ•°æ®åº“äº¤äº’æ¬¡æ•°
            if source_keywords:
                matches = list(keyword_collection.find({
                    "keyword": {"$in": source_keywords},
                    "field_name": target_field
                }).limit(200))  # æ€»ä½“é™åˆ¶å€™é€‰æ•°é‡
                
                for match in matches:
                    doc_id = match['doc_id']
                    keyword = match['keyword']
                    candidate_docs.add(doc_id)
                    
                    # è®¡ç®—å…³é”®è¯åŒ¹é…å¾—åˆ†
                    if doc_id not in keyword_scores:
                        keyword_scores[doc_id] = 0
                    keyword_scores[doc_id] += 1
            
            # æŒ‰åŒ¹é…å¾—åˆ†æ’åºï¼Œä¼˜å…ˆè¿”å›åŒ¹é…æ›´å¤šå…³é”®è¯çš„è®°å½•
            sorted_candidates = sorted(candidate_docs, key=lambda x: keyword_scores.get(x, 0), reverse=True)
            
            # é™åˆ¶å€™é€‰æ•°é‡ï¼Œé¿å…æ€§èƒ½é—®é¢˜
            top_candidates = sorted_candidates[:50]
            
            # è¿”å›å€™é€‰è®°å½•ä¿¡æ¯
            candidates = []
            for doc_id in top_candidates:
                candidates.append({
                    'doc_id': doc_id,
                    'score': keyword_scores.get(doc_id, 0)
                })
            
            return candidates
            
        except Exception as e:
            # å¦‚æœç´¢å¼•æŸ¥è¯¢å¤±è´¥ï¼Œé™é»˜å›é€€åˆ°ä¼ ç»ŸæŸ¥è¯¢
            return []
    
    def _extract_address_keywords(self, address: str) -> List[str]:
        """æå–åœ°å€å…³é”®è¯ç”¨äºå®½æ¾åŒ¹é…"""
        import re
        
        keywords = []
        
        # æå–çœå¸‚åŒº
        province_match = re.search(r'([^çœå¸‚åŒºå¿]{2,8}(?:çœ|å¸‚|è‡ªæ²»åŒº))', address)
        if province_match:
            keywords.append(province_match.group(1))
        
        city_match = re.search(r'([^çœå¸‚åŒºå¿]{2,8}(?:å¸‚|å·|å¿))', address)
        if city_match:
            keywords.append(city_match.group(1))
        
        district_match = re.search(r'([^çœå¸‚åŒºå¿]{2,8}(?:åŒº|å¿|é•‡|å¼€å‘åŒº|é«˜æ–°åŒº|ç»æµåŒº))', address)
        if district_match:
            keywords.append(district_match.group(1))
        
        # æå–è¡—é“è·¯å
        street_matches = re.findall(r'([^è·¯è¡—é“å··å¼„é‡Œ]{1,20}(?:è·¯|è¡—|é“|å··|å¼„|é‡Œ|å¤§è¡—|å¤§é“|è¡—é“))', address)
        keywords.extend(street_matches)
        
        # æå–é—¨ç‰Œå·
        number_matches = re.findall(r'(\d+(?:å·|æ ‹|å¹¢|åº§|æ¥¼|å®¤|å±‚))', address)
        keywords.extend(number_matches)
        
        # æå–å»ºç­‘ç‰©åç§°
        building_matches = re.findall(r'([^è·¯è¡—é“å··å¼„é‡Œå·æ ‹å¹¢åº§æ¥¼å®¤å±‚]{2,20}(?:å¤§å¦|å¤§æ¥¼|å¹¿åœº|ä¸­å¿ƒ|é™¢|å›­|æ‘|å°åŒº|å…¬å¸|å‚|åº—|é¦†|æ‰€|ç«™|åœº|ç”Ÿæ€å›­|ç§‘æŠ€å›­|å·¥ä¸šå›­|äº§ä¸šå›­|å…»è€é™¢|æ•¬è€é™¢))', address)
        keywords.extend(building_matches)
        
        # å»é‡å¹¶è¿‡æ»¤çŸ­è¯
        unique_keywords = list(set([kw for kw in keywords if len(kw) >= 2]))
        
        return unique_keywords
    
    def _process_single_record_with_candidates(self, source_record: Dict, candidates: List[Dict],
                                             mappings: List[Dict], source_table: str, task_id: str) -> Optional[Dict]:
        """ä½¿ç”¨é¢„è·å–çš„å€™é€‰è®°å½•è¿›è¡Œå•è®°å½•å¤„ç†ï¼ˆè¶…é«˜æ€§èƒ½ï¼‰"""
        try:
            if not candidates:
                return None
            
            # ä½¿ç”¨å€™é€‰æ’åºå™¨ä¼˜åŒ–é¡ºåºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.candidate_ranker:
                candidates = self.candidate_ranker.rank_candidates(candidates, source_record)
            
            # ä¼˜å…ˆä½¿ç”¨åˆ†å±‚åŒ¹é…ç®—æ³•
            if self.hierarchical_matcher and self.use_hierarchical_matching:
                hierarchical_matches = self.hierarchical_matcher.match_record(source_record, candidates)
                
                if hierarchical_matches:
                    best_match = hierarchical_matches[0]
                    hierarchical_result = {
                        'target_record': best_match.candidate,
                        'similarity': best_match.final_score,
                        'matched_fields': list(best_match.field_scores.keys()),
                        'details': {
                            'field_scores': best_match.field_scores,
                            'match_type': best_match.match_type,
                            'confidence_level': best_match.confidence_level,
                            'primary_score': best_match.primary_score,
                            'secondary_score': best_match.secondary_score
                        }
                    }
                    
                    result = self._format_optimized_match_result(
                        source_record, hierarchical_result, mappings, task_id
                    )
                    
                    # æ·»åŠ åˆ†å±‚åŒ¹é…ç‰¹æœ‰ä¿¡æ¯
                    result['match_details']['hierarchical_matching'] = True
                    result['match_details']['match_strategy'] = best_match.match_type
                    result['match_details']['primary_score'] = best_match.primary_score
                    result['match_details']['secondary_score'] = best_match.secondary_score
                    
                    return result
            
            # é™çº§åˆ°ä¼ ç»ŸåŒ¹é…ç®—æ³•
            return self._process_candidates_with_traditional_matching(
                source_record, candidates, mappings, source_table, task_id
            )
            
        except Exception as e:
            logger.error(f"å¤„ç†å¸¦å€™é€‰è®°å½•çš„å•è®°å½•å¤±è´¥: {str(e)}")
            return None
    
    def _process_candidates_with_traditional_matching(self, source_record: Dict, candidates: List[Dict],
                                                    mappings: List[Dict], source_table: str, task_id: str) -> Optional[Dict]:
        """ä½¿ç”¨ä¼ ç»ŸåŒ¹é…ç®—æ³•å¤„ç†å€™é€‰è®°å½•"""
        best_match = None
        best_similarity = 0.0
        
        # ä½¿ç”¨å¢å¼ºæ¨¡ç³ŠåŒ¹é…å™¨è¿›è¡Œå¿«é€ŸåŒ¹é…
        for candidate in candidates[:30]:  # åªå¤„ç†å‰30ä¸ªå€™é€‰ä»¥æé«˜é€Ÿåº¦
            try:
                if self.enhanced_fuzzy_matcher:
                    similarity = self.enhanced_fuzzy_matcher.calculate_similarity(source_record, candidate)
                else:
                    # ç®€å•ç›¸ä¼¼åº¦è®¡ç®—
                    similarity = self._calculate_simple_similarity(source_record, candidate, mappings)
                
                if similarity > best_similarity and similarity >= 0.6:  # æé«˜é˜ˆå€¼
                    best_similarity = similarity
                    best_match = {
                        'target_record': candidate,
                        'similarity': similarity,
                        'matched_fields': [m['source_field'] for m in mappings],
                        'details': {'match_type': 'traditional', 'algorithm': 'enhanced_fuzzy'}
                    }
            
            except Exception as e:
                logger.debug(f"å€™é€‰åŒ¹é…å¤±è´¥: {str(e)}")
                continue
        
        if best_match:
            return self._format_optimized_match_result(source_record, best_match, mappings, task_id)
        
        return None
    
    def _calculate_simple_similarity(self, source_record: Dict, target_record: Dict, mappings: List[Dict]) -> float:
        """è®¡ç®—ç®€å•ç›¸ä¼¼åº¦"""
        total_similarity = 0.0
        valid_fields = 0
        
        for mapping in mappings:
            source_field = mapping.get('source_field')
            target_field = mapping.get('target_field')
            
            if source_field in source_record and target_field in target_record:
                source_value = str(source_record[source_field]).strip().lower()
                target_value = str(target_record[target_field]).strip().lower()
                
                if source_value and target_value:
                    if source_value == target_value:
                        similarity = 1.0
                    elif source_value in target_value or target_value in source_value:
                        similarity = 0.8
                    else:
                        # ç®€å•çš„å­—ç¬¦é›†ç›¸ä¼¼åº¦
                        common = len(set(source_value) & set(target_value))
                        total_chars = len(set(source_value) | set(target_value))
                        similarity = common / total_chars if total_chars > 0 else 0.0
                    
                    total_similarity += similarity
                    valid_fields += 1
        
        return total_similarity / valid_fields if valid_fields > 0 else 0.0
    
    def _process_single_record_optimized(self, source_record: Dict, mappings: List[Dict], 
                                       source_table: str, task_id: str) -> Optional[Dict]:
        """ä¼˜åŒ–çš„å•è®°å½•å¤„ç†ï¼ˆåŸé¡¹ç›®çº§åˆ«ç®—æ³•ï¼‰"""
        try:
            # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šä½¿ç”¨åˆ‡ç‰‡å¢å¼ºåŒ¹é…å™¨ï¼ˆæœ€å¿«ï¼‰
            candidates = []
            if self.slice_matcher and self.use_high_performance:
                candidates = self._get_slice_enhanced_candidates(source_record, mappings)
                if candidates:
                    logger.debug(f"âœ… åˆ‡ç‰‡åŒ¹é…è·å¾— {len(candidates)} ä¸ªå€™é€‰")
            
            # ç¬¬äºŒä¼˜å…ˆçº§ï¼šä½¿ç”¨å›¾åŒ¹é…å™¨ï¼ˆé«˜è´¨é‡ï¼‰
            if not candidates and self.graph_matcher:
                candidates = self._get_graph_matching_candidates(source_record, mappings)
                if candidates:
                    logger.debug(f"âœ… å›¾åŒ¹é…è·å¾— {len(candidates)} ä¸ªå€™é€‰")
            
            # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šä½¿ç”¨é¢„è¿‡æ»¤ç³»ç»Ÿ
            if not candidates and self.prefilter:
                candidates = self.prefilter.get_optimized_candidates(source_record, source_table)
                if candidates:
                    logger.debug(f"âœ… é¢„è¿‡æ»¤è·å¾— {len(candidates)} ä¸ªå€™é€‰")
            
            # æœ€åé™çº§ï¼šç®€å•æŸ¥è¯¢
            if not candidates:
                candidates = self._get_simple_candidates(source_record, mappings)
                if candidates:
                    logger.debug(f"âœ… ç®€å•æŸ¥è¯¢è·å¾— {len(candidates)} ä¸ªå€™é€‰")
            
            if not candidates:
                return None
            
            # ä½¿ç”¨å€™é€‰æ’åºå™¨ä¼˜åŒ–é¡ºåº
            if self.candidate_ranker and candidates:
                candidates = self.candidate_ranker.rank_candidates(candidates, source_record)
            
            # ä¼˜å…ˆä½¿ç”¨åˆ†å±‚åŒ¹é…ç®—æ³•
            if self.hierarchical_matcher and self.use_hierarchical_matching:
                hierarchical_matches = self.hierarchical_matcher.match_record(source_record, candidates)
                
                if hierarchical_matches:
                    # è½¬æ¢åˆ†å±‚åŒ¹é…ç»“æœä¸ºç»Ÿä¸€æ ¼å¼
                    best_hierarchical_match = hierarchical_matches[0]
                    hierarchical_result = {
                        'target_record': best_hierarchical_match.candidate,
                        'similarity': best_hierarchical_match.final_score,
                        'matched_fields': list(best_hierarchical_match.field_scores.keys()),
                        'details': {
                            'field_scores': best_hierarchical_match.field_scores,
                            'match_type': best_hierarchical_match.match_type,
                            'confidence_level': best_hierarchical_match.confidence_level,
                            'primary_score': best_hierarchical_match.primary_score,
                            'secondary_score': best_hierarchical_match.secondary_score
                        }
                    }
                    
                    result = self._format_optimized_match_result(
                        source_record, hierarchical_result, mappings, task_id
                    )
                    
                    # æ·»åŠ åˆ†å±‚åŒ¹é…ç‰¹æœ‰ä¿¡æ¯
                    result['match_details']['hierarchical_matching'] = True
                    result['match_details']['match_strategy'] = best_hierarchical_match.match_type
                    result['match_details']['primary_score'] = best_hierarchical_match.primary_score
                    result['match_details']['secondary_score'] = best_hierarchical_match.secondary_score
                    
                    logger.debug(f"åˆ†å±‚åŒ¹é…æˆåŠŸ: {best_hierarchical_match.match_type}, "
                               f"æœ€ç»ˆå¾—åˆ†: {best_hierarchical_match.final_score:.3f}")
                    
                    return result
            
            # é™çº§åˆ°ä¼ ç»Ÿé«˜æ€§èƒ½åŒ¹é…ç®—æ³•
            matches = self._execute_high_performance_matching(
                source_record, candidates, mappings
            )
            
            # è¿”å›æœ€ä½³åŒ¹é…ç»“æœ
            if matches:
                best_match = matches[0]  # å–ç¬¬ä¸€ä¸ªæœ€ä½³åŒ¹é…
                result = self._format_optimized_match_result(
                    source_record, best_match, mappings, task_id
                )
                result['match_details']['hierarchical_matching'] = False  # æ ‡è®°ä¸ºéåˆ†å±‚åŒ¹é…
                return result
            
            return None
            
        except Exception as e:
            logger.error(f"å•è®°å½•å¤„ç†å¤±è´¥: {source_record.get('_id', 'Unknown')}, é”™è¯¯: {e}")
            return None
        
        return batch_results
    
    def _format_optimized_match_result(self, source_record: Dict, match: Dict, 
                                     mappings: List[Dict], task_id: str) -> Dict:
        """æ ¼å¼åŒ–ä¼˜åŒ–åŒ¹é…ç»“æœ"""
        return {
            # åŸºç¡€æ ‡è¯†ä¿¡æ¯
            'source_id': str(source_record.get('_id', '')),
            'target_id': str(match['target_record'].get('_id', '')),
            'source_table': match.get('source_table', ''),
            'target_table': match.get('target_table', ''),
            
            # åŒ¹é…æ ¸å¿ƒä¿¡æ¯
            'similarity_score': match['similarity'],
            'matched_fields': match['matched_fields'],
            'match_algorithm': 'enhanced_optimized',
            
            # æºè®°å½•å…³é”®å­—æ®µ
            'source_key_fields': self._extract_key_fields(source_record, mappings, 'source'),
            'target_key_fields': self._extract_key_fields(match['target_record'], mappings, 'target'),
            
            # åŒ¹é…è¯¦ç»†ä¿¡æ¯
            'match_details': {
                'field_scores': match.get('details', {}).get('field_scores', {}),
                'total_fields': len(mappings),
                'matched_field_count': len(match['matched_fields']),
                'confidence_level': self._calculate_confidence_level(match['similarity']),
                'match_type': self._determine_match_type(match['similarity'], match['matched_fields']),
                'prefilter_candidates': match.get('prefilter_candidates', 0)
            },
            
            # å…ƒæ•°æ®ä¿¡æ¯
            'created_at': datetime.now().isoformat(),
            'task_id': task_id,
            'match_version': '2.1',  # æ ‡è¯†ä¼˜åŒ–ç‰ˆæœ¬
            'optimization_enabled': True
        }
    
    def _get_simple_candidates(self, source_record: Dict, mappings: List[Dict]) -> List[Dict]:
        """è·å–ç®€å•å€™é€‰è®°å½•ï¼ˆé™çº§æ–¹æ³•ï¼‰"""
        if not self.db_manager or not self.db_manager.mongo_client:
            return []
        
        db = self.db_manager.mongo_client.get_database()
        all_candidates = []
        
        # ä¸ºæ¯ä¸ªæ˜ å°„å­—æ®µæŸ¥è¯¢å€™é€‰
        for mapping in mappings[:2]:  # é™åˆ¶å­—æ®µæ•°é‡
            source_field = mapping['source_field']
            target_field = mapping['target_field']
            target_table = mapping['target_table']
            
            source_value = source_record.get(source_field)
            if not source_value:
                continue
            
            try:
                collection = db[target_table]
                
                # ç²¾ç¡®åŒ¹é…
                exact_candidates = list(collection.find({target_field: source_value}).limit(20))
                all_candidates.extend(exact_candidates)
                
                # å¦‚æœå€™é€‰ä¸è¶³ï¼Œè¿›è¡Œæ–‡æœ¬æœç´¢
                if len(all_candidates) < 50:
                    try:
                        text_candidates = list(collection.find(
                            {'$text': {'$search': str(source_value)}}
                        ).limit(30))
                        all_candidates.extend(text_candidates)
                    except:
                        pass
                        
            except Exception as e:
                logger.debug(f"ç®€å•å€™é€‰æŸ¥è¯¢å¤±è´¥: {str(e)}")
        
        # å»é‡
        seen_ids = set()
        unique_candidates = []
        for candidate in all_candidates:
            candidate_id = candidate.get('_id')
            if candidate_id and candidate_id not in seen_ids:
                seen_ids.add(candidate_id)
                unique_candidates.append(candidate)
        
        return unique_candidates[:100]  # é™åˆ¶å€™é€‰æ•°é‡
    
    def _get_slice_enhanced_candidates(self, source_record: Dict, mappings: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨åˆ‡ç‰‡å¢å¼ºåŒ¹é…å™¨è·å–å€™é€‰è®°å½•"""
        if not self.slice_matcher:
            return []
        
        try:
            # æå–ä¸»è¦åŒ¹é…å­—æ®µ
            primary_field = None
            for mapping in mappings:
                if 'name' in mapping['source_field'].lower() or 'company' in mapping['source_field'].lower():
                    primary_field = mapping['source_field']
                    break
            
            if not primary_field or not source_record.get(primary_field):
                return []
            
            source_name = str(source_record[primary_field]).strip()
            if len(source_name) < 3:
                return []
            
            # ä½¿ç”¨åˆ‡ç‰‡åŒ¹é…å™¨è¿›è¡Œå¿«é€ŸåŒ¹é…
            matches = self.slice_matcher.fast_fuzzy_match(
                source_name, 
                threshold=0.7,
                max_results=50
            )
            
            # è½¬æ¢ä¸ºå€™é€‰è®°å½•æ ¼å¼
            candidates = []
            for match in matches:
                if match.get('target_record'):
                    candidates.append(match['target_record'])
            
            return candidates[:50]  # é™åˆ¶æ•°é‡
            
        except Exception as e:
            logger.debug(f"åˆ‡ç‰‡åŒ¹é…å¤±è´¥: {str(e)}")
            return []
    
    def _get_graph_matching_candidates(self, source_record: Dict, mappings: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨å›¾åŒ¹é…å™¨è·å–å€™é€‰è®°å½•"""
        if not self.graph_matcher or not self.graph_matcher._is_built:
            return []
        
        try:
            source_id = str(source_record.get('_id', ''))
            if not source_id:
                return []
            
            # ä½¿ç”¨å›¾åŒ¹é…æŸ¥æ‰¾ç›¸å…³å€™é€‰
            similar_entities = self.graph_matcher.find_similar_entities(
                source_id, 
                similarity_threshold=0.6,
                max_results=30
            )
            
            if not similar_entities:
                return []
            
            # è·å–å®Œæ•´çš„å€™é€‰è®°å½•
            db = self.db_manager.mongo_client.get_database()
            candidates = []
            
            for entity_info in similar_entities:
                entity_id = entity_info.get('entity_id')
                if entity_id:
                    # ä»ç›®æ ‡è¡¨æŸ¥è¯¢å®Œæ•´è®°å½•
                    for mapping in mappings:
                        target_table = mapping['target_table']
                        collection = db[target_table]
                        
                        record = collection.find_one({'_id': entity_id})
                        if record:
                            candidates.append(record)
                            break
            
            return candidates[:30]
            
        except Exception as e:
            logger.debug(f"å›¾åŒ¹é…å¤±è´¥: {str(e)}")
            return []
    
    def _execute_high_performance_matching(self, source_record: Dict, candidates: List[Dict], 
                                         mappings: List[Dict]) -> List[Dict]:
        """æ‰§è¡Œé«˜æ€§èƒ½åŒ¹é…ç®—æ³•"""
        matches = []
        
        try:
            # ä¼˜å…ˆä½¿ç”¨åˆ‡ç‰‡å¢å¼ºåŒ¹é…å™¨è¿›è¡Œç²¾ç¡®è¯„åˆ†
            if self.slice_matcher:
                for candidate in candidates[:20]:  # é™åˆ¶å€™é€‰æ•°é‡ä»¥æå‡é€Ÿåº¦
                    similarity_score = self._calculate_slice_similarity(
                        source_record, candidate, mappings
                    )
                    
                    if similarity_score >= 0.6:  # é˜ˆå€¼
                        matches.append({
                            'target_record': candidate,
                            'similarity': similarity_score,
                            'matched_fields': self._get_matched_fields(source_record, candidate, mappings),
                            'details': {
                                'algorithm': 'slice_enhanced',
                                'field_scores': self._calculate_field_scores(source_record, candidate, mappings)
                            }
                        })
            
            # å¦‚æœåˆ‡ç‰‡åŒ¹é…ç»“æœä¸è¶³ï¼Œä½¿ç”¨å¢å¼ºæ¨¡ç³ŠåŒ¹é…å™¨
            if len(matches) < 3 and self.enhanced_fuzzy_matcher:
                fuzzy_matches = self._match_record_to_targets(
                    source_record, candidates, mappings,
                    self.enhanced_fuzzy_matcher,
                    0.6, 5
                )
                
                # åˆå¹¶ç»“æœï¼Œå»é‡
                existing_ids = {match['target_record'].get('_id') for match in matches}
                for fuzzy_match in fuzzy_matches:
                    target_id = fuzzy_match['target_record'].get('_id')
                    if target_id not in existing_ids:
                        matches.append(fuzzy_match)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            matches.sort(key=lambda x: x['similarity'], reverse=True)
            
            return matches[:5]  # è¿”å›å‰5ä¸ªæœ€ä½³åŒ¹é…
            
        except Exception as e:
            logger.warning(f"é«˜æ€§èƒ½åŒ¹é…æ‰§è¡Œå¤±è´¥: {str(e)}")
            return []
    
    def _calculate_slice_similarity(self, source_record: Dict, target_record: Dict, 
                                  mappings: List[Dict]) -> float:
        """è®¡ç®—åˆ‡ç‰‡ç›¸ä¼¼åº¦"""
        if not self.slice_matcher:
            return 0.0
        
        total_score = 0.0
        total_weight = 0.0
        
        for mapping in mappings:
            source_field = mapping['source_field']
            target_field = mapping['target_field']
            weight = mapping.get('similarity_score', 1.0)
            
            source_value = source_record.get(source_field, '')
            target_value = target_record.get(target_field, '')
            
            if source_value and target_value:
                # ä½¿ç”¨åˆ‡ç‰‡åŒ¹é…å™¨è®¡ç®—ç›¸ä¼¼åº¦
                field_similarity = self.slice_matcher.calculate_similarity(
                    str(source_value), str(target_value)
                )
                
                total_score += field_similarity * weight
                total_weight += weight
        
        return total_score / max(total_weight, 0.1)
    
    def _calculate_field_scores(self, source_record: Dict, target_record: Dict, 
                              mappings: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—å„å­—æ®µå¾—åˆ†"""
        field_scores = {}
        
        for mapping in mappings:
            source_field = mapping['source_field']
            target_field = mapping['target_field']
            
            source_value = source_record.get(source_field, '')
            target_value = target_record.get(target_field, '')
            
            if source_value and target_value:
                if self.slice_matcher:
                    score = self.slice_matcher.calculate_similarity(
                        str(source_value), str(target_value)
                    )
                else:
                    # ç®€å•å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
                    score = self._simple_string_similarity(str(source_value), str(target_value))
                
                field_scores[f"{source_field}->{target_field}"] = score
        
        return field_scores
    
    def _simple_string_similarity(self, str1: str, str2: str) -> float:
        """ç®€å•å­—ç¬¦ä¸²ç›¸ä¼¼åº¦è®¡ç®—"""
        if str1 == str2:
            return 1.0
        
        if not str1 or not str2:
            return 0.0
        
        # ç®€å•çš„å­—ç¬¦é‡å ç‡è®¡ç®—
        set1 = set(str1)
        set2 = set(str2)
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / max(union, 1)
    
    def _get_matched_fields(self, source_record: Dict, target_record: Dict, 
                          mappings: List[Dict]) -> List[str]:
        """è·å–åŒ¹é…çš„å­—æ®µåˆ—è¡¨"""
        matched_fields = []
        
        for mapping in mappings:
            source_field = mapping['source_field']
            target_field = mapping['target_field']
            
            source_value = source_record.get(source_field, '')
            target_value = target_record.get(target_field, '')
            
            if source_value and target_value:
                similarity = self._simple_string_similarity(str(source_value), str(target_value))
                if similarity >= 0.5:  # å­—æ®µåŒ¹é…é˜ˆå€¼
                    matched_fields.append(f"{source_field}->{target_field}")
        
        return matched_fields
    
    def _get_source_records_batch(self, collection, batch_size: int = 5000):
        """
        æ‰¹é‡è·å–æºè®°å½•
        
        Args:
            collection: MongoDBé›†åˆ
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Yields:
            List[Dict]: æ‰¹æ¬¡è®°å½•
        """
        skip = 0
        while True:
            batch = list(collection.find({}).skip(skip).limit(batch_size))
            if not batch:
                break
            yield batch
            skip += batch_size

    def _update_task_status(self, task_id: str, status: str, progress: float = 0, 
                           message: str = "", processed: int = 0, total: int = 0, matches: int = 0):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        # æ›´æ–°å†…å­˜ä¸­çš„ä»»åŠ¡çŠ¶æ€
        if task_id in self.running_tasks:
            task = self.running_tasks[task_id]
            task.update({
                'status': status,
                'progress': progress,
                'message': message,
                'processed': processed,
                'total': total,
                'matches': matches,
                'updated_time': datetime.now()
            })
            
            # åŒæ—¶æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€ - æ·»åŠ æ•°æ®åº“é‡è¿æœºåˆ¶
            try:
                if self.db_manager and hasattr(self.db_manager, 'get_db'):
                    # ä½¿ç”¨get_collectionæ–¹æ³•ï¼Œå®ƒåŒ…å«äº†è¿æ¥æ£€æŸ¥é€»è¾‘
                    task_collection = self.db_manager.get_collection('user_matching_tasks')
                    
                    task_collection.update_one(
                        {'task_id': task_id},
                        {
                            '$set': {
                                'status': status,
                                'progress': {
                                    'progress': progress,
                                    'processed': processed,
                                    'total': total,
                                    'matches': matches,
                                    'message': message
                                },
                                'updated_at': datetime.now().isoformat()
                            }
                        }
                    )
                    logger.debug(f"ä»»åŠ¡çŠ¶æ€æ›´æ–°ï¼ˆå†…å­˜+æ•°æ®åº“ï¼‰: {task_id} - {status} ({progress:.1f}%)")
            except Exception as e:
                logger.warning(f"æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥é—®é¢˜ï¼Œå°è¯•é‡è¿
                if "connection" in str(e).lower() or "network" in str(e).lower():
                    try:
                        logger.info("æ£€æµ‹åˆ°æ•°æ®åº“è¿æ¥é—®é¢˜ï¼Œå°è¯•é‡æ–°è¿æ¥å¹¶æ›´æ–°ä»»åŠ¡çŠ¶æ€...")
                        self.db_manager._reconnect_mongodb()
                        task_collection = self.db_manager.get_collection('user_matching_tasks')
                        task_collection.update_one(
                            {'task_id': task_id},
                            {
                                '$set': {
                                    'status': status,
                                    'progress': {
                                        'progress': progress,
                                        'processed': processed,
                                        'total': total,
                                        'matches': matches,
                                        'message': message
                                    },
                                    'updated_at': datetime.now().isoformat()
                                }
                            }
                        )
                        logger.debug(f"æ•°æ®åº“é‡è¿æˆåŠŸï¼Œä»»åŠ¡çŠ¶æ€æ›´æ–°å®Œæˆ: {task_id} - {status} ({progress:.1f}%)")
                    except Exception as retry_error:
                        logger.error(f"æ•°æ®åº“é‡è¿åä»»åŠ¡çŠ¶æ€æ›´æ–°ä»ç„¶å¤±è´¥: {retry_error}")
                        logger.debug(f"ä»»åŠ¡çŠ¶æ€æ›´æ–°ï¼ˆä»…å†…å­˜ï¼‰: {task_id} - {status} ({progress:.1f}%)")
                else:
                    logger.debug(f"ä»»åŠ¡çŠ¶æ€æ›´æ–°ï¼ˆä»…å†…å­˜ï¼‰: {task_id} - {status} ({progress:.1f}%)")
    
    def _execute_matching_task(self, task_id: str, config: Dict[str, Any]):
        """
        æ‰§è¡ŒåŒ¹é…ä»»åŠ¡ï¼ˆåå°çº¿ç¨‹ï¼‰
        
        Args:
            task_id: ä»»åŠ¡ID
            config: ä»»åŠ¡é…ç½®
        """
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œç”¨æˆ·æ•°æ®åŒ¹é…ä»»åŠ¡: {task_id}")
            
            # è·å–æ•°æ®åº“è¿æ¥
            if not self.db_manager or not self.db_manager.mongo_client:
                raise Exception("æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–")
            
            db = self.db_manager.mongo_client.get_database()
            task_collection = db['user_matching_tasks']
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€çš„å‡½æ•°
            def update_progress(percentage, processed, total, matches, status='running', current_operation=''):
                task_collection.update_one(
                    {'task_id': task_id},
                    {
                        '$set': {
                            'progress': {
                                'percentage': percentage,
                                'processed': processed,
                                'total': total,
                                'matches': matches,
                                'current_operation': current_operation
                            },
                            'status': status,
                            'updated_at': datetime.now().isoformat()
                        }
                    }
                )
            
            # è·å–æºè¡¨å’Œç›®æ ‡è¡¨æ•°æ®
            source_table = config['source_table']
            target_tables = config['target_tables']
            mappings = config['mappings']
            algorithm_type = config.get('algorithm_type', 'optimized')
            similarity_threshold = config.get('similarity_threshold', 0.7)
            batch_size = config.get('batch_size', 5000)  # ä¼˜åŒ–æ‰¹æ¬¡å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
            max_results = config.get('max_results', 10)
            
            update_progress(0, 0, 0, 0, 'running', 'æ­£åœ¨å‡†å¤‡æ•°æ®...')
            
            # è·å–æºè¡¨æ•°æ®
            source_collection = db[source_table]
            total_records = source_collection.count_documents({})
            
            if total_records == 0:
                raise Exception(f"æºè¡¨ {source_table} æ²¡æœ‰æ•°æ®")
            
            update_progress(5, 0, total_records, 0, 'running', 'æ­£åœ¨åŠ è½½æºæ•°æ®...')
            
            # è·å–ç›®æ ‡è¡¨æ•°æ®
            target_data = {}
            for target_table in target_tables:
                target_collection = db[target_table]
                target_data[target_table] = list(target_collection.find({}))
                logger.info(f"åŠ è½½ç›®æ ‡è¡¨ {target_table}: {len(target_data[target_table])} æ¡è®°å½•")
            
            update_progress(10, 0, total_records, 0, 'running', 'æ•°æ®åŠ è½½å®Œæˆï¼Œå¼€å§‹åŒ¹é…...')
            
            # é€‰æ‹©åŒ¹é…ç®—æ³•
            matcher = self._select_matcher(algorithm_type)
            
            # æ‰¹é‡å¤„ç†æºæ•°æ®
            processed = 0
            total_matches = 0
            results = []
            
            # ä½¿ç”¨æ¸¸æ ‡åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
            cursor = source_collection.find({}).batch_size(batch_size)
            
            for batch_start in range(0, total_records, batch_size):
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                current_task = task_collection.find_one({'task_id': task_id})
                if current_task and current_task.get('status') == 'stopped':
                    logger.info(f"ç”¨æˆ·æ•°æ®åŒ¹é…ä»»åŠ¡è¢«åœæ­¢: {task_id}")
                    return
                
                # è·å–å½“å‰æ‰¹æ¬¡æ•°æ®
                batch_records = []
                for _ in range(batch_size):
                    try:
                        record = next(cursor)
                        batch_records.append(record)
                    except StopIteration:
                        break
                
                if not batch_records:
                    break
                
                # æ‰§è¡Œæ‰¹æ¬¡åŒ¹é…
                batch_matches = self._execute_batch_matching(
                    batch_records, target_data, mappings, matcher,
                    similarity_threshold, max_results
                )
                
                results.extend(batch_matches)
                processed += len(batch_records)
                total_matches += len(batch_matches)
                
                # æ›´æ–°è¿›åº¦
                percentage = min(int((processed / total_records) * 100), 100)
                update_progress(
                    percentage, processed, total_records, total_matches,
                    'running', f'å·²å¤„ç† {processed}/{total_records} æ¡è®°å½•'
                )
                
                # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´ï¼ˆå®é™…åŒ¹é…ä¼šæœ‰è®¡ç®—æ—¶é—´ï¼‰
                time.sleep(0.1)
            
            # ä¿å­˜åŒ¹é…ç»“æœ
            update_progress(95, processed, total_records, total_matches, 'running', 'æ­£åœ¨ä¿å­˜ç»“æœ...')
            
            result_collection = db[f'user_match_results_{task_id}']
            if results:
                result_collection.insert_many(results)
                result_collection.create_index([('source_id', 1)])
                result_collection.create_index([('similarity_score', -1)])
            
            # ä»»åŠ¡å®Œæˆ
            update_progress(100, total_records, total_records, total_matches, 'completed', 'åŒ¹é…å®Œæˆ')
            
            logger.info(f"ç”¨æˆ·æ•°æ®åŒ¹é…ä»»åŠ¡å®Œæˆ: {task_id}, åŒ¹é…æ•°: {total_matches}")
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œç”¨æˆ·æ•°æ®åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            try:
                db = self.db_manager.mongo_client.get_database()
                task_collection = db['user_matching_tasks']
                task_collection.update_one(
                    {'task_id': task_id},
                    {
                        '$set': {
                            'status': 'failed',
                            'error': str(e),
                            'updated_at': datetime.now().isoformat()
                        }
                    }
                )
            except:
                pass
        finally:
            # æ¸…ç†ä»»åŠ¡ç¼“å­˜
            if task_id in self.running_tasks:
                del self.running_tasks[task_id]
    
    def _select_matcher(self, algorithm_type: str):
        """
        é€‰æ‹©åŒ¹é…ç®—æ³•
        
        Args:
            algorithm_type: ç®—æ³•ç±»å‹
            
        Returns:
            åŒ¹é…å™¨å®ä¾‹
        """
        matchers = {
            'exact': self.exact_matcher,
            'fuzzy': self.fuzzy_matcher,
            'enhanced': self.enhanced_fuzzy_matcher,
            'optimized': self.optimized_processor if hasattr(self, 'optimized_processor') else self.enhanced_fuzzy_matcher,
            'hybrid': self.enhanced_fuzzy_matcher  # æ··åˆç®—æ³•ä½¿ç”¨å¢å¼ºæ¨¡ç³ŠåŒ¹é…
        }
        
        return matchers.get(algorithm_type, self.enhanced_fuzzy_matcher)
    
    def _execute_batch_matching(self, source_records: List[Dict], target_data: Dict[str, List[Dict]],
                               mappings: List[Dict], matcher, similarity_threshold: float,
                               max_results: int) -> List[Dict]:
        """
        æ‰§è¡Œæ‰¹æ¬¡åŒ¹é…
        
        Args:
            source_records: æºè®°å½•åˆ—è¡¨
            target_data: ç›®æ ‡æ•°æ®å­—å…¸
            mappings: å­—æ®µæ˜ å°„é…ç½®
            matcher: åŒ¹é…å™¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            List[Dict]: åŒ¹é…ç»“æœåˆ—è¡¨
        """
        batch_results = []
        
        for source_record in source_records:
            # å¯¹æ¯ä¸ªç›®æ ‡è¡¨è¿›è¡ŒåŒ¹é…
            for target_table, target_records in target_data.items():
                # è·å–è¯¥ç›®æ ‡è¡¨çš„æ˜ å°„é…ç½®
                table_mappings = [m for m in mappings if m['target_table'] == target_table]
                
                if not table_mappings:
                    continue
                
                # æ‰§è¡ŒåŒ¹é…
                matches = self._match_record_to_targets(
                    source_record, target_records, table_mappings,
                    matcher, similarity_threshold, max_results
                )
                
                # æ·»åŠ åˆ°ç»“æœä¸­ï¼Œå‚ç…§åŸé¡¹ç›®è®¾è®¡ï¼Œä¿ç•™å¯è¿½æº¯çš„æºè¡¨IDå­—æ®µ
                for match in matches:
                    # æ„å»ºåŒ¹é…ç»“æœï¼Œå‚ç…§åŸé¡¹ç›®çš„æ•°æ®ç»“æ„
                    result = {
                        # åŸºç¡€æ ‡è¯†ä¿¡æ¯
                        'source_id': str(source_record.get('_id', '')),
                        'target_id': str(match['target_record'].get('_id', '')),
                        'source_table': source_table,
                        'target_table': target_table,
                        
                        # åŒ¹é…æ ¸å¿ƒä¿¡æ¯
                        'similarity_score': match['similarity'],
                        'matched_fields': match['matched_fields'],
                        'match_algorithm': config.get('algorithm_type', 'enhanced'),
                        
                        # æºè®°å½•å…³é”®å­—æ®µï¼ˆç”¨äºå¿«é€ŸæŸ¥çœ‹å’Œè¿½æº¯ï¼‰
                        'source_key_fields': self._extract_key_fields(source_record, mappings, 'source'),
                        'target_key_fields': self._extract_key_fields(match['target_record'], mappings, 'target'),
                        
                        # å®Œæ•´è®°å½•æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºè¯¦ç»†åˆ†æï¼‰
                        'source_record': source_record if config.get('save_full_records', False) else None,
                        'target_record': match['target_record'] if config.get('save_full_records', False) else None,
                        
                        # åŒ¹é…è¯¦ç»†ä¿¡æ¯
                        'match_details': {
                            'field_scores': match.get('details', {}).get('field_scores', {}),
                            'total_fields': match.get('details', {}).get('total_fields', 0),
                            'matched_field_count': match.get('details', {}).get('matched_field_count', 0),
                            'confidence_level': self._calculate_confidence_level(match['similarity']),
                            'match_type': self._determine_match_type(match['similarity'], match['matched_fields'])
                        },
                        
                        # å…ƒæ•°æ®ä¿¡æ¯
                        'created_at': datetime.now().isoformat(),
                        'task_id': task_id,
                        'config_id': config.get('config_id', ''),
                        'match_version': '2.0'  # æ ‡è¯†æ–°ç‰ˆæœ¬åŒ¹é…
                    }
                    batch_results.append(result)
        
        return batch_results
    
    def _match_record_to_targets(self, source_record: Dict, target_records: List[Dict],
                                mappings: List[Dict], matcher, similarity_threshold: float,
                                max_results: int) -> List[Dict]:
        """
        å°†æºè®°å½•ä¸ç›®æ ‡è®°å½•è¿›è¡ŒåŒ¹é…
        
        Args:
            source_record: æºè®°å½•
            target_records: ç›®æ ‡è®°å½•åˆ—è¡¨
            mappings: å­—æ®µæ˜ å°„
            matcher: åŒ¹é…å™¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            List[Dict]: åŒ¹é…ç»“æœ
        """
        matches = []
        
        for target_record in target_records:
            # è®¡ç®—åŒ¹é…ç›¸ä¼¼åº¦
            similarity, matched_fields, details = self._calculate_similarity(
                source_record, target_record, mappings, matcher
            )
            
            # å¦‚æœç›¸ä¼¼åº¦è¾¾åˆ°é˜ˆå€¼ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if similarity >= similarity_threshold:
                matches.append({
                    'target_record': target_record,
                    'similarity': similarity,
                    'matched_fields': matched_fields,
                    'details': details
                })
                logger.debug(f"âœ… åŒ¹é…æˆåŠŸ: ç›¸ä¼¼åº¦ {similarity:.3f} >= é˜ˆå€¼ {similarity_threshold}, åŒ¹é…å­—æ®µ: {matched_fields}")
            else:
                logger.debug(f"âŒ åŒ¹é…å¤±è´¥: ç›¸ä¼¼åº¦ {similarity:.3f} < é˜ˆå€¼ {similarity_threshold}, å­—æ®µå¾—åˆ†: {details.get('field_scores', {})}")
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œè¿”å›å‰Nä¸ªç»“æœ
        matches.sort(key=lambda x: x['similarity'], reverse=True)
        return matches[:max_results]
    
    def _calculate_similarity(self, source_record: Dict, target_record: Dict,
                            mappings: List[Dict], matcher) -> Tuple[float, List[str], Dict]:
        """
        è®¡ç®—ä¸¤æ¡è®°å½•çš„ç›¸ä¼¼åº¦
        
        Args:
            source_record: æºè®°å½•
            target_record: ç›®æ ‡è®°å½•
            mappings: å­—æ®µæ˜ å°„
            matcher: åŒ¹é…å™¨
            
        Returns:
            Tuple[float, List[str], Dict]: (ç›¸ä¼¼åº¦, åŒ¹é…å­—æ®µåˆ—è¡¨, è¯¦ç»†ä¿¡æ¯)
        """
        total_score = 0.0
        field_scores = {}
        matched_fields = []
        
        for mapping in mappings:
            source_field = mapping['source_field']
            target_field = mapping['target_field']
            
            source_value = source_record.get(source_field, '')
            target_value = target_record.get(target_field, '')
            
            # è·³è¿‡ç©ºå€¼
            if not source_value or not target_value:
                continue
            
            # è®¡ç®—å­—æ®µç›¸ä¼¼åº¦
            field_similarity = self._calculate_field_similarity(
                str(source_value), str(target_value), matcher, source_field, target_field
            )
            
            field_scores[f"{source_field}->{target_field}"] = field_similarity
            total_score += field_similarity
            
            if field_similarity > 0.5:  # å­—æ®µåŒ¹é…é˜ˆå€¼
                matched_fields.append(f"{source_field}->{target_field}")
        
        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
        avg_similarity = total_score / len(mappings) if mappings else 0.0
        
        details = {
            'field_scores': field_scores,
            'total_fields': len(mappings),
            'matched_field_count': len(matched_fields)
        }
        
        return avg_similarity, matched_fields, details
    
    def _calculate_field_similarity(self, value1: str, value2: str, matcher, source_field: str = '', target_field: str = '') -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå­—æ®µå€¼çš„ç›¸ä¼¼åº¦ï¼ˆæ”¯æŒåœ°å€è¯­ä¹‰åŒ¹é…ï¼‰
        
        Args:
            value1: å€¼1
            value2: å€¼2
            matcher: åŒ¹é…å™¨
            source_field: æºå­—æ®µåï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºåœ°å€å­—æ®µï¼‰
            target_field: ç›®æ ‡å­—æ®µåï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºåœ°å€å­—æ®µï¼‰
            
        Returns:
            float: ç›¸ä¼¼åº¦ (0-1)
        """
        try:
            # ã€å…³é”®ä¿®å¤ã€‘ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºåœ°å€å­—æ®µï¼Œä½¿ç”¨åœ°å€è¯­ä¹‰åŒ¹é…
            if self._is_address_field_simple(source_field, target_field):
                similarity = self.similarity_calculator.calculate_address_similarity(value1, value2)
                logger.debug(f"åœ°å€è¯­ä¹‰åŒ¹é…: {source_field}->{target_field}, {value1[:20]}... <-> {value2[:20]}... = {similarity:.3f}")
                return similarity
            
            # ä½¿ç”¨ä¸åŒçš„åŒ¹é…å™¨è®¡ç®—ç›¸ä¼¼åº¦
            if hasattr(matcher, 'calculate_similarity'):
                return matcher.calculate_similarity(value1, value2)
            elif hasattr(matcher, 'calculate_string_similarity'):
                return matcher.calculate_string_similarity(value1, value2)
            elif hasattr(matcher, 'fuzzy_match_score'):
                return matcher.fuzzy_match_score(value1, value2)
            else:
                # é»˜è®¤ä½¿ç”¨ç›¸ä¼¼åº¦è¯„åˆ†å™¨
                return self.similarity_calculator.calculate_string_similarity(value1, value2)
        except Exception as e:
            logger.warning(f"è®¡ç®—å­—æ®µç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            return 0.0
    
    def preview_matching(self, config_id: str, preview_count: int = 5,
                        algorithm_type: str = 'enhanced',
                        similarity_threshold: float = 0.7) -> List[Dict]:
        """
        é¢„è§ˆåŒ¹é…ç»“æœ
        
        Args:
            config_id: é…ç½®ID
            preview_count: é¢„è§ˆæ•°é‡
            algorithm_type: ç®—æ³•ç±»å‹
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            List[Dict]: é¢„è§ˆç»“æœ
        """
        try:
            if not self.db_manager or not self.db_manager.mongo_client:
                raise Exception("æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–")
            
            db = self.db_manager.mongo_client.get_database()
            
            # è·å–å­—æ®µæ˜ å°„é…ç½®
            config_collection = db['field_mapping_configs']
            config = config_collection.find_one({'config_id': config_id})
            
            if not config:
                raise Exception("æ˜ å°„é…ç½®ä¸å­˜åœ¨")
            
            # è·å–å°‘é‡æ•°æ®è¿›è¡Œé¢„è§ˆ
            source_collection = db[config['source_table']]
            source_records = list(source_collection.find({}).limit(preview_count))
            
            target_data = {}
            for target_table in config['target_tables']:
                target_collection = db[target_table]
                target_data[target_table] = list(target_collection.find({}).limit(100))  # é™åˆ¶ç›®æ ‡æ•°æ®é‡
            
            # é€‰æ‹©åŒ¹é…å™¨
            matcher = self._select_matcher(algorithm_type)
            
            # æ‰§è¡Œé¢„è§ˆåŒ¹é…
            preview_results = []
            for source_record in source_records:
                for target_table, target_records in target_data.items():
                    table_mappings = [m for m in config['mappings'] if m['target_table'] == target_table]
                    
                    if not table_mappings:
                        continue
                    
                    matches = self._match_record_to_targets(
                        source_record, target_records, table_mappings,
                        matcher, similarity_threshold, 1  # æ¯ä¸ªæºè®°å½•åªè¿”å›æœ€ä½³åŒ¹é…
                    )
                    
                    if matches:
                        preview_results.append({
                            'source_record': {k: v for k, v in source_record.items() if k != '_id'},
                            'matched_record': {k: v for k, v in matches[0]['target_record'].items() if k != '_id'},
                            'similarity': matches[0]['similarity'],
                            'matched_fields': matches[0]['matched_fields'],
                            'target_table': target_table
                        })
            
            return preview_results[:preview_count]
            
        except Exception as e:
            logger.error(f"é¢„è§ˆåŒ¹é…å¤±è´¥: {str(e)}")
            raise
    
    def stop_task(self, task_id: str) -> bool:
        """
        åœæ­¢åŒ¹é…ä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            if not self.db_manager or not self.db_manager.mongo_client:
                return False
            
            db = self.db_manager.mongo_client.get_database()
            task_collection = db['user_matching_tasks']
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºåœæ­¢
            result = task_collection.update_one(
                {'task_id': task_id},
                {
                    '$set': {
                        'status': 'stopped',
                        'updated_at': datetime.now().isoformat()
                    }
                }
            )
            
            # æ¸…ç†ä»»åŠ¡ç¼“å­˜
            if task_id in self.running_tasks:
                del self.running_tasks[task_id]
            
            return result.modified_count > 0
            
        except Exception as e:
            logger.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥: {str(e)}")
            return False
    
    def get_task_progress(self, task_id: str) -> Optional[Dict]:
        """
        è·å–ä»»åŠ¡è¿›åº¦
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            Optional[Dict]: ä»»åŠ¡è¿›åº¦ä¿¡æ¯
        """
        try:
            if not self.db_manager or not self.db_manager.mongo_client:
                return None
            
            db = self.db_manager.mongo_client.get_database()
            task_collection = db['user_matching_tasks']
            
            task = task_collection.find_one({'task_id': task_id})
            
            if not task:
                return None
            
            return {
                'progress': task.get('progress', {}),
                'status': task.get('status', 'unknown'),
                'error': task.get('error'),
                'created_at': task.get('created_at'),
                'updated_at': task.get('updated_at')
            }
            
        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")
            return None
    
    def _extract_key_fields(self, record: Dict, mappings: List[Dict], field_type: str) -> Dict:
        """
        æå–è®°å½•çš„å…³é”®å­—æ®µï¼ˆç”¨äºå¿«é€ŸæŸ¥çœ‹å’Œè¿½æº¯ï¼‰
        
        Args:
            record: è®°å½•æ•°æ®
            mappings: å­—æ®µæ˜ å°„é…ç½®
            field_type: å­—æ®µç±»å‹ ('source' æˆ– 'target')
            
        Returns:
            Dict: å…³é”®å­—æ®µå­—å…¸
        """
        key_fields = {}
        
        for mapping in mappings:
            if field_type == 'source':
                field_name = mapping['source_field']
            else:
                field_name = mapping['target_field']
            
            value = record.get(field_name)
            if value is not None:
                key_fields[field_name] = str(value)[:100]  # é™åˆ¶é•¿åº¦
        
        return key_fields
    
    def _calculate_confidence_level(self, similarity_score: float) -> str:
        """
        è®¡ç®—ç½®ä¿¡åº¦ç­‰çº§
        
        Args:
            similarity_score: ç›¸ä¼¼åº¦åˆ†æ•°
            
        Returns:
            str: ç½®ä¿¡åº¦ç­‰çº§
        """
        if similarity_score >= 0.9:
            return 'very_high'
        elif similarity_score >= 0.8:
            return 'high'
        elif similarity_score >= 0.7:
            return 'medium'
        elif similarity_score >= 0.6:
            return 'low'
        else:
            return 'very_low'
    
    def _determine_match_type(self, similarity_score: float, matched_fields: List[str]) -> str:
        """
        ç¡®å®šåŒ¹é…ç±»å‹
        
        Args:
            similarity_score: ç›¸ä¼¼åº¦åˆ†æ•°
            matched_fields: åŒ¹é…çš„å­—æ®µåˆ—è¡¨
            
        Returns:
            str: åŒ¹é…ç±»å‹
        """
        field_count = len(matched_fields)
        
        if similarity_score >= 0.95 and field_count >= 3:
            return 'exact_match'
        elif similarity_score >= 0.8 and field_count >= 2:
            return 'strong_match'
        elif similarity_score >= 0.7:
            return 'good_match'
        elif similarity_score >= 0.6:
            return 'weak_match'
        else:
            return 'poor_match'
