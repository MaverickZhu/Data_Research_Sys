#!/usr/bin/env python3
"""
ä¿®å¤çš„åˆ‡ç‰‡å¢å¼ºåŒ¹é…å™¨
è§£å†³calculate_similarityæ–¹æ³•è°ƒç”¨é—®é¢˜
"""

import re
import time
import jieba
import pymongo
from typing import Dict, List, Set, Optional
from fuzzywuzzy import fuzz
from src.utils.logger import logger

class SliceEnhancedMatcher:
    """ä¿®å¤ç‰ˆçš„åˆ‡ç‰‡å¢å¼ºåŒ¹é…å™¨"""
    
    def __init__(self, db_manager=None):
        """åˆå§‹åŒ–åˆ‡ç‰‡å¢å¼ºåŒ¹é…å™¨"""
        if db_manager:
            # ä½¿ç”¨get_db()æ–¹æ³•è·å–æ•°æ®åº“å®ä¾‹
            if hasattr(db_manager, 'get_db'):
                self.db = db_manager.get_db()
            elif hasattr(db_manager, 'db'):
                self.db = db_manager.db
            else:
                # é™çº§åˆ°é»˜è®¤è¿æ¥
                client = pymongo.MongoClient('mongodb://localhost:27017/')
                self.db = client['Unit_Info']
        else:
            client = pymongo.MongoClient('mongodb://localhost:27017/')
            self.db = client['Unit_Info']
        
        # ç¼“å­˜é›†åˆå¼•ç”¨
        self.source_slice_collection = self.db['xfaqpc_jzdwxx_name_slices']
        self.source_keyword_collection = self.db['xfaqpc_jzdwxx_name_keywords']
        self.target_slice_collection = self.db['xxj_shdwjbxx_name_slices']
        self.target_keyword_collection = self.db['xxj_shdwjbxx_name_keywords']
        self.target_collection = self.db['xxj_shdwjbxx']
        self.cache_collection = self.db['unit_name_similarity_cache']
        
        # é…ç½®å‚æ•°
        self.config = {
            'slice_match_threshold': 0.3,      # åˆ‡ç‰‡åŒ¹é…é˜ˆå€¼
            'keyword_match_threshold': 0.5,    # å…³é”®è¯åŒ¹é…é˜ˆå€¼
            'final_similarity_threshold': 0.75, # æœ€ç»ˆç›¸ä¼¼åº¦é˜ˆå€¼
            'max_candidates': 50,               # æœ€å¤§å€™é€‰æ•°é‡
            'use_cache': True,                  # å¯ç”¨ç¼“å­˜
            'cache_ttl': 7 * 24 * 3600         # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆ7å¤©ï¼‰
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'slice_queries': 0,
            'keyword_queries': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'total_matches': 0
        }
        
        logger.info("ğŸš€ ä¿®å¤ç‰ˆåˆ‡ç‰‡å¢å¼ºåŒ¹é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def calculate_similarity(self, source_name: str, target_name: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰
        
        Args:
            source_name: æºå­—ç¬¦ä¸²
            target_name: ç›®æ ‡å­—ç¬¦ä¸²
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0.0-1.0)
        """
        try:
            if not source_name or not target_name:
                return 0.0
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç¡®ä¿ç±»å‹æ­£ç¡®
            source_str = str(source_name).strip()
            target_str = str(target_name).strip()
            
            if not source_str or not target_str:
                return 0.0
            
            # ä½¿ç”¨å¤šç§ç®—æ³•è®¡ç®—ç›¸ä¼¼åº¦
            basic_similarity = fuzz.ratio(source_str, target_str) / 100.0
            token_similarity = fuzz.token_sort_ratio(source_str, target_str) / 100.0
            partial_similarity = fuzz.partial_ratio(source_str, target_str) / 100.0
            
            # åŠ æƒå¹³å‡
            weighted_score = (
                basic_similarity * 0.4 +
                token_similarity * 0.4 +
                partial_similarity * 0.2
            )
            
            return min(max(weighted_score, 0.0), 1.0)  # ç¡®ä¿ç»“æœåœ¨0-1ä¹‹é—´
            
        except Exception as e:
            logger.warning(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {source_name} vs {target_name}, é”™è¯¯: {e}")
            return 0.0
    
    def create_ngram_slices(self, text: str, n: int = 3) -> Set[str]:
        """åˆ›å»ºN-gramåˆ‡ç‰‡"""
        if not text or len(text) < n:
            return set()
        
        # æ¸…ç†æ–‡æœ¬
        clean_text = re.sub(r'[^\u4e00-\u9fff\w]', '', text)
        
        # ç”ŸæˆN-gramåˆ‡ç‰‡
        slices = set()
        for i in range(len(clean_text) - n + 1):
            slice_text = clean_text[i:i+n]
            if len(slice_text) == n:
                slices.add(slice_text)
        
        return slices
    
    def extract_keywords(self, text: str) -> Set[str]:
        """æå–å…³é”®è¯"""
        if not text:
            return set()
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(text)
        keywords = set()
        
        for word in words:
            word = word.strip()
            # è¿‡æ»¤æ‰é•¿åº¦å°äº2çš„è¯å’Œå¸¸è§åœç”¨è¯
            if len(word) >= 2 and word not in {'æœ‰é™', 'å…¬å¸', 'ä¼ä¸š', 'é›†å›¢', 'å·¥å‚', 'å•†åº—', 'ä¸­å¿ƒ'}:
                keywords.add(word)
        
        return keywords
    
    def find_candidates_by_slices(self, source_name: str, limit: int = 30) -> List[Dict]:
        """é€šè¿‡åˆ‡ç‰‡æŸ¥æ‰¾å€™é€‰è®°å½•ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # ç”Ÿæˆåˆ‡ç‰‡
            slices_3 = self.create_ngram_slices(source_name, 3)
            slices_2 = self.create_ngram_slices(source_name, 2)
            all_slices = slices_3.union(slices_2)
            
            if not all_slices:
                return []
            
            # ç®€åŒ–æŸ¥è¯¢ï¼šç›´æ¥ä»ç›®æ ‡é›†åˆä¸­æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„è®°å½•
            keywords = self.extract_keywords(source_name)
            if not keywords:
                return []
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„è®°å½•
            query_conditions = []
            for keyword in list(keywords)[:5]:  # é™åˆ¶å…³é”®è¯æ•°é‡
                query_conditions.append({'dwmc': {'$regex': keyword, '$options': 'i'}})
            
            if query_conditions:
                query = {'$or': query_conditions}
                candidates = list(self.target_collection.find(
                    query,
                    {'dwmc': 1, 'dwdz': 1, 'tyshxydm': 1, 'fddbr': 1}
                ).limit(limit))
                
                # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                for candidate in candidates:
                    target_name = candidate.get('dwmc', '')
                    if target_name:
                        candidate['slice_score'] = self.calculate_similarity(source_name, target_name)
                
                # æŒ‰ç›¸ä¼¼åº¦æ’åº
                candidates.sort(key=lambda x: x.get('slice_score', 0), reverse=True)
                return candidates[:limit]
            
            return []
            
        except Exception as e:
            logger.error(f"åˆ‡ç‰‡æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def find_best_matches(self, source_name: str, limit: int = 10) -> List[Dict]:
        """æŸ¥æ‰¾æœ€ä½³åŒ¹é…ï¼ˆä¸»è¦æ¥å£ï¼‰"""
        try:
            candidates = self.find_candidates_by_slices(source_name, limit * 3)
            
            if not candidates:
                return []
            
            # é‡æ–°è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
            results = []
            for candidate in candidates:
                target_name = candidate.get('dwmc', '')
                if target_name:
                    similarity = self.calculate_similarity(source_name, target_name)
                    if similarity >= 0.5:  # åªä¿ç•™ç›¸ä¼¼åº¦è¾ƒé«˜çš„ç»“æœ
                        candidate['similarity'] = similarity
                        results.append(candidate)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
            return results[:limit]
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾æœ€ä½³åŒ¹é…å¤±è´¥: {e}")
            return []
    
    def get_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return self.stats.copy()
