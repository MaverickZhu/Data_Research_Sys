#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„åŒ¹é…å¤„ç†å™¨æ¨¡å—
è§£å†³é‡å¤æ•°æ®ã€å¢é‡åŒ¹é…å’Œä¿¡æ¯å®Œæ•´æ€§é—®é¢˜
"""

import logging
import uuid
import json
from typing import Dict, List, Optional, Union, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import time

from .exact_matcher import ExactMatcher, MatchResult
from .fuzzy_matcher import FuzzyMatcher
from .optimized_fuzzy_matcher import OptimizedFuzzyMatcher, FuzzyMatchResult
from .enhanced_fuzzy_matcher import EnhancedFuzzyMatcher, EnhancedFuzzyMatchResult
from .graph_matcher import GraphMatcher
from .prefilter_system import PrefilterSystem
from src.utils.helpers import batch_iterator, generate_match_id, format_timestamp

logger = logging.getLogger(__name__)


class MatchingMode:
    """åŒ¹é…æ¨¡å¼æšä¸¾"""
    INCREMENTAL = "incremental"  # å¢é‡åŒ¹é…ï¼Œåªå¤„ç†æœªåŒ¹é…çš„è®°å½•
    UPDATE = "update"           # æ›´æ–°åŒ¹é…ï¼Œé‡æ–°åŒ¹é…å·²æœ‰ç»“æœ
    FULL = "full"              # å…¨é‡åŒ¹é…ï¼Œæ¸…ç©ºåé‡æ–°åŒ¹é…


class OptimizedMatchProgress:
    """ä¼˜åŒ–çš„åŒ¹é…è¿›åº¦è·Ÿè¸ª"""
    
    def __init__(self, task_id: str, total_records: int, mode: str = MatchingMode.INCREMENTAL):
        self.task_id = task_id
        self.total_records = total_records
        self.mode = mode
        self.processed_records = 0
        self.matched_records = 0
        self.updated_records = 0
        self.skipped_records = 0
        self.error_records = 0
        self.start_time = datetime.now()
        self.status = "running"  # running, completed, error, stopped
        self.lock = Lock()
        self.current_batch = 0
        self.last_processed_id = None
    
    def update_progress(self, processed: int = 1, matched: int = 0, updated: int = 0, 
                       skipped: int = 0, error: int = 0, last_id: str = None):
        """æ›´æ–°è¿›åº¦"""
        with self.lock:
            self.processed_records += processed
            self.matched_records += matched
            self.updated_records += updated
            self.skipped_records += skipped
            self.error_records += error
            if last_id:
                self.last_processed_id = last_id
    
    def get_progress(self) -> Dict:
        """è·å–è¿›åº¦ä¿¡æ¯"""
        with self.lock:
            elapsed_time = (datetime.now() - self.start_time).total_seconds()
            
            # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            progress_percent = (self.processed_records / self.total_records * 100) if self.total_records > 0 else 0
            
            # ä¼°ç®—å‰©ä½™æ—¶é—´ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé˜²æ­¢å¼‚å¸¸å€¼
            if self.processed_records > 0 and elapsed_time > 0:
                avg_time_per_record = elapsed_time / self.processed_records
                remaining_records = self.total_records - self.processed_records
                
                # åŸºç¡€å‰©ä½™æ—¶é—´è®¡ç®—
                raw_estimated_time = remaining_records * avg_time_per_record
                
                # å¼‚å¸¸å€¼å¤„ç† - é˜²æ­¢æ˜¾ç¤ºè¿‡å¤§çš„æ—¶é—´
                if raw_estimated_time > 86400:  # è¶…è¿‡24å°æ—¶
                    # ä½¿ç”¨æœ€è¿‘çš„å¤„ç†é€Ÿåº¦æ¥ä¼°ç®—ï¼ˆæœ€è¿‘100æ¡è®°å½•çš„å¹³å‡é€Ÿåº¦ï¼‰
                    if self.processed_records >= 10:
                        recent_avg_time = elapsed_time / min(self.processed_records, 100)
                        estimated_remaining_time = min(raw_estimated_time, remaining_records * recent_avg_time)
                        
                        # å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œåˆ™é™åˆ¶æœ€å¤§æ˜¾ç¤ºæ—¶é—´
                        if estimated_remaining_time > 7200:  # è¶…è¿‡2å°æ—¶
                            estimated_remaining_time = min(estimated_remaining_time, 7200)  # æœ€å¤šæ˜¾ç¤º2å°æ—¶
                    else:
                        estimated_remaining_time = min(raw_estimated_time, 3600)  # æœ€å¤šæ˜¾ç¤º1å°æ—¶
                else:
                    estimated_remaining_time = raw_estimated_time
                
                # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
                if estimated_remaining_time < 0 or estimated_remaining_time > 86400:
                    estimated_remaining_time = 0  # å¼‚å¸¸æƒ…å†µä¸‹è®¾ä¸º0
            else:
                estimated_remaining_time = 0
            
            return {
                'task_id': self.task_id,
                'mode': self.mode,
                'status': self.status,
                'total_records': self.total_records,
                'processed_records': self.processed_records,
                'matched_records': self.matched_records,
                'updated_records': self.updated_records,
                'skipped_records': self.skipped_records,
                'error_records': self.error_records,
                'current_batch': self.current_batch,
                'last_processed_id': self.last_processed_id,
                'progress_percent': round(progress_percent, 2),
                'elapsed_time': round(elapsed_time, 2),
                'estimated_remaining_time': round(estimated_remaining_time, 2),
                'match_rate': round((self.matched_records / self.processed_records * 100) if self.processed_records > 0 else 0, 2)
            }
    
    def set_status(self, status: str):
        """è®¾ç½®çŠ¶æ€"""
        with self.lock:
            self.status = status
    
    def set_current_batch(self, batch_num: int):
        """è®¾ç½®å½“å‰æ‰¹æ¬¡"""
        with self.lock:
            self.current_batch = batch_num


class OptimizedMatchProcessor:
    """ä¼˜åŒ–çš„åŒ¹é…å¤„ç†å™¨"""
    
    def __init__(self, db_manager, config: Dict):
        """
        åˆå§‹åŒ–ä¼˜åŒ–åŒ¹é…å¤„ç†å™¨
        
        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            config: åŒ¹é…é…ç½®
        """
        self.db_manager = db_manager
        self.config = config
        
        # åˆå§‹åŒ–åŒ¹é…å™¨
        self.exact_matcher = ExactMatcher(config)
        self.fuzzy_matcher = FuzzyMatcher(config)
        self.optimized_fuzzy_matcher = OptimizedFuzzyMatcher(config)
        self.enhanced_fuzzy_matcher = EnhancedFuzzyMatcher(config)  # æ–°å¢å¢å¼ºæ¨¡ç³ŠåŒ¹é…å™¨
        self.prefilter_system = PrefilterSystem(db_manager.get_db()) # åˆå§‹åŒ–é¢„è¿‡æ»¤å™¨
        
        # å›¾åŒ¹é…å™¨é…ç½®
        self.graph_config = config.get('graph_matching', {})
        self.use_graph_matcher = self.graph_config.get('enabled', True)
        if self.use_graph_matcher:
            logger.info("å›¾åŒ¹é…å™¨æ¨¡å—å·²å¯ç”¨ã€‚")
            self.graph_matcher = GraphMatcher(self.db_manager.get_db(), self.config)
            # æ„å»ºä¸€ä¸ªåŒ…å«å°‘é‡æ•°æ®çš„çƒ­å¯åŠ¨å›¾
            initial_build_limit = self.graph_config.get('initial_build_limit', 5000)
            self.graph_matcher.build_graph(limit=initial_build_limit)
        
        # æ‰¹å¤„ç†é…ç½®
        self.batch_config = config.get('batch_processing', {})
        self.batch_size = self.batch_config.get('batch_size', 100)
        self.max_workers = self.batch_config.get('max_workers', 4)
        self.timeout = self.batch_config.get('timeout', 300)
        
        # ä»»åŠ¡ç®¡ç†
        self.active_tasks = {}  # task_id -> OptimizedMatchProgress
        self.tasks_lock = Lock()
    
    def _safe_str(self, value, default: str = '') -> str:
        """å®‰å…¨åœ°å°†ä»»ä½•ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å»é™¤ç©ºç™½"""
        if value is None:
            return default
        try:
            return str(value).strip()
        except Exception:
            return default
    
    def start_optimized_matching_task(self, match_type: str = "both", 
                                    mode: str = MatchingMode.INCREMENTAL,
                                    batch_size: Optional[int] = None,
                                    clear_existing: bool = False) -> str:
        """
        å¯åŠ¨ä¼˜åŒ–çš„åŒ¹é…ä»»åŠ¡
        
        Args:
            match_type: åŒ¹é…ç±»å‹ ('exact', 'fuzzy', 'both')
            mode: åŒ¹é…æ¨¡å¼ (incremental, update, full)
            batch_size: æ‰¹å¤„ç†å¤§å°
            clear_existing: æ˜¯å¦æ¸…ç©ºç°æœ‰ç»“æœ
            
        Returns:
            str: ä»»åŠ¡ID
        """
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # ä½¿ç”¨è‡ªå®šä¹‰æ‰¹æ¬¡å¤§å°
        if batch_size:
            self.batch_size = batch_size
        
        try:
            # å¦‚æœæ˜¯å…¨é‡åŒ¹é…æˆ–è¦æ±‚æ¸…ç©ºï¼Œå…ˆæ¸…ç©ºç°æœ‰ç»“æœ
            if mode == MatchingMode.FULL or clear_existing:
                self._clear_match_results()
                logger.info("å·²æ¸…ç©ºç°æœ‰åŒ¹é…ç»“æœ")
            
            # è·å–éœ€è¦å¤„ç†çš„è®°å½•æ•°ï¼ˆå®‰å…¨æ’æŸ¥ç³»ç»Ÿï¼‰
            if mode == MatchingMode.INCREMENTAL:
                total_records = self._get_unmatched_count()
            else:
                total_records = self.db_manager.get_collection_count('xfaqpc_jzdwxx')
            
            if total_records == 0:
                if mode == MatchingMode.INCREMENTAL:
                    raise ValueError("æ‰€æœ‰è®°å½•å·²åŒ¹é…ï¼Œæ— éœ€å¢é‡å¤„ç†")
                else:
                    raise ValueError("æºæ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒåŒ¹é…")
            
            # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
            progress = OptimizedMatchProgress(task_id, total_records, mode)
            
            with self.tasks_lock:
                self.active_tasks[task_id] = progress
            
            # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
            from threading import Thread
            task_thread = Thread(
                target=self._execute_optimized_matching_task,
                args=(task_id, match_type, mode),
                daemon=True
            )
            task_thread.start()
            
            logger.info(f"ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å¯åŠ¨æˆåŠŸ: {task_id}, ç±»å‹: {match_type}, æ¨¡å¼: {mode}, æ€»è®°å½•æ•°: {total_records}")
            
            return task_id
            
        except Exception as e:
            logger.error(f"å¯åŠ¨ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
            raise
    
    def _execute_optimized_matching_task(self, task_id: str, match_type: str, mode: str):
        """æ‰§è¡Œä¼˜åŒ–çš„åŒ¹é…ä»»åŠ¡"""
        progress = self.active_tasks.get(task_id)
        if not progress:
            return
        
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œä¼˜åŒ–åŒ¹é…ä»»åŠ¡: {task_id}, æ¨¡å¼: {mode}")
            
            # è·å–ç›®æ ‡æ•°æ®ï¼ˆæ¶ˆé˜²ç›‘ç£ç®¡ç†ç³»ç»Ÿï¼‰- ä¸å†å…¨é‡åŠ è½½
            # target_records = self._load_target_records()
            # logger.info(f"åŠ è½½ç›®æ ‡æ•°æ®ï¼ˆæ¶ˆé˜²ç›‘ç£ç®¡ç†ç³»ç»Ÿï¼‰: {len(target_records)} æ¡")
            
            # æ ¹æ®æ¨¡å¼è·å–éœ€è¦å¤„ç†çš„æºæ•°æ®ï¼ˆå®‰å…¨æ’æŸ¥ç³»ç»Ÿï¼‰
            if mode == MatchingMode.INCREMENTAL:
                source_records_generator = self._get_unmatched_records_generator()
            else:
                source_records_generator = self._get_all_records_generator()
            
            batch_count = 0
            
            # åˆ†æ‰¹å¤„ç†
            for source_batch in source_records_generator:
                if not source_batch:
                    break
                
                batch_count += 1
                progress.set_current_batch(batch_count)
                logger.info(f"å¤„ç†ç¬¬ {batch_count} æ‰¹æ•°æ®: {len(source_batch)} æ¡")
                
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢ - åœ¨å¤„ç†æ‰¹æ¬¡ä¹‹å‰æ£€æŸ¥
                if progress.status == "stopped":
                    logger.info(f"ä»»åŠ¡åœæ­¢ä¿¡å·æ£€æµ‹åˆ°ï¼Œåœæ­¢å¤„ç†æ–°æ‰¹æ¬¡: {task_id}")
                    break
                
                # å¤„ç†å½“å‰æ‰¹æ¬¡ (ä¸å†ä¼ å…¥ target_records)
                self._process_optimized_batch(task_id, source_batch, match_type, mode)
                
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨æ‰¹æ¬¡å¤„ç†è¿‡ç¨‹ä¸­è¢«åœæ­¢
                if progress.status == "stopped":
                    logger.info(f"ä»»åŠ¡åœ¨æ‰¹æ¬¡å¤„ç†è¿‡ç¨‹ä¸­è¢«åœæ­¢: {task_id}")
                    break
            
            # ä»»åŠ¡å®Œæˆ - ç¡®ä¿æœ€åçš„æ•°æ®ä¿å­˜
            if progress.status != "stopped":
                progress.set_status("completed")
                logger.info(f"ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å®Œæˆ: {task_id}")
            else:
                logger.info(f"ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å·²åœæ­¢: {task_id}")
                # å¼ºåˆ¶æœ€ç»ˆæ•°æ®ä¿å­˜æ£€æŸ¥
                final_save_count = self._force_final_save_check()
                logger.info(f"ä»»åŠ¡åœæ­¢æ—¶æœ€ç»ˆä¿å­˜æ£€æŸ¥å®Œæˆ: ä¿å­˜äº† {final_save_count} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–åŒ¹é…ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {task_id}: {str(e)}")
            progress.set_status("error")
    
    def _get_unmatched_count(self) -> int:
        """è·å–æœªåŒ¹é…è®°å½•æ•°é‡"""
        try:
            # è·å–å·²åŒ¹é…çš„æºè®°å½•IDåˆ—è¡¨
            matched_ids = self._get_matched_source_ids()
            
            # è®¡ç®—æ€»è®°å½•æ•°ï¼ˆå®‰å…¨æ’æŸ¥ç³»ç»Ÿï¼‰
            total_count = self.db_manager.get_collection_count('xfaqpc_jzdwxx')
            
            # è¿”å›æœªåŒ¹é…æ•°é‡
            return total_count - len(matched_ids)
            
        except Exception as e:
            logger.error(f"è·å–æœªåŒ¹é…è®°å½•æ•°é‡å¤±è´¥: {str(e)}")
            return 0
    
    def _get_matched_source_ids(self) -> set:
        """è·å–å·²åŒ¹é…çš„æºè®°å½•IDé›†åˆ"""
        try:
            collection = self.db_manager.get_collection('unit_match_results')
            
            # æŸ¥è¯¢æ‰€æœ‰å·²åŒ¹é…çš„æºè®°å½•IDï¼Œç§»é™¤hintè®©MongoDBè‡ªåŠ¨é€‰æ‹©ç´¢å¼•
            cursor = collection.find(
                {'primary_record_id': {'$exists': True, '$ne': None}},
                {'primary_record_id': 1}
            )
            
            matched_ids = {str(doc['primary_record_id']) for doc in cursor}
            logger.debug(f"å·²åŒ¹é…è®°å½•æ•°: {len(matched_ids)}")
            
            return matched_ids
            
        except Exception as e:
            logger.error(f"è·å–å·²åŒ¹é…è®°å½•IDå¤±è´¥: {str(e)}")
            return set()
    
    def _get_unmatched_records_generator(self):
        """è·å–æœªåŒ¹é…è®°å½•çš„ç”Ÿæˆå™¨ï¼ˆå®‰å…¨æ’æŸ¥ç³»ç»Ÿï¼‰"""
        try:
            matched_ids = self._get_matched_source_ids()
            
            skip = 0
            while True:
                # è·å–ä¸€æ‰¹æºæ•°æ®ï¼ˆå®‰å…¨æ’æŸ¥ç³»ç»Ÿï¼‰
                source_batch = self.db_manager.get_inspection_units(
                    skip=skip, 
                    limit=self.batch_size
                )
                
                if not source_batch:
                    break
                
                # è¿‡æ»¤å‡ºæœªåŒ¹é…çš„è®°å½•
                unmatched_batch = [
                    record for record in source_batch 
                    if str(record.get('_id')) not in matched_ids
                ]
                
                if unmatched_batch:
                    yield unmatched_batch
                
                skip += self.batch_size
                
        except Exception as e:
            logger.error(f"è·å–æœªåŒ¹é…è®°å½•å¤±è´¥: {str(e)}")
            yield []
    
    def _get_all_records_generator(self):
        """è·å–æ‰€æœ‰è®°å½•çš„ç”Ÿæˆå™¨ï¼ˆå®‰å…¨æ’æŸ¥ç³»ç»Ÿï¼‰"""
        try:
            skip = 0
            while True:
                # è·å–ä¸€æ‰¹æºæ•°æ®ï¼ˆå®‰å…¨æ’æŸ¥ç³»ç»Ÿï¼‰
                source_batch = self.db_manager.get_inspection_units(
                    skip=skip, 
                    limit=self.batch_size
                )
                
                if not source_batch:
                    break
                
                yield source_batch
                skip += self.batch_size
                
        except Exception as e:
            logger.error(f"è·å–æ‰€æœ‰è®°å½•å¤±è´¥: {str(e)}")
            yield []
    
    def _process_optimized_batch(self, task_id: str, source_batch: List[Dict], 
                               match_type: str, mode: str):
        """å¤„ç†ä¼˜åŒ–çš„æ‰¹æ¬¡æ•°æ®"""
        progress = self.active_tasks.get(task_id)
        if not progress:
            return
        
        batch_results = []
        task_stopped = False
        
        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†æ‰¹æ¬¡: {len(source_batch)} æ¡è®°å½•ï¼Œæ¨¡å¼: {mode}")
        
        for source_record in source_batch:
            try:
                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                if progress.status == "stopped":
                    task_stopped = True
                    logger.info(f"ğŸ›‘ ä»»åŠ¡åœæ­¢ä¿¡å·æ£€æµ‹åˆ°ï¼Œå½“å‰æ‰¹æ¬¡å·²å¤„ç† {len(batch_results)} æ¡è®°å½•")
                    break
                
                # å¤„ç†å•æ¡è®°å½• (ä¸å†ä¼ å…¥ target_records)
                result = self._process_optimized_single_record(
                    source_record, match_type, mode
                )
                
                # è¯¦ç»†è®°å½•ç»“æœå¤„ç†
                if result:
                    operation = result.get('operation', 'unknown')
                    unit_name = source_record.get('UNIT_NAME', 'Unknown')
                    
                    if operation == 'skipped':
                        logger.info(f"ğŸ“ è®°å½•è·³è¿‡: {unit_name} - {result.get('reason', 'unknown')}")
                    else:
                        batch_results.append(result)
                        logger.info(f"ğŸ“ è®°å½•æ·»åŠ åˆ°æ‰¹æ¬¡: {unit_name} - æ“ä½œ: {operation}")
                    
                    # æ›´æ–°è¿›åº¦
                    if operation == 'matched':
                        progress.update_progress(
                            processed=1, matched=1, 
                            last_id=str(source_record.get('_id'))
                        )
                    elif operation == 'updated':
                        progress.update_progress(
                            processed=1, updated=1,
                            last_id=str(source_record.get('_id'))
                        )
                    elif operation == 'skipped':
                        progress.update_progress(
                            processed=1, skipped=1,
                            last_id=str(source_record.get('_id'))
                        )
                    else:
                        progress.update_progress(
                            processed=1,
                            last_id=str(source_record.get('_id'))
                        )
                else:
                    logger.warning(f"ğŸ“ è®°å½•å¤„ç†å¤±è´¥: {source_record.get('UNIT_NAME', 'Unknown')}")
                    progress.update_progress(
                        processed=1, error=1,
                        last_id=str(source_record.get('_id'))
                    )
                    
            except Exception as e:
                logger.error(f"å¤„ç†è®°å½•å¤±è´¥: {str(e)}")
                progress.update_progress(processed=1, error=1)
        
        # æ‰¹é‡ä¿å­˜ç»“æœ - æ— è®ºæ˜¯å¦åœæ­¢éƒ½è¦ä¿å­˜å·²å¤„ç†çš„ç»“æœ
        logger.info(f"ğŸ’¾ å‡†å¤‡ä¿å­˜æ‰¹æ¬¡ç»“æœ: {len(batch_results)} æ¡è®°å½•")
        
        if batch_results:
            logger.info(f"ğŸ’¾ å¼€å§‹æ‰¹é‡ä¿å­˜ {len(batch_results)} æ¡åŒ¹é…ç»“æœ...")
            
            # æ˜¾ç¤ºå‰å‡ æ¡ç»“æœçš„è¯¦ç»†ä¿¡æ¯
            for i, result in enumerate(batch_results[:3]):
                logger.info(f"ğŸ’¾ ç»“æœ {i+1}: {result.get('unit_name', 'Unknown')} -> {result.get('operation', 'unknown')}")
            
            save_success = self._batch_save_optimized_results(batch_results)
            if save_success:
                if task_stopped:
                    logger.info(f"âœ… ä»»åŠ¡åœæ­¢å‰æˆåŠŸä¿å­˜ {len(batch_results)} æ¡åŒ¹é…ç»“æœ")
                else:
                    logger.info(f"âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {len(batch_results)} æ¡åŒ¹é…ç»“æœ")
            else:
                logger.error(f"âŒ ä¿å­˜ {len(batch_results)} æ¡åŒ¹é…ç»“æœå¤±è´¥")
        else:
            if task_stopped:
                logger.warning("âš ï¸ ä»»åŠ¡åœæ­¢ï¼Œå½“å‰æ‰¹æ¬¡æ— éœ€ä¿å­˜çš„åŒ¹é…ç»“æœ")
            else:
                logger.warning("âš ï¸ æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œä½†æ²¡æœ‰ç”Ÿæˆä»»ä½•åŒ¹é…ç»“æœ")
            
            # å¢åŠ è°ƒè¯•ä¿¡æ¯
            logger.info(f"ğŸ” è°ƒè¯•ä¿¡æ¯: æ‰¹æ¬¡å¤§å°={len(source_batch)}, ç»“æœæ•°é‡={len(batch_results)}, æ¨¡å¼={mode}")
            
        # å¦‚æœä»»åŠ¡åœæ­¢ï¼Œæ ‡è®°ä»»åŠ¡çŠ¶æ€
        if task_stopped:
            logger.info(f"ä»»åŠ¡åœ¨æ‰¹æ¬¡å¤„ç†è¿‡ç¨‹ä¸­è¢«åœæ­¢: {task_id}")
            progress.set_status("stopped")
    
    def _process_optimized_single_record(self, source_record: Dict, 
                                       match_type: str, mode: str) -> Optional[Dict]:
        """å¤„ç†ä¼˜åŒ–çš„å•æ¡è®°å½•"""
        try:
            source_id = str(source_record.get('_id'))
            
            # 1. ä½¿ç”¨é¢„è¿‡æ»¤ç³»ç»Ÿè·å–å€™é€‰è®°å½•
            target_candidates = self.prefilter_system.get_candidates(source_record)
            if not target_candidates:
                logger.info(f"é¢„è¿‡æ»¤æœªèƒ½æ‰¾åˆ°ä»»ä½•å€™é€‰è®°å½•: {source_record.get('UNIT_NAME', 'Unknown')}")
                return self._format_optimized_no_match_result(source_record)
            
            logger.info(f"ä¸º {source_record.get('UNIT_NAME', 'Unknown')} æ‰¾åˆ° {len(target_candidates)} ä¸ªå€™é€‰ã€‚")

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒ¹é…ç»“æœ
            existing_result = self._get_existing_match_result(source_id)
            
            # æ‰§è¡ŒåŒ¹é…
            match_result = None
            
            # ç²¾ç¡®åŒ¹é…
            if match_type in ['exact', 'both']:
                exact_result = self.exact_matcher.match_single_record(source_record, target_candidates)
                
                if exact_result.matched:
                    match_result = self._format_optimized_match_result(
                        exact_result, 'exact', source_record
                    )
                    logger.info(f"ä¿¡ç”¨ä»£ç ç²¾ç¡®åŒ¹é…æˆåŠŸ: {source_record.get('UNIT_NAME', 'Unknown')}")
            
            # æ¨¡ç³ŠåŒ¹é…ï¼ˆä»…åœ¨ç²¾ç¡®åŒ¹é…å¤±è´¥æˆ–æŒ‡å®šæ¨¡ç³ŠåŒ¹é…æ—¶è¿›è¡Œï¼‰
            if match_type in ['fuzzy', 'both'] and not match_result:
                # ä¼˜å…ˆä½¿ç”¨å¢å¼ºçš„æ¨¡ç³ŠåŒ¹é…å™¨ï¼ˆè§£å†³åŒ¹é…å¹»è§‰é—®é¢˜ï¼‰
                enhanced_fuzzy_result = self.enhanced_fuzzy_matcher.match_single_record(
                    source_record, target_candidates
                )
                
                if enhanced_fuzzy_result.matched:
                    # å›¾åŒ¹é…å¢å¼ºé€»è¾‘
                    if self.use_graph_matcher and 0.7 < enhanced_fuzzy_result.similarity_score < 1.0:
                        logger.info(f"è§¦å‘å›¾åŒ¹é…äºŒæ¬¡éªŒè¯ï¼Œå½“å‰åˆ†æ•°: {enhanced_fuzzy_result.similarity_score:.3f}")
                        
                        # åŠ¨æ€å°†å½“å‰æ¯”è¾ƒçš„è®°å½•æ·»åŠ åˆ°å›¾ä¸­ï¼Œç¡®ä¿å®ƒä»¬å­˜åœ¨
                        self.graph_matcher.add_unit_to_graph(source_record, 'xfaqpc', 'UNIT_NAME', 'UNIT_ADDRESS', 'LEGAL_PEOPLE')
                        if enhanced_fuzzy_result.target_record:
                            self.graph_matcher.add_unit_to_graph(enhanced_fuzzy_result.target_record, 'xxj', 'dwmc', 'dwdz', 'fddbr')
                        
                        graph_score = self.graph_matcher.calculate_graph_score(
                            source_record, 
                            enhanced_fuzzy_result.target_record
                        )
                        
                        if graph_score > 0:
                            original_score = enhanced_fuzzy_result.similarity_score
                            enhanced_fuzzy_result.similarity_score = 0.98  # æå‡åˆ†æ•°
                            warning_msg = f"å›¾åŒ¹é…å¢å¼º: å…±äº«å±æ€§å‘ç°ï¼Œåˆ†æ•°ä» {original_score:.3f} æå‡è‡³ 0.98"
                            enhanced_fuzzy_result.match_warnings.append(warning_msg)
                            logger.info(f"âœ… {warning_msg} for {source_record.get('UNIT_NAME')}")

                    match_result = self._format_optimized_match_result(
                        enhanced_fuzzy_result, 'fuzzy_enhanced', source_record
                    )
                    logger.info(f"å¢å¼ºæ¨¡ç³ŠåŒ¹é…æˆåŠŸ: {source_record.get('UNIT_NAME', 'Unknown')}, "
                               f"ç›¸ä¼¼åº¦: {enhanced_fuzzy_result.similarity_score:.3f}")
                    
                    # è®°å½•åŒ¹é…è­¦å‘Š
                    if enhanced_fuzzy_result.match_warnings:
                        logger.warning(f"åŒ¹é…è­¦å‘Š: {', '.join(enhanced_fuzzy_result.match_warnings)}")
                else:
                    # å¦‚æœå¢å¼ºæ¨¡ç³ŠåŒ¹é…å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä¼˜åŒ–çš„æ¨¡ç³ŠåŒ¹é…å™¨ä½œä¸ºå¤‡ç”¨
                    optimized_fuzzy_result = self.optimized_fuzzy_matcher.match_single_record_optimized(
                        source_record, target_candidates
                    )
                    
                    if optimized_fuzzy_result.get('matched', False):
                        # è½¬æ¢ä¸ºæ ‡å‡†çš„FuzzyMatchResultæ ¼å¼
                        fuzzy_result = FuzzyMatchResult(
                            matched=True,
                            similarity_score=optimized_fuzzy_result.get('similarity_score', 0.0),
                            source_record=source_record,
                            target_record=optimized_fuzzy_result.get('target_record'),
                            match_details=optimized_fuzzy_result.get('match_details', {})
                        )
                        
                        match_result = self._format_optimized_match_result(
                            fuzzy_result, 'fuzzy_optimized', source_record
                        )
                        logger.info(f"ä¼˜åŒ–æ¨¡ç³ŠåŒ¹é…æˆåŠŸ: {source_record.get('UNIT_NAME', 'Unknown')}, "
                                   f"å€™é€‰æ•°: {optimized_fuzzy_result.get('match_details', {}).get('candidates_count', 0)}")
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…ç»“æœï¼Œåˆ›å»ºæœªåŒ¹é…è®°å½•
            if not match_result:
                match_result = self._format_optimized_no_match_result(source_record)
                logger.info(f"æœªæ‰¾åˆ°åŒ¹é…: {source_record.get('UNIT_NAME', 'Unknown')}")
            
            # å¢é‡æ¨¡å¼é€»è¾‘ä¿®å¤ï¼šåªæœ‰åœ¨æ²¡æœ‰æ‰¾åˆ°æ–°åŒ¹é…ä¸”å·²å­˜åœ¨ç»“æœæ—¶æ‰è·³è¿‡
            if mode == MatchingMode.INCREMENTAL and existing_result:
                # æ£€æŸ¥æ–°åŒ¹é…ç»“æœæ˜¯å¦ä¸ç°æœ‰ç»“æœç›¸åŒ
                if match_result and existing_result:
                    existing_matched_id = existing_result.get('matched_record_id')
                    new_matched_id = match_result.get('matched_record_id')
                    
                    # å¦‚æœåŒ¹é…åˆ°ç›¸åŒçš„ç›®æ ‡è®°å½•ï¼Œåˆ™è·³è¿‡
                    if existing_matched_id == new_matched_id:
                        logger.info(f"å¢é‡æ¨¡å¼è·³è¿‡: {source_record.get('UNIT_NAME', 'Unknown')} (å·²å­˜åœ¨ç›¸åŒåŒ¹é…)")
                        return {
                            'operation': 'skipped',
                            'source_id': source_id,
                            'reason': 'identical_match_exists'
                        }
                    else:
                        # æ‰¾åˆ°äº†ä¸åŒçš„åŒ¹é…ç›®æ ‡ï¼Œæ›´æ–°ç°æœ‰è®°å½•
                        logger.info(f"å¢é‡æ¨¡å¼æ›´æ–°: {source_record.get('UNIT_NAME', 'Unknown')} (å‘ç°æ›´å¥½çš„åŒ¹é…)")
                        match_result['operation'] = 'updated'
                        match_result['previous_result_id'] = existing_result.get('_id')
                        return match_result
                else:
                    # ç°æœ‰è®°å½•æ˜¯æœªåŒ¹é…ï¼Œä½†ç°åœ¨æ‰¾åˆ°äº†åŒ¹é…
                    if match_result and match_result.get('match_status') == 'matched':
                        logger.info(f"å¢é‡æ¨¡å¼æ›´æ–°: {source_record.get('UNIT_NAME', 'Unknown')} (ä»æœªåŒ¹é…å˜ä¸ºåŒ¹é…)")
                        match_result['operation'] = 'updated'
                        match_result['previous_result_id'] = existing_result.get('_id')
                        return match_result
                    else:
                        # éƒ½æ˜¯æœªåŒ¹é…çŠ¶æ€ï¼Œè·³è¿‡
                        logger.info(f"å¢é‡æ¨¡å¼è·³è¿‡: {source_record.get('UNIT_NAME', 'Unknown')} (æœªåŒ¹é…çŠ¶æ€æœªå˜)")
                        return {
                            'operation': 'skipped',
                            'source_id': source_id,
                            'reason': 'no_change_needed'
                        }
            
            # è®¾ç½®æ“ä½œç±»å‹
            if existing_result:
                match_result['operation'] = 'updated'
                match_result['previous_result_id'] = existing_result.get('_id')
            else:
                match_result['operation'] = 'matched'
            
            return match_result
            
        except Exception as e:
            logger.error(f"å¤„ç†ä¼˜åŒ–å•æ¡è®°å½•å¤±è´¥: {str(e)}")
            return None
    
    def _format_optimized_match_result(self, match_result: Union[MatchResult, FuzzyMatchResult], 
                                     match_type: str, source_record: Dict) -> Dict:
        """æ ¼å¼åŒ–ä¼˜åŒ–çš„åŒ¹é…ç»“æœ - ä»¥å®‰å…¨æ’æŸ¥ç³»ç»Ÿä¸ºä¸»è®°å½•"""
        target_record = match_result.target_record
        # åŸºç¡€ä¿¡æ¯ - ä»¥å®‰å…¨æ’æŸ¥ç³»ç»Ÿï¼ˆsourceï¼‰ä¸ºä¸»è®°å½•
        result = {
            'primary_record_id': source_record.get('_id'),
            'primary_system': 'inspection',
            'unit_name': source_record.get('UNIT_NAME', ''),
            'unit_address': source_record.get('ADDRESS', ''),
            'unit_type': source_record.get('UNIT_TYPE', ''),
            'contact_person': source_record.get('LEGAL_PEOPLE', ''),
            'contact_phone': source_record.get('SECURITY_TEL', ''),
            'security_manager': source_record.get('SECURITY_PEOPLE', ''),
            'primary_security_manager': source_record.get('SECURITY_PEOPLE', ''),
            'legal_tel': source_record.get('SECURITY_TEL', ''),
            'matched_record_id': target_record.get('_id') if target_record else None,
            'matched_system': 'supervision',
            'matched_unit_name': target_record.get('dwmc', '') if target_record else '',
            'matched_unit_address': target_record.get('dwdz', '') if target_record else '',
            'matched_credit_code': target_record.get('tyshxydm', '') if target_record else '',
            'matched_legal_person': target_record.get('fddbr', '') if target_record else '',
            'matched_contact_phone': target_record.get('lxdh', '') if target_record else '',
            'matched_registration_date': target_record.get('djrq', '') if target_record else '',
            'matched_business_scope': target_record.get('jyfw', '') if target_record else '',
            'matched_security_manager': target_record.get('xfaqglr', '') if target_record else '',
            'match_type': self._determine_actual_match_type(match_result, match_type),
            'match_status': 'matched',
            'similarity_score': match_result.similarity_score if hasattr(match_result, 'similarity_score') else 0.0,
            'match_confidence': 'high' if match_type == 'exact' else 'medium',
            'match_fields': [],
            'match_details': {}
        }
        
        # å°è¯•è·å–explanation
        explanation = getattr(match_result, 'explanation', None)
        if explanation:
            result['match_details']['explanation'] = explanation
        
        # æ ¹æ®åŒ¹é…ç±»å‹æ·»åŠ è¯¦ç»†ä¿¡æ¯
        if match_type == 'exact':
            # æ£€æŸ¥å…³é”®ä¿¡æ¯æ˜¯å¦ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™æ ‡æ³¨æ•°æ®æ¥æº
            data_consistency = self._check_data_consistency(source_record, target_record)
            
            result.update({
                'similarity_score': match_result.confidence,
                'match_fields': ['credit_code'],  # ç²¾ç¡®åŒ¹é…åªåŸºäºä¿¡ç”¨ä»£ç 
                'match_details': {
                    'exact_match_fields': ['credit_code'],
                    'match_method': 'exact_credit_code',
                    'confidence_level': match_result.confidence,
                    'match_basis': 'ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç ',
                    'data_consistency': data_consistency
                }
            })
        elif match_type == 'fuzzy':
            field_similarities = match_result.field_similarities or {}
            matched_fields = []
            fuzzy_details = {}
            for field, similarity in field_similarities.items():
                # ç±»å‹å®‰å…¨åˆ¤æ–­
                sim_val = similarity
                if isinstance(sim_val, dict):
                    sim_val = sim_val.get('similarity', 0)
                if isinstance(sim_val, (int, float)) and sim_val > 0.5:
                    matched_fields.append(field)
                    fuzzy_details[f'{field}_similarity'] = sim_val
            
            # ç¡®ä¿similarity_scoreæœ‰å€¼
            actual_similarity = match_result.similarity_score
            if actual_similarity == 0 and hasattr(match_result, 'match_details'):
                # ä»match_detailsä¸­è·å–å®é™…ç›¸ä¼¼åº¦
                match_details = match_result.match_details or {}
                actual_similarity = match_details.get('overall_similarity', 0)
            
            # å¦‚æœæ˜¯fuzzy_perfectç±»å‹ä½†ç›¸ä¼¼åº¦ä¸º0ï¼Œè®¾ç½®ä¸º1.0
            actual_match_type = result['match_type']
            if actual_match_type == 'fuzzy_perfect' and actual_similarity == 0:
                actual_similarity = 1.0
            
            result.update({
                'similarity_score': actual_similarity,
                'match_fields': matched_fields,
                'match_details': {
                    'fuzzy_match_fields': matched_fields,
                    'field_similarities': field_similarities,
                    'overall_similarity': actual_similarity,
                    'threshold_used': match_result.match_details.get('threshold', 0.75) if match_result.match_details else 0.75,
                    'match_method': 'fuzzy',
                    **fuzzy_details
                }
            })
            if actual_similarity >= 0.9:
                result['match_confidence'] = 'high'
            elif actual_similarity >= 0.75:
                result['match_confidence'] = 'medium'
            else:
                result['match_confidence'] = 'low'
        
        # ç”ŸæˆåŒ¹é…ID
        result['match_id'] = generate_match_id(
            str(result['primary_record_id']), 
            str(result['matched_record_id'])
        )
        
        # æ·»åŠ å®¡æ ¸ä¿¡æ¯ - è®¾ç½®é»˜è®¤å€¼
        if match_type == 'exact':
            # ç²¾ç¡®åŒ¹é…é»˜è®¤æ— éœ€äººå·¥å®¡æ ¸
            result['review_status'] = 'pending'  # å¯ä»¥è®¾ä¸ºapprovedï¼Œä½†ä¸ºäº†ä¸€è‡´æ€§è®¾ä¸ºpending
        else:
            # æ¨¡ç³ŠåŒ¹é…éœ€è¦äººå·¥å®¡æ ¸
            result['review_status'] = 'pending'
        
        result['review_notes'] = ''
        result['reviewer'] = ''
        result['review_time'] = None
        
        # æ·»åŠ æ—¶é—´æˆ³
        current_time = datetime.now()
        result['created_time'] = current_time
        result['updated_time'] = current_time
        result['matching_time'] = current_time
        
        return result
    
    def _determine_actual_match_type(self, match_result: Union[MatchResult, FuzzyMatchResult], 
                                   declared_type: str) -> str:
        """
        ç¡®å®šå®é™…çš„åŒ¹é…ç±»å‹
        
        Args:
            match_result: åŒ¹é…ç»“æœå¯¹è±¡
            declared_type: å£°æ˜çš„åŒ¹é…ç±»å‹
            
        Returns:
            str: å®é™…çš„åŒ¹é…ç±»å‹
        """
        # å¦‚æœæ˜¯ç²¾ç¡®åŒ¹é…ç»“æœå¯¹è±¡
        if hasattr(match_result, 'match_type'):
            if match_result.match_type == 'credit_code':
                return 'exact_credit_code'
            elif match_result.match_type in ['fuzzy', 'fuzzy_match']:
                return 'fuzzy'
        
        # å¦‚æœæ˜¯æ¨¡ç³ŠåŒ¹é…ç»“æœå¯¹è±¡
        if hasattr(match_result, 'similarity_score'):
            similarity_score = match_result.similarity_score
            if similarity_score == 1.0:
                # ç›¸ä¼¼åº¦ä¸º1.0ï¼Œå¯èƒ½æ˜¯ç²¾ç¡®åŒ¹é…
                if declared_type == 'exact':
                    return 'exact'
                else:
                    return 'fuzzy_perfect'
            elif similarity_score >= 0.95:
                return 'fuzzy_perfect'  # é«˜ç›¸ä¼¼åº¦ä¹Ÿè®¤ä¸ºæ˜¯å®Œç¾åŒ¹é…
            else:
                return 'fuzzy'
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯æ¨¡ç³ŠåŒ¹é…ä½†æ²¡æœ‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥æ˜¯å®Œç¾åŒ¹é…
        if declared_type == 'fuzzy' and hasattr(match_result, 'match_details'):
            match_details = match_result.match_details or {}
            if match_details.get('overall_similarity', 0) >= 0.95:
                # ç¡®ä¿è®¾ç½®ç›¸ä¼¼åº¦åˆ†æ•°
                if hasattr(match_result, 'similarity_score'):
                    match_result.similarity_score = match_details.get('overall_similarity', 1.0)
                return 'fuzzy_perfect'
        
        # å›é€€åˆ°å£°æ˜çš„ç±»å‹
        return declared_type
    
    def _check_data_consistency(self, source_record, target_record):
        """æ£€æŸ¥ä¸¤æ¡è®°å½•çš„å…³é”®ä¿¡æ¯ä¸€è‡´æ€§"""
        consistency = {
            'unit_name': {'consistent': True, 'source': None, 'target': None, 'note': None},
            'legal_person': {'consistent': True, 'source': None, 'target': None, 'note': None},
            'address': {'consistent': True, 'source': None, 'target': None, 'note': None}
        }
        
        # æ£€æŸ¥å•ä½åç§°
        source_name = self._safe_str(source_record.get('UNIT_NAME'))
        target_name = self._safe_str(target_record.get('dwmc'))  # æ¶ˆé˜²ç›‘ç£ç³»ç»Ÿå­—æ®µ
        if source_name and target_name:
            consistency['unit_name']['source'] = f"{source_name} (å®‰å…¨æ’æŸ¥ç³»ç»Ÿ)"
            consistency['unit_name']['target'] = f"{target_name} (æ¶ˆé˜²ç›‘ç£ç³»ç»Ÿ)"
            if source_name != target_name:
                consistency['unit_name']['consistent'] = False
                consistency['unit_name']['note'] = 'å•ä½åç§°ä¸ä¸€è‡´ï¼Œè¯·æ ¸å®'
        
        # æ£€æŸ¥æ³•å®šä»£è¡¨äºº
        source_legal = self._safe_str(source_record.get('LEGAL_PEOPLE'))
        target_legal = self._safe_str(target_record.get('fddbr'))  # æ¶ˆé˜²ç›‘ç£ç³»ç»Ÿå­—æ®µ
        if source_legal and target_legal:
            consistency['legal_person']['source'] = f"{source_legal} (å®‰å…¨æ’æŸ¥ç³»ç»Ÿ)"
            consistency['legal_person']['target'] = f"{target_legal} (æ¶ˆé˜²ç›‘ç£ç³»ç»Ÿ)"
            if source_legal != target_legal:
                consistency['legal_person']['consistent'] = False
                consistency['legal_person']['note'] = 'æ³•å®šä»£è¡¨äººä¸ä¸€è‡´ï¼Œè¯·æ ¸å®'
        
        # æ£€æŸ¥åœ°å€
        source_address = self._safe_str(source_record.get('ADDRESS'))
        target_address = self._safe_str(target_record.get('dwdz'))  # æ¶ˆé˜²ç›‘ç£ç³»ç»Ÿå­—æ®µ
        if source_address and target_address:
            consistency['address']['source'] = f"{source_address} (å®‰å…¨æ’æŸ¥ç³»ç»Ÿ)"
            consistency['address']['target'] = f"{target_address} (æ¶ˆé˜²ç›‘ç£ç³»ç»Ÿ)"
            # ç®€å•çš„åœ°å€ç›¸ä¼¼æ€§æ£€æŸ¥
            if source_address != target_address and not self._addresses_similar(source_address, target_address):
                consistency['address']['consistent'] = False
                consistency['address']['note'] = 'åœ°å€ä¸ä¸€è‡´ï¼Œè¯·æ ¸å®'
        
        return consistency
    
    def _addresses_similar(self, addr1, addr2, threshold=0.7):
        """ç®€å•çš„åœ°å€ç›¸ä¼¼æ€§æ£€æŸ¥"""
        if not addr1 or not addr2:
            return False
        
        # æå–ä¸»è¦è¡—é“ä¿¡æ¯è¿›è¡Œæ¯”è¾ƒ
        import re
        
        # æå–è¡—é“ã€è·¯ã€å¼„ã€å·ç­‰å…³é”®ä¿¡æ¯
        pattern = r'([^åŒºå¿å¸‚]+(?:è¡—é“|è·¯|å¼„|å·|å··|é‡Œ|æ‘|é•‡))'
        addr1_parts = re.findall(pattern, addr1)
        addr2_parts = re.findall(pattern, addr2)
        
        if addr1_parts and addr2_parts:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒçš„è¡—é“ä¿¡æ¯
            for part1 in addr1_parts:
                for part2 in addr2_parts:
                    if part1 == part2:
                        return True
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°è¡—é“ä¿¡æ¯ï¼Œä½¿ç”¨å­—ç¬¦ç›¸ä¼¼åº¦
        from difflib import SequenceMatcher
        similarity = SequenceMatcher(None, addr1, addr2).ratio()
        return similarity >= threshold
    
    def _format_optimized_no_match_result(self, source_record: Dict) -> Dict:
        """æ ¼å¼åŒ–ä¼˜åŒ–çš„æ— åŒ¹é…ç»“æœ"""
        return {
            # ä¸»è®°å½•ä¿¡æ¯ï¼ˆå®‰å…¨æ’æŸ¥ç³»ç»Ÿï¼‰
            'primary_record_id': source_record.get('_id'),
            'primary_system': 'inspection',
            'unit_name': source_record.get('UNIT_NAME', ''),
            'unit_address': source_record.get('ADDRESS', ''),
            'unit_type': source_record.get('UNIT_TYPE', ''),
            'contact_person': source_record.get('LEGAL_PEOPLE', ''),
            'contact_phone': source_record.get('SECURITY_TEL', ''),
            
            # åŒ¹é…è®°å½•ä¿¡æ¯ï¼ˆç©ºï¼Œå› ä¸ºæ²¡æœ‰åŒ¹é…ï¼‰
            'matched_record_id': None,
            'matched_system': None,
            'matched_unit_name': '',
            'matched_unit_address': '',
            'matched_credit_code': '',
            'matched_legal_person': '',
            'matched_contact_phone': '',
            'matched_registration_date': '',
            'matched_business_scope': '',
            
            # åŒ¹é…ä¿¡æ¯
            'match_type': 'none',
            'match_status': 'unmatched',
            'similarity_score': 0.0,
            'match_confidence': 'none',
            'match_fields': [],
            'match_details': {
                'match_method': 'none',
                'reason': 'no_suitable_match_found'
            },
            
            # å®¡æ ¸ä¿¡æ¯
            'review_status': 'pending',
            'review_notes': 'æœªæ‰¾åˆ°åŒ¹é…ï¼Œéœ€è¦äººå·¥å®¡æ ¸',
            'reviewer': '',
            'review_time': None,
            
            # æ—¶é—´æˆ³
            'created_time': datetime.now(),
            'updated_time': datetime.now(),
            
            # åŒ¹é…ID
            'match_id': generate_match_id(str(source_record.get('_id')), 'no_match')
        }
    
    def _get_existing_match_result(self, source_id: str) -> Optional[Dict]:
        """è·å–ç°æœ‰çš„åŒ¹é…ç»“æœ"""
        try:
            collection = self.db_manager.get_collection('unit_match_results')
            
            # æŸ¥æ‰¾åŸºäºä¸»è®°å½•IDçš„åŒ¹é…ç»“æœ
            result = collection.find_one({'primary_record_id': source_id})
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–ç°æœ‰åŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
            return None
    
    def _batch_save_optimized_results(self, results: List[Dict]) -> bool:
        """æ‰¹é‡ä¿å­˜ä¼˜åŒ–çš„åŒ¹é…ç»“æœ - å¢å¼ºç‰ˆæœ¬"""
        if not results:
            logger.warning("æ²¡æœ‰ç»“æœéœ€è¦ä¿å­˜")
            return True
        
        try:
            import traceback
            from pymongo import ReplaceOne
            
            collection = self.db_manager.get_collection('unit_match_results')
            
            # é¢„å¤„ç†éªŒè¯
            operations = []
            valid_results = []
            skipped_count = 0
            
            for result in results:
                if result.get('operation') == 'skipped':
                    skipped_count += 1
                    continue
                
                primary_id = result.get('primary_record_id')
                if not primary_id:
                    logger.warning(f"è·³è¿‡æ— primary_record_idçš„è®°å½•: {result}")
                    continue
                
                # ç§»é™¤æ“ä½œæ ‡è¯†ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“
                result_to_save = {k: v for k, v in result.items() if k not in ['operation', 'previous_result_id']}
                
                # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                if 'created_time' not in result_to_save:
                    result_to_save['created_time'] = datetime.now()
                if 'updated_time' not in result_to_save:
                    result_to_save['updated_time'] = datetime.now()
                
                # æ•°æ®ç±»å‹å®‰å…¨å¤„ç†
                for key, value in result_to_save.items():
                    if key in ['created_time', 'updated_time', 'review_time'] and value is not None:
                        if not isinstance(value, datetime):
                            try:
                                if isinstance(value, str):
                                    result_to_save[key] = datetime.fromisoformat(value.replace('Z', '+00:00'))
                                else:
                                    result_to_save[key] = datetime.now()
                            except:
                                result_to_save[key] = datetime.now()
                    elif key == 'similarity_score' and value is not None:
                        try:
                            result_to_save[key] = float(value) if value != '' else 0.0
                        except:
                            result_to_save[key] = 0.0
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                required_fields = ['primary_record_id', 'unit_name', 'match_status']
                if all(field in result_to_save for field in required_fields):
                    valid_results.append(result_to_save)
                    
                    # ä½¿ç”¨æ­£ç¡®çš„PyMongoæ“ä½œæ ¼å¼
                    operations.append(
                        ReplaceOne(
                            filter={'primary_record_id': primary_id},
                            replacement=result_to_save,
                            upsert=True
                        )
                    )
                else:
                    logger.warning(f"è·³è¿‡ä¸å®Œæ•´çš„è®°å½•: {result_to_save}")
            
            # æ‰§è¡Œä¿å­˜æ“ä½œ
            if operations:
                logger.info(f"å‡†å¤‡æ‰¹é‡ä¿å­˜: æ€»è®¡={len(results)}, è·³è¿‡={skipped_count}, æœ‰æ•ˆ={len(operations)}")
                
                # è®°å½•ä¿å­˜å‰çš„æ•°æ®åº“çŠ¶æ€
                before_count = collection.count_documents({})
                
                # æ‰§è¡Œæ‰¹é‡æ“ä½œ
                bulk_result = collection.bulk_write(operations, ordered=False)
                
                # éªŒè¯ä¿å­˜ç»“æœ
                after_count = collection.count_documents({})
                actual_new_records = after_count - before_count
                
                logger.info(f"æ‰¹é‡ä¿å­˜ç»“æœè¯¦æƒ…:")
                logger.info(f"  - åŒ¹é…è®°å½•: {bulk_result.matched_count}")
                logger.info(f"  - ä¿®æ”¹è®°å½•: {bulk_result.modified_count}")
                logger.info(f"  - æ’å…¥è®°å½•: {bulk_result.upserted_count}")
                logger.info(f"  - æ•°æ®åº“å‰: {before_count} æ¡")
                logger.info(f"  - æ•°æ®åº“å: {after_count} æ¡")
                logger.info(f"  - å®é™…æ–°å¢: {actual_new_records} æ¡")
                
                # å¼ºåˆ¶éªŒè¯ä¿å­˜æˆåŠŸ
                if bulk_result.upserted_count > 0 or bulk_result.modified_count > 0:
                    # å†æ¬¡éªŒè¯æ•°æ®çœŸçš„ä¿å­˜äº†
                    saved_count = 0
                    for result in valid_results:
                        check_result = collection.find_one({'primary_record_id': result['primary_record_id']})
                        if check_result:
                            saved_count += 1
                    
                    logger.info(f"æ•°æ®ä¿å­˜éªŒè¯: {saved_count}/{len(valid_results)} æ¡è®°å½•ç¡®è®¤ä¿å­˜")
                    
                    if saved_count == len(valid_results):
                        logger.info("âœ… æ•°æ®ä¿å­˜æˆåŠŸéªŒè¯é€šè¿‡")
                        return True
                    else:
                        logger.error(f"âŒ æ•°æ®ä¿å­˜éªŒè¯å¤±è´¥: æœŸæœ›{len(valid_results)}æ¡ï¼Œå®é™…{saved_count}æ¡")
                        return False
                else:
                    logger.error("âŒ æ•°æ®ä¿å­˜éªŒè¯å¤±è´¥ï¼šæ²¡æœ‰è®°å½•è¢«æ’å…¥æˆ–ä¿®æ”¹")
                    return False
            else:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„è®°å½•éœ€è¦ä¿å­˜")
                return True
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜ä¼˜åŒ–åŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
            logger.error(f"è¯¦ç»†é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False
    
    
    def _force_final_save_check(self) -> int:
        """å¼ºåˆ¶æœ€ç»ˆä¿å­˜æ£€æŸ¥ - ç¡®ä¿åœæ­¢æ—¶æ•°æ®å®Œå…¨ä¿å­˜"""
        try:
            import traceback
            saved_count = 0
            
            logger.info("å¼€å§‹å¼ºåˆ¶æœ€ç»ˆä¿å­˜æ£€æŸ¥...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœªä¿å­˜çš„æ‰¹æ¬¡ç»“æœ
            # è¿™é‡Œå¯ä»¥æ£€æŸ¥å†…å­˜ä¸­æ˜¯å¦è¿˜æœ‰æœªä¿å­˜çš„æ•°æ®
            # ç”±äºæˆ‘ä»¬çš„è®¾è®¡æ˜¯æ¯ä¸ªæ‰¹æ¬¡éƒ½ç«‹å³ä¿å­˜ï¼Œè¿™é‡Œä¸»è¦æ˜¯éªŒè¯
            
            # éªŒè¯æ•°æ®åº“ä¸­çš„è®°å½•æ•°
            collection = self.db_manager.get_collection('unit_match_results')
            current_count = collection.count_documents({})
            
            logger.info(f"æ•°æ®åº“å½“å‰è®°å½•æ•°: {current_count}")
            
            # æ£€æŸ¥æœ€è¿‘çš„ä¿å­˜æ“ä½œæ˜¯å¦æˆåŠŸ
            recent_records = list(collection.find().sort("created_time", -1).limit(10))
            
            if recent_records:
                logger.info(f"æœ€è¿‘ä¿å­˜çš„è®°å½•: {len(recent_records)} æ¡")
                latest_record = recent_records[0]
                logger.info(f"æœ€æ–°è®°å½•æ—¶é—´: {latest_record.get('created_time')}")
                logger.info(f"æœ€æ–°è®°å½•å•ä½: {latest_record.get('unit_name')}")
            else:
                logger.warning("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…è®°å½•")
            
            saved_count = current_count
            logger.info(f"å¼ºåˆ¶æœ€ç»ˆä¿å­˜æ£€æŸ¥å®Œæˆ: ç¡®è®¤ä¿å­˜ {saved_count} æ¡è®°å½•")
            
            return saved_count
            
        except Exception as e:
            logger.error(f"å¼ºåˆ¶æœ€ç»ˆä¿å­˜æ£€æŸ¥å¤±è´¥: {str(e)}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return 0

    def _clear_match_results(self) -> bool:
        """æ¸…ç©ºåŒ¹é…ç»“æœ"""
        try:
            collection = self.db_manager.get_collection('unit_match_results')
            
            # åˆ é™¤æ‰€æœ‰åŒ¹é…ç»“æœ
            result = collection.delete_many({})
            
            logger.info(f"æ¸…ç©ºåŒ¹é…ç»“æœ: åˆ é™¤äº† {result.deleted_count} æ¡è®°å½•")
            
            return True
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºåŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
            return False
    
    def get_optimized_task_progress(self, task_id: str) -> Dict:
        """è·å–ä¼˜åŒ–ä»»åŠ¡è¿›åº¦"""
        with self.tasks_lock:
            progress = self.active_tasks.get(task_id)
            
            if not progress:
                return {
                    'error': 'ä»»åŠ¡ä¸å­˜åœ¨',
                    'task_id': task_id
                }
            
            return progress.get_progress()
    
    def stop_optimized_matching_task(self, task_id: str) -> bool:
        """åœæ­¢ä¼˜åŒ–åŒ¹é…ä»»åŠ¡ - å¢å¼ºç‰ˆæœ¬ï¼Œç¡®ä¿æ•°æ®ä¿å­˜"""
        try:
            with self.tasks_lock:
                progress = self.active_tasks.get(task_id)
                
                if not progress:
                    logger.warning(f"å°è¯•åœæ­¢ä¸å­˜åœ¨çš„ä»»åŠ¡: {task_id}")
                    return False
                
                if progress.status == "running":
                    # è®¾ç½®åœæ­¢çŠ¶æ€
                    progress.set_status("stopped")
                    logger.info(f"ä¼˜åŒ–åŒ¹é…ä»»åŠ¡åœæ­¢ä¿¡å·å·²å‘é€: {task_id}")
                    
                    # ç­‰å¾…å½“å‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼ˆæœ€å¤šç­‰å¾…30ç§’ï¼‰
                    import time
                    wait_count = 0
                    max_wait = 30
                    
                    while wait_count < max_wait:
                        # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨å¤„ç†ä¸­
                        current_progress = progress.get_progress()
                        if current_progress.get('status') == 'stopped':
                            break
                        
                        time.sleep(1)
                        wait_count += 1
                        
                        if wait_count % 5 == 0:  # æ¯5ç§’è¾“å‡ºä¸€æ¬¡ç­‰å¾…ä¿¡æ¯
                            logger.info(f"ç­‰å¾…ä»»åŠ¡ {task_id} å®Œæˆå½“å‰æ‰¹æ¬¡å¤„ç†... ({wait_count}s)")
                    
                    # å¼ºåˆ¶æ‰§è¡Œæœ€ç»ˆæ•°æ®ä¿å­˜æ£€æŸ¥
                    logger.info("æ‰§è¡Œåœæ­¢æ—¶çš„æœ€ç»ˆæ•°æ®ä¿å­˜æ£€æŸ¥...")
                    final_save_count = self._force_final_save_check()
                    logger.info(f"åœæ­¢æ—¶æœ€ç»ˆä¿å­˜æ£€æŸ¥å®Œæˆ: ç¡®è®¤ {final_save_count} æ¡è®°å½•å·²ä¿å­˜")
                    
                    if wait_count >= max_wait:
                        logger.warning(f"ä»»åŠ¡ {task_id} åœæ­¢è¶…æ—¶ï¼Œä½†æ•°æ®ä¿å­˜æ£€æŸ¥å·²å®Œæˆ")
                    else:
                        logger.info(f"ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å·²å®Œå…¨åœæ­¢: {task_id}")
                    
                    return True
                elif progress.status in ["completed", "error", "stopped"]:
                    logger.info(f"ä»»åŠ¡ {task_id} å·²å¤„äº {progress.status} çŠ¶æ€")
                    # å³ä½¿ä»»åŠ¡å·²åœæ­¢ï¼Œä¹Ÿæ‰§è¡Œä¸€æ¬¡æ•°æ®ä¿å­˜æ£€æŸ¥
                    final_save_count = self._force_final_save_check()
                    logger.info(f"çŠ¶æ€æ£€æŸ¥æ—¶çš„æ•°æ®ä¿å­˜éªŒè¯: ç¡®è®¤ {final_save_count} æ¡è®°å½•å·²ä¿å­˜")
                    return True
                else:
                    logger.warning(f"ä»»åŠ¡ {task_id} çŠ¶æ€å¼‚å¸¸: {progress.status}")
                    return False
                    
        except Exception as e:
            logger.error(f"åœæ­¢ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
            return False
    
    def get_optimized_matching_statistics(self) -> Dict:
        """è·å–ä¼˜åŒ–åŒ¹é…ç»Ÿè®¡ä¿¡æ¯"""
        try:
            collection = self.db_manager.get_collection('unit_match_results')
            
            # åŸºç¡€ç»Ÿè®¡
            total_results = collection.count_documents({})
            matched_results = collection.count_documents({'match_status': 'matched'})
            unmatched_results = collection.count_documents({'match_status': 'unmatched'})
            
            # æŒ‰åŒ¹é…ç±»å‹ç»Ÿè®¡
            match_type_stats = list(collection.aggregate([
                {
                    '$group': {
                        '_id': '$match_type',
                        'count': {'$sum': 1},
                        'avg_similarity': {'$avg': '$similarity_score'}
                    }
                }
            ]))
            
            # æŒ‰ç½®ä¿¡åº¦ç»Ÿè®¡
            confidence_stats = list(collection.aggregate([
                {
                    '$group': {
                        '_id': '$match_confidence',
                        'count': {'$sum': 1}
                    }
                }
            ]))
            
            # å®¡æ ¸çŠ¶æ€ç»Ÿè®¡
            review_stats = list(collection.aggregate([
                {
                    '$group': {
                        '_id': '$review_status',
                        'count': {'$sum': 1}
                    }
                }
            ]))
            
            # ä»»åŠ¡ç»Ÿè®¡
            with self.tasks_lock:
                task_stats = {
                    'active_tasks': len([p for p in self.active_tasks.values() if p.status == 'running']),
                    'completed_tasks': len([p for p in self.active_tasks.values() if p.status == 'completed']),
                    'total_tasks': len(self.active_tasks)
                }
            
            return {
                'total_results': total_results,
                'matched_results': matched_results,
                'unmatched_results': unmatched_results,
                'match_rate': round((matched_results / total_results * 100) if total_results > 0 else 0, 2),
                'match_type_stats': match_type_stats,
                'confidence_stats': confidence_stats,
                'review_stats': review_stats,
                'task_stats': task_stats,
                'last_updated': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"è·å–ä¼˜åŒ–åŒ¹é…ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {
                'total_results': 0,
                'matched_results': 0,
                'unmatched_results': 0,
                'match_rate': 0,
                'match_type_stats': [],
                'confidence_stats': [],
                'review_stats': [],
                'task_stats': {},
                'error': str(e)
            }
    
    def cleanup_optimized_completed_tasks(self, max_age_hours: int = 24):
        """æ¸…ç†å·²å®Œæˆçš„ä¼˜åŒ–ä»»åŠ¡"""
        try:
            current_time = datetime.now()
            to_remove = []
            
            with self.tasks_lock:
                for task_id, progress in self.active_tasks.items():
                    if progress.status in ['completed', 'error', 'stopped']:
                        elapsed_hours = (current_time - progress.start_time).total_seconds() / 3600
                        if elapsed_hours > max_age_hours:
                            to_remove.append(task_id)
                
                for task_id in to_remove:
                    del self.active_tasks[task_id]
                    logger.info(f"æ¸…ç†ä¼˜åŒ–ä»»åŠ¡: {task_id}")
                    
        except Exception as e:
            logger.error(f"æ¸…ç†ä¼˜åŒ–ä»»åŠ¡å¤±è´¥: {str(e)}")
    
    def get_match_result_details(self, match_id: str) -> Optional[Dict]:
        """è·å–åŒ¹é…ç»“æœè¯¦æƒ…"""
        try:
            collection = self.db_manager.get_collection('unit_match_results')
            
            result = collection.find_one({'match_id': match_id})
            
            if result:
                # è½¬æ¢ObjectIdä¸ºå­—ç¬¦ä¸²
                from src.utils.helpers import convert_objectid_to_str
                return convert_objectid_to_str(result)
            
            return None
            
        except Exception as e:
            logger.error(f"è·å–åŒ¹é…ç»“æœè¯¦æƒ…å¤±è´¥: {str(e)}")
            return None
    
    def update_review_status(self, match_id: str, review_status: str, 
                           review_notes: str = '', reviewer: str = '') -> bool:
        """æ›´æ–°å®¡æ ¸çŠ¶æ€ - æ”¯æŒé€šè¿‡_idæˆ–match_idæŸ¥è¯¢"""
        try:
            from bson import ObjectId
            collection = self.db_manager.get_collection('unit_match_results')
            
            update_data = {
                'review_status': review_status,
                'review_notes': review_notes,
                'reviewer': reviewer,
                'review_time': datetime.now(),
                'updated_time': datetime.now()
            }
            
            # å°è¯•é€šè¿‡ä¸åŒçš„å­—æ®µè¿›è¡ŒæŸ¥è¯¢
            result = None
            
            # é¦–å…ˆå°è¯•é€šè¿‡ObjectIdæŸ¥è¯¢ï¼ˆå‰ç«¯ä¼ é€’çš„_idï¼‰
            try:
                if ObjectId.is_valid(match_id):
                    result = collection.update_one(
                        {'_id': ObjectId(match_id)},
                        {'$set': update_data}
                    )
                    if result.modified_count > 0:
                        logger.info(f"é€šè¿‡_idæ›´æ–°å®¡æ ¸çŠ¶æ€æˆåŠŸ: {match_id} -> {review_status}")
                        return True
            except Exception as e:
                logger.debug(f"é€šè¿‡_idæŸ¥è¯¢å¤±è´¥: {str(e)}")
            
            # å¦‚æœé€šè¿‡_idæŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•é€šè¿‡match_idå­—æ®µæŸ¥è¯¢
            if not result or result.modified_count == 0:
                result = collection.update_one(
                    {'match_id': match_id},
                    {'$set': update_data}
                )
                if result.modified_count > 0:
                    logger.info(f"é€šè¿‡match_idæ›´æ–°å®¡æ ¸çŠ¶æ€æˆåŠŸ: {match_id} -> {review_status}")
                    return True
            
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå°è¯•é€šè¿‡primary_record_idæŸ¥è¯¢
            if not result or result.modified_count == 0:
                result = collection.update_one(
                    {'primary_record_id': match_id},
                    {'$set': update_data}
                )
                if result.modified_count > 0:
                    logger.info(f"é€šè¿‡primary_record_idæ›´æ–°å®¡æ ¸çŠ¶æ€æˆåŠŸ: {match_id} -> {review_status}")
                    return True
            
            logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…è®°å½•è¿›è¡Œå®¡æ ¸çŠ¶æ€æ›´æ–°: {match_id}")
            return False
                
        except Exception as e:
            logger.error(f"æ›´æ–°å®¡æ ¸çŠ¶æ€å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False

    def _batch_save_optimized_results_enhanced(self, results: List[Dict]) -> bool:
        """å¢å¼ºçš„æ‰¹é‡ä¿å­˜æ–¹æ³• - ç¡®ä¿æ•°æ®å®Œæ•´æ€§"""
        if not results:
            logger.warning("âš ï¸ æ²¡æœ‰ç»“æœéœ€è¦ä¿å­˜")
            return True
            
        try:
            logger.info(f"ğŸ’¾ å¼€å§‹å¢å¼ºæ‰¹é‡ä¿å­˜: {len(results)} æ¡è®°å½•")
            
            # æ•°æ®å®Œæ•´æ€§éªŒè¯
            validated_results = []
            for result in results:
                # å¿…è¦å­—æ®µæ£€æŸ¥
                if not result.get('primary_record_id'):
                    logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆè®°å½•: ç¼ºå°‘primary_record_id")
                    continue
                    
                # æ•°æ®ç±»å‹å®‰å…¨å¤„ç†
                safe_result = {}
                for key, value in result.items():
                    if value is not None:
                        # æ—¶é—´å­—æ®µæ ‡å‡†åŒ–
                        if 'time' in key and hasattr(value, 'isoformat'):
                            safe_result[key] = value
                        elif isinstance(value, (str, int, float, bool, dict, list)):
                            safe_result[key] = value
                        else:
                            safe_result[key] = str(value)
                    else:
                        safe_result[key] = None
                        
                validated_results.append(safe_result)
            
            if not validated_results:
                logger.warning("âš ï¸ æ‰€æœ‰è®°å½•éªŒè¯å¤±è´¥ï¼Œæ²¡æœ‰æœ‰æ•ˆæ•°æ®ä¿å­˜")
                return False
                
            logger.info(f"âœ… æ•°æ®éªŒè¯å®Œæˆ: {len(validated_results)} æ¡æœ‰æ•ˆè®°å½•")
            
            # åŒé‡ä¿å­˜éªŒè¯
            collection = self.db_manager.get_collection("unit_match_results")
            
            # ç¬¬ä¸€æ¬¡ä¿å­˜
            insert_result = collection.insert_many(validated_results)
            inserted_count = len(insert_result.inserted_ids)
            
            logger.info(f"ğŸ’¾ ç¬¬ä¸€æ¬¡ä¿å­˜å®Œæˆ: {inserted_count} æ¡è®°å½•")
            
            # éªŒè¯ä¿å­˜ç»“æœ
            if inserted_count != len(validated_results):
                logger.error(f"âŒ ä¿å­˜æ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(validated_results)}, å®é™… {inserted_count}")
                return False
            
            # æ•°æ®åº“è®°å½•æ•°éªŒè¯
            total_count = collection.count_documents({})
            logger.info(f"ğŸ“Š æ•°æ®åº“æ€»è®°å½•æ•°: {total_count}")
            
            # éªŒè¯æœ€æ–°è®°å½•
            latest_records = list(collection.find().sort("_id", -1).limit(3))
            logger.info(f"ğŸ” æœ€æ–°è®°å½•éªŒè¯: æ‰¾åˆ° {len(latest_records)} æ¡æœ€æ–°è®°å½•")
            
            for i, record in enumerate(latest_records):
                unit_name = record.get('unit_name', 'Unknown')
                match_status = record.get('match_status', 'Unknown')
                logger.info(f"   è®°å½• {i+1}: {unit_name} - {match_status}")
            
            logger.info(f"âœ… å¢å¼ºæ‰¹é‡ä¿å­˜æˆåŠŸ: {inserted_count} æ¡è®°å½•å·²ç¡®è®¤ä¿å­˜")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºæ‰¹é‡ä¿å­˜å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False


    def stop_optimized_matching_task_enhanced(self, task_id: str) -> bool:
        """å¢å¼ºçš„åœæ­¢åŒ¹é…ä»»åŠ¡æ–¹æ³•"""
        try:
            progress = self.active_tasks.get(task_id)
            if not progress:
                logger.warning(f"ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²å®Œæˆ: {task_id}")
                return False
            
            logger.info(f"ğŸ›‘ å¼€å§‹åœæ­¢ä¼˜åŒ–åŒ¹é…ä»»åŠ¡: {task_id}")
            
            # è®¾ç½®åœæ­¢çŠ¶æ€
            progress.set_status("stopping")
            logger.info("ğŸ“‹ ä»»åŠ¡çŠ¶æ€è®¾ç½®ä¸ºåœæ­¢ä¸­...")
            
            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            max_wait = 30  # æœ€å¤šç­‰å¾…30ç§’
            wait_count = 0
            
            while wait_count < max_wait:
                if progress.status == "stopped":
                    logger.info("âœ… ä»»åŠ¡å·²è‡ªç„¶åœæ­¢")
                    break
                    
                time.sleep(1)
                wait_count += 1
                
                if wait_count % 5 == 0:
                    logger.info(f"â³ ç­‰å¾…ä»»åŠ¡åœæ­¢ä¸­... ({wait_count}s)")
            
            # å¼ºåˆ¶åœæ­¢
            if progress.status != "stopped":
                progress.set_status("stopped")
                logger.info("ğŸ”¨ å¼ºåˆ¶è®¾ç½®ä»»åŠ¡ä¸ºå·²åœæ­¢çŠ¶æ€")
            
            # æ‰§è¡Œåœæ­¢æ—¶çš„æœ€ç»ˆæ•°æ®ä¿å­˜æ£€æŸ¥
            logger.info("ğŸ” æ‰§è¡Œåœæ­¢æ—¶çš„æœ€ç»ˆæ•°æ®ä¿å­˜æ£€æŸ¥...")
            saved_count = self._force_final_save_check_enhanced()
            
            logger.info(f"âœ… åœæ­¢æ—¶æœ€ç»ˆä¿å­˜æ£€æŸ¥å®Œæˆï¼Œç¡®ä¿ä¿å­˜ {saved_count} æ¡è®°å½•")
            
            # æ¸…ç†ä»»åŠ¡
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
                logger.info(f"ğŸ§¹ ä»»åŠ¡å·²ä»æ´»åŠ¨åˆ—è¡¨ä¸­ç§»é™¤: {task_id}")
            
            logger.info(f"ğŸ‰ ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å·²å®Œå…¨åœæ­¢: {task_id}")
            return True
            
        except Exception as e:
            logger.error(f"åœæ­¢ä¼˜åŒ–åŒ¹é…ä»»åŠ¡å¤±è´¥: {str(e)}")
            return False


    def _force_final_save_check_enhanced(self) -> int:
        """å¢å¼ºçš„å¼ºåˆ¶æœ€ç»ˆä¿å­˜æ£€æŸ¥"""
        try:
            logger.info("ğŸ” å¼€å§‹å¢å¼ºå¼ºåˆ¶æœ€ç»ˆä¿å­˜æ£€æŸ¥...")
            
            collection = self.db_manager.get_collection("unit_match_results")
            current_count = collection.count_documents({})
            
            logger.info(f"ğŸ“Š æ•°æ®åº“å½“å‰è®°å½•æ•°: {current_count}")
            
            # è·å–æœ€è¿‘ä¿å­˜çš„è®°å½•è¿›è¡ŒéªŒè¯
            recent_records = list(collection.find().sort("_id", -1).limit(10))
            logger.info(f"ğŸ” æœ€è¿‘ä¿å­˜çš„è®°å½•: {len(recent_records)} æ¡")
            
            if recent_records:
                latest_record = recent_records[0]
                latest_time = latest_record.get('created_time', 'Unknown')
                latest_unit = latest_record.get('unit_name', 'Unknown')
                
                logger.info(f"ğŸ“… æœ€æ–°è®°å½•æ—¶é—´: {latest_time}")
                logger.info(f"ğŸ¢ æœ€æ–°è®°å½•å•ä½: {latest_unit}")
            
            logger.info(f"âœ… å¢å¼ºå¼ºåˆ¶æœ€ç»ˆä¿å­˜æ£€æŸ¥å®Œæˆï¼Œç¡®ä¿ä¿å­˜ {current_count} æ¡è®°å½•")
            return current_count
            
        except Exception as e:
            logger.error(f"å¢å¼ºå¼ºåˆ¶æœ€ç»ˆä¿å­˜æ£€æŸ¥å¤±è´¥: {str(e)}")
            return 0
