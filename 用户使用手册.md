# 数据研究与关联分析系统 - 用户使用手册

## 一、系统概述

本系统是一个面向数据研究的高级工具，旨在解决异构数据源之间的实体匹配与关联挖掘问题。系统核心能力包括：

1.  **智能实体匹配**：通过多层次、可配置的匹配策略（精确、结构化、模糊），高效地识别并关联来自不同数据源的同一实体（如企业）。
2.  **深度关联分析**：在完成初步匹配后，系统能进一步挖掘实体间的深层关系，例如通过共同的地址、法人、联系方式等信息，构建实体关联网络。
3.  **人机协同审核**：提供交互式界面，支持用户对机器匹配结果进行人工审核、修正与确认，形成高质量的数据资产。

本系统采用Flask作为后端框架，MongoDB作为数据存储，并提供了友好的Web用户界面。

## 二、快速上手

### 1. 环境要求

- Python 3.8+
- MongoDB 4.x+
- Git

### 2. 安装与配置

```bash
# 1. 克隆项目
git clone <your-repository-url>
cd Data_Research_Sys

# 2. 安装依赖
pip install -r requirements.txt

# 3. 配置数据库
#    请打开 config/database.yaml 文件，根据您的MongoDB实例信息修改 host, port, username, 和 password。
#    默认连接本地无密码的MongoDB实例。
```

### 3. 启动系统

在Windows环境下，项目根目录提供了 `start_system.bat` 批处理文件，它会自动设置必要的环境变量并启动Web服务。

```bash
# 双击或在命令行中运行
.\start_system.bat
```

服务启动后，默认监听地址为 `http://127.0.0.1:8888`。

### 4. 访问系统

打开您的浏览器，访问 [http://127.0.0.1:8888](http://127.0.0.1:8888) 即可看到系统主页。

## 三、核心功能详解

### 1. 优化匹配 (Optimized Matching)

此功能是系统的核心，用于在"待匹配数据源"和"目标数据库"之间建立精准、高效的关联。

#### 1.1. 核心业务逻辑与分层匹配策略

为了在海量数据中同时保证**匹配效率**和**结果准确性**，系统摒弃了单一的匹配算法，采用了一套精心设计的**分层瀑布式匹配策略 (Layered Waterfall Strategy)**。该策略从高精度、低成本的匹配方式开始，层层递进，逐步放宽条件，将计算量最大的模糊匹配留到最后，并尽可能缩小其处理范围。

![匹配策略流程图](https://mermaid.ink/img/pako:eNqNVMtqwzAQ_JdC-gCVhxZLD4GQEHol9NBLCZe1jSxJtokklP_eSUvbwzZLLs2OzuxsRz5ahjZwRmvQhS8YhXUj9l3KOFdE4L2tZl29tH2vTzMh2gWqgB-I1zFSC55N9sW8lRjAWM47r1X0bM5hV9V5nK-77eW3bA32jXg_L4W7aWnQ7zZ7IqXk-Vb8f5K1m2D-B55-wE4n8zLp-v41X4f7fX_eUuY8hB_EaE9XG7X4Qj6k3A-Q01zU3e0Wp3u_Tq55a01yJjYl6wVqO40R2kU13R9m5W3eC2Ea1T4h17E7tQyCQUwWk_hJ8h83sRjW9eO8q-i9r_oGgHqXp-X9l45z2b-x8FqHj8C581lQyUjJ4t7QfJ_x-2k9v6Q5rK39-pYf9l1gY9p3S_g6J_y5l94Xb3Jk71V5G4b6a-mJvTzL2Xlq5Q_Uu1c6rV75g1Iu1UqM4aD9B101G3X2Z1bLd2Ff1QjH4n239jV8b9W_8b1S364mR3l2v-41T5T?type=png)
*（上述为匹配策略的简化流程图）*

---

**第一层：确定性匹配 (Deterministic Matching)**

-   **算法说明**: 基于`统一社会信用代码 (credit_code)`进行**哈希精确匹配**。这是最严格、最可靠的匹配方式。
-   **业务逻辑**: 如果源数据和目标数据中存在完全相同的信用代码，系统就认为它们指向同一实体。这是国家赋予市场主体的唯一合法身份代码，具有极高的权威性。
-   **匹配结果**: `match_type` 标记为 `exact`，`similarity` (相似度) 恒为 `1.0`。一旦在此层匹配成功，该记录的匹配流程即告结束，不再进入后续层级。

---

**第二层：结构化名称匹配 (Structured Name Matching)**

-   **算法说明**: 当第一层匹配失败后，系统将对`单位名称 (unit_name)`进行**标准化和归一化处理**。处理步骤包括：
    1.  去除行政区划词缀（如"上海市"、"金山区"）。
    2.  去除行业或组织形式词缀（如"有限公司"、"合作社"）。
    3.  转换全角字符为半角，统一大小写。
    处理完成后，对标准化之后的名称进行精确匹配。
-   **业务逻辑**: 解决因注册习惯、数据录入不规范等原因造成的名称表面不一致，但实质指向同一实体的问题。例如，"上海ABC科技有限公司" 与 "ABC科技" 在此层可以被成功匹配。
-   **匹配结果**: `match_type` 标记为 `structured`，`similarity` 恒为 `1.0`。匹配成功后流程结束。

---

**第三层：预过滤增强模糊匹配 (Prefiltered Enhanced Fuzzy Matching)**

-   **算法说明**: 这是本系统效率与效果平衡的关键。它由两步构成：
    1.  **预过滤 (Prefiltering)**: 首先，使用一个计算开销远小于模糊匹配、但仍有较好区分度的字段——`法人代表 (legal_representative)`——进行精确匹配，从庞大的目标数据库中筛选出一个极小的"候选集"。
    2.  **增强模糊匹配 (Enhanced Fuzzy Matching)**: 然后，仅针对这个"候选集"，使用基于**编辑距离**的模糊匹配算法（由`rapidfuzz`库提供支持）进行精细比对。
-   **业务逻辑**: 避免了在全量目标数据上直接运行高成本的模糊匹配，从而将计算量减少数个数量级。其逻辑基础是：如果两家公司名称相似，其法人代表也相同的概率会非常高。这极大地提升了大规模数据处理的可行性。
-   **匹配结果**: `match_type` 标记为 `enhanced_fuzzy`，`similarity` 为 `rapidfuzz` 库计算出的 0.0-1.0 之间的浮点数。

---

**第四层：全局模糊匹配 (Global Fuzzy Matching)**

-   **算法说明**: 作为兜底策略，对前三层都未能匹配的"硬骨头"，系统会执行更大范围的模糊匹配。
-   **业务逻辑**: 尽最大努力为每一条记录寻找可能的关联，确保匹配的召回率。此层级的匹配结果通常需要更多的人工审核介入。
-   **匹配结果**: `match_type` 标记为 `fuzzy`，`similarity` 为 `rapidfuzz` 库计算出的 0.0-1.0 之间的浮点数。


#### 1.2. 匹配得分 (Similarity) 解释

`similarity` 字段是衡量匹配结果可信度的核心指标。其含义与 `match_type` 密切相关：

| `match_type` | `similarity` (相似度) | 解释 | 建议操作 |
| :--- | :--- | :--- | :--- |
| `exact` | `1.0` (固定) | **完全可信**。基于唯一ID的确定性匹配。 | 自动接受 |
| `structured`| `1.0` (固定) | **高度可信**。基于标准化名称的精确匹配。 | 自动接受 |
| `enhanced_fuzzy` | `0.9` ~ `1.0` | **非常可靠**。名称极度相似，且有法人加持。 | 建议抽样审核 |
| `enhanced_fuzzy` | `0.7` ~ `0.9` | **比较可靠**。可能是简称、别名或轻微录入错误。 | **建议人工审核** |
| `fuzzy` | `> 0.8` | **比较可靠**。名称本身高度相似。 | **建议人工审核** |
| `fuzzy` / `enhanced_fuzzy`| `< 0.7` | **可信度低**。可能只是偶然的文本相似，是无关实体的概率很高。 | 系统通常会设置阈值过滤掉，若保留则**必须人工审核** |

> **核心原则**: `similarity` 分数越高，匹配的可信度越高。但任何非`1.0`分的匹配，尤其是`fuzzy`类型，都应被视为"疑似匹配"，人工审核是保证最终数据质量不可或缺的一环。


### 2. 增强关联 (Enhanced Association)

此功能在"优化匹配"的基础上，进一步挖掘已匹配实体与其他业务数据（如：监督检查记录）之间的深层次联系。

-   **业务逻辑**:
    该功能的核心是后台一个强大的 **MongoDB 聚合管道 (Aggregation Pipeline)**。它代替了旧版客户端循环查询的方式，将所有复杂的计算（分组、关联、统计）全部在数据库服务端完成，极大地提升了性能并从根本上解决了旧版因内存溢出导致的崩溃问题。
    聚合管道会基于已匹配上的单位，通过`单位名称`、`地址`、`法人`等关键字段，去关联`监督检查`等其他集合，最终将关联结果（如每个单位关联到了多少条检查记录、数据质量评分等）输出到一个新的集合 `enhanced_association_results`。

-   **使用方法**:
    1.  从首页进入"增强关联分析"页面。
    2.  点击 **"开始增强关联"** 按钮。
    3.  后台任务启动，完成后即可在结果页面查看分析报告。

### 3. 结果浏览与审核

-   **结果查看**: 在"匹配结果"页面，可以分页浏览所有匹配记录。
-   **模糊搜索**: 页面顶部的搜索框支持按 **源单位名称** 或 **匹配到的单位名称** 进行模糊搜索，方便快速定位。
-   **人工审核**:
    -   每一条记录后面都有"审核"按钮。
    -   点击后会弹出模态框，展示匹配详情。
    -   您可以根据详情，选择 **"审核通过"** 或 **"审核未通过"**。
    -   审核状态会实时更新并保存在数据库中。

## 四、数据库字典

系统主要使用以下几个MongoDB集合：

### 1. `source_data` (待匹配数据源)

存储需要进行匹配的源数据。

| 字段名 | 类型 | 中文说明 | 备注 |
| :--- | :--- | :--- | :--- |
| `_id` | ObjectId | 唯一标识符 | |
| `source_id`| String | 源数据ID | **注意**：此字段已从数字类型迁移为字符串，以防精度丢失。 |
| `name` | String | 单位名称 | |
| `credit_code`| String | 统一社会信用代码 | 可能为空 |
| `address` | String | 地址 | |
| `legal_representative` | String | 法人代表 | |
| `business_scope` | String | 经营范围 | |
| ... | ... | 其他业务字段 | |

### 2. `target_data` (目标数据库)

用于匹配的标准数据库。

| 字段名 | 类型 | 中文说明 | 备注 |
| :--- | :--- | :--- | :--- |
| `_id` | ObjectId | 唯一标识符 | |
| `DWMC` | String | 单位名称 | |
| `TYSHXYDM` | String | 统一社会信用代码 | |
| `FDDBR` | String | 法定代表人 | |
| `LXDZ` | String | 联系地址 | |
| `QYZZCH` | String | 企业注册号 | |
| ... | ... | 其他业务字段 | |


### 3. `match_results` (匹配结果)

存储"优化匹配"功能产生的核心结果。

| 字段名 | 类型 | 中文说明 | 备注 |
| :--- | :--- | :--- | :--- |
| `_id` | ObjectId | 唯一标识符 | |
| `source_id` | String | 关联的 `source_data` 的 `source_id` | |
| `unit_name` | String | 源单位名称 (来自 `source_data`) | |
| `matched_unit_name` | String | 匹配到的单位名称 (来自 `target_data`) | |
| `matched_credit_code`| String | 匹配到的信用代码 | |
| `primary_credit_code`| String | 源单位的信用代码 | |
| `match_type` | String | 匹配类型 | `exact`, `structured`, `fuzzy`, `enhanced_fuzzy` |
| `similarity` | Float | 相似度得分 | 0.0 ~ 1.0 |
| `status` | String | 人工审核状态 | `pending`, `approved`, `rejected` |
| `timestamp` | Datetime | 匹配发生时间 | |
| `match_details` | Object | 匹配详情 | 包含匹配过程中的一些额外信息 |

### 4. `enhanced_association_results` (增强关联结果)

存储"增强关联"功能产生的分析结果。

| 字段名 | 类型 | 中文说明 | 备注 |
| :--- | :--- | :--- | :--- |
| `_id` | ObjectId | 唯一标识符 | |
| `source_unit_name` | String | 源单位名称 | |
| `matched_credit_code`| String | 匹配到的单位的信用代码 | |
| `associated_records_count` | Integer | 关联到的记录总数 | 如关联到的监督检查次数 |
| `last_association_date` | Date | 最近一次关联记录的日期 | |
| `data_quality_score` | Float | 数据质量分 | 基于关联记录数量、新旧程度等计算的综合得分 |
| `association_details`| Array | 关联详情列表 | 包含关联到的具体记录摘要 |

## 五、系统维护

### 1. 查看日志

系统运行日志位于项目根目录下的 `logs/` 文件夹中。主要日志文件为 `app.log`，其中记录了详细的匹配过程、API调用和潜在错误信息。

### 2. 清理数据

项目 `scripts/` 目录下提供了一些维护脚本，例如：

-   `clear_match_results.py`: 用于清空 `match_results` 和 `enhanced_association_results` 集合，方便重新进行测试。

### 3. 创建索引

`scripts/create_indexes.py` 脚本可以为数据库中的集合创建必要的索引，这对于保证查询性能至关重要。在新部署或数据迁移后，建议运行此脚本。

```bash
python scripts/create_indexes.py
``` 