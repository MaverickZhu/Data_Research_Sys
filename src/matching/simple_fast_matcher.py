#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•å¿«é€ŸåŒ¹é…å™¨ - ä¸“ä¸ºé€Ÿåº¦ä¼˜åŒ–
åŸºäº188ä¸‡æ•°æ®æˆåŠŸç»éªŒï¼Œä½¿ç”¨æœ€ç®€å•ä½†æœ€å¿«çš„ç®—æ³•
"""

import time
import logging
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)

class SimpleFastMatcher:
    """ç®€å•å¿«é€ŸåŒ¹é…å™¨ - ä¼˜å…ˆé€Ÿåº¦"""
    
    def __init__(self, db_manager):
        self.db_manager = db_manager
        self.stats = {
            'total_queries': 0,
            'total_matches': 0,
            'avg_query_time': 0.0
        }
    
    def batch_match(self, source_records: List[Dict], mappings: List[Dict], 
                   source_table: str, task_id: str) -> List[Dict]:
        """æ‰¹é‡åŒ¹é… - è¶…é«˜é€Ÿç‰ˆæœ¬"""
        start_time = time.time()
        batch_size = len(source_records)
        
        logger.info(f"ğŸš€ ç®€å•å¿«é€ŸåŒ¹é…å¼€å§‹: {batch_size} æ¡è®°å½•")
        
        # è·å–ç›®æ ‡è¡¨å’Œå­—æ®µæ˜ å°„
        target_tables = {}
        for mapping in mappings:
            target_table = mapping['target_table']
            if target_table not in target_tables:
                target_tables[target_table] = []
            target_tables[target_table].append(mapping)
        
        all_results = []
        
        # å¯¹æ¯ä¸ªç›®æ ‡è¡¨è¿›è¡ŒåŒ¹é…
        for target_table, table_mappings in target_tables.items():
            logger.info(f"ğŸ“Š åŒ¹é…ç›®æ ‡è¡¨: {target_table}")
            
            # è·å–ç›®æ ‡æ•°æ®ï¼ˆä¸€æ¬¡æ€§åŠ è½½ï¼Œé¿å…é‡å¤æŸ¥è¯¢ï¼‰
            target_collection = self.db_manager.get_collection(target_table)
            target_records = list(target_collection.find({}).limit(100000))  # é™åˆ¶æ•°é‡é¿å…å†…å­˜é—®é¢˜
            
            logger.info(f"ğŸ“Š ç›®æ ‡è®°å½•æ•°: {len(target_records)}")
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=16) as executor:  # é«˜å¹¶å‘
                future_to_source = {}
                
                for source_record in source_records:
                    future = executor.submit(
                        self._match_single_record_fast,
                        source_record, target_records, table_mappings,
                        source_table, target_table, task_id
                    )
                    future_to_source[future] = source_record
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_source):
                    try:
                        result = future.result(timeout=5)  # å¿«é€Ÿè¶…æ—¶
                        if result:
                            all_results.extend(result)
                    except Exception as e:
                        logger.warning(f"å•è®°å½•åŒ¹é…å¤±è´¥: {e}")
        
        duration = time.time() - start_time
        speed = batch_size / duration if duration > 0 else 0
        
        logger.info(f"âœ… ç®€å•å¿«é€ŸåŒ¹é…å®Œæˆ: {batch_size} æ¡è®°å½•, "
                   f"è€—æ—¶: {duration:.2f}ç§’, é€Ÿåº¦: {speed:.1f} æ¡/ç§’, "
                   f"åŒ¹é…ç»“æœ: {len(all_results)}")
        
        return all_results
    
    def _match_single_record_fast(self, source_record: Dict, target_records: List[Dict],
                                 mappings: List[Dict], source_table: str, 
                                 target_table: str, task_id: str) -> List[Dict]:
        """å•è®°å½•å¿«é€ŸåŒ¹é…"""
        results = []
        
        try:
            # è·å–ä¸»è¦åŒ¹é…å­—æ®µ
            primary_mappings = [m for m in mappings if m.get('field_priority') == 'primary']
            if not primary_mappings:
                primary_mappings = mappings[:2]  # å–å‰ä¸¤ä¸ªå­—æ®µ
            
            # å¿«é€Ÿç²¾ç¡®åŒ¹é…
            for target_record in target_records:
                match_score = 0
                matched_fields = []
                
                # åªæ£€æŸ¥ä¸»è¦å­—æ®µï¼Œå¿«é€ŸåŒ¹é…
                for mapping in primary_mappings:
                    source_field = mapping['source_field']
                    target_field = mapping['target_field']
                    
                    source_value = str(source_record.get(source_field, '')).strip()
                    target_value = str(target_record.get(target_field, '')).strip()
                    
                    if source_value and target_value:
                        # ç®€å•å­—ç¬¦ä¸²åŒ¹é…ï¼ˆæœ€å¿«ï¼‰
                        if source_value == target_value:
                            match_score += 1.0
                            matched_fields.append(f"{source_field}->{target_field}")
                        elif source_value in target_value or target_value in source_value:
                            match_score += 0.7  # è°ƒæ•´åŒ…å«åŒ¹é…å¾—åˆ†ä»0.8åˆ°0.7
                            matched_fields.append(f"{source_field}->{target_field}")
                
                # è®¡ç®—æœ€ç»ˆç›¸ä¼¼åº¦
                similarity = match_score / len(primary_mappings) if primary_mappings else 0
                
                # å¿«é€Ÿé˜ˆå€¼è¿‡æ»¤
                if similarity >= 0.5:  # è°ƒæ•´é˜ˆå€¼é€‚é…0.7åŒ…å«åŒ¹é…å¾—åˆ†
                    result = {
                        'source_id': str(source_record.get('_id', '')),
                        'target_id': str(target_record.get('_id', '')),
                        'source_table': source_table,
                        'target_table': target_table,
                        'similarity_score': similarity,
                        'matched_fields': matched_fields,
                        'match_algorithm': 'simple_fast',
                        'algorithm_used': 'simple_fast',
                        'source_key_fields': {m['source_field']: source_record.get(m['source_field'], '') for m in primary_mappings},
                        'target_key_fields': {m['target_field']: target_record.get(m['target_field'], '') for m in primary_mappings},
                        'match_details': {
                            'field_scores': {f"{m['source_field']}->{m['target_field']}": 1.0 if f"{m['source_field']}->{m['target_field']}" in matched_fields else 0.0 for m in primary_mappings},
                            'total_fields': len(primary_mappings),
                            'matched_field_count': len(matched_fields),
                            'confidence_level': 'high' if similarity >= 0.9 else 'medium',
                            'match_type': 'exact' if similarity >= 0.9 else 'fuzzy'
                        },
                        'created_at': time.time(),
                        'task_id': task_id,
                        'match_version': '2.0_fast'
                    }
                    results.append(result)
                    
                    # é™åˆ¶ç»“æœæ•°é‡ï¼Œæå‡é€Ÿåº¦
                    if len(results) >= 5:
                        break
        
        except Exception as e:
            logger.error(f"å¿«é€ŸåŒ¹é…å•è®°å½•å¤±è´¥: {e}")
        
        return results
